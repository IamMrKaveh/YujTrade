[data\sources\alphavantage_fetcher.py] :

import asyncio
import json
from typing import Any, Dict

from data.sources.base_fetcher import BaseFetcher
from common.cache import CacheKeyBuilder, CacheCategory
from common.utils import RateLimiter
from config.logger import logger


alpha_vantage_rate_limiter = RateLimiter(max_requests=5, time_window=60)


class AlphaVantageFetcher(BaseFetcher):
    def __init__(self, api_key: str, **kwargs):
        super().__init__(**kwargs)
        self.api_key = api_key
        self.base_url = "https://www.alphavantage.co/query"
        self.source_name = "alphavantage"

    async def get_macro_economic_data(self) -> Dict[str, Any]:
        cache_key = CacheKeyBuilder.macro_key(self.source_name, "all")
        cache_ttl = self.config_manager.get_cache_ttl("macro")

        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        if not self.api_key:
            return {}

        tasks = {
            "cpi": self._fetch_json(
                self.base_url,
                params={
                    "function": "CPI",
                    "interval": "monthly",
                    "apikey": self.api_key,
                },
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="cpi",
            ),
            "fed_rate": self._fetch_json(
                self.base_url,
                params={
                    "function": "FEDERAL_FUNDS_RATE",
                    "interval": "monthly",
                    "apikey": self.api_key,
                },
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="fed_rate",
            ),
            "gdp": self._fetch_json(
                self.base_url,
                params={
                    "function": "REAL_GDP",
                    "interval": "quarterly",
                    "apikey": self.api_key,
                },
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="gdp",
            ),
            "unemployment": self._fetch_json(
                self.base_url,
                params={"function": "UNEMPLOYMENT", "apikey": self.api_key},
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="unemployment",
            ),
            "treasury_yield": self._fetch_json(
                self.base_url,
                params={
                    "function": "TREASURY_YIELD",
                    "interval": "monthly",
                    "maturity": "10year",
                    "apikey": self.api_key,
                },
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="treasury_yield",
            ),
            "inflation": self._fetch_json(
                self.base_url,
                params={"function": "INFLATION", "apikey": self.api_key},
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="inflation",
            ),
            "consumer_sentiment": self._fetch_json(
                self.base_url,
                params={"function": "CONSUMER_SENTIMENT", "apikey": self.api_key},
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="consumer_sentiment",
            ),
            "retail_sales": self._fetch_json(
                self.base_url,
                params={"function": "RETAIL_SALES", "apikey": self.api_key},
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="retail_sales",
            ),
        }
        results = await asyncio.gather(*tasks.values(), return_exceptions=True)

        macro_data = {}
        (
            cpi_data,
            fed_rate_data,
            gdp_data,
            unemployment_data,
            yield_data,
            inflation_data,
            consumer_sentiment_data,
            retail_sales_data,
        ) = results

        if (
            not isinstance(cpi_data, Exception)
            and cpi_data
            and "data" in cpi_data
            and cpi_data["data"]
        ):
            macro_data["CPI"] = float(cpi_data["data"][0]["value"])
        if (
            not isinstance(fed_rate_data, Exception)
            and fed_rate_data
            and "data" in fed_rate_data
            and fed_rate_data["data"]
        ):
            macro_data["FED_RATE"] = float(fed_rate_data["data"][0]["value"])
        if (
            not isinstance(gdp_data, Exception)
            and gdp_data
            and "data" in gdp_data
            and gdp_data["data"]
        ):
            macro_data["GDP"] = float(gdp_data["data"][0]["value"])
        if (
            not isinstance(unemployment_data, Exception)
            and unemployment_data
            and "data" in unemployment_data
            and unemployment_data["data"]
        ):
            macro_data["UNEMPLOYMENT"] = float(unemployment_data["data"][0]["value"])
        if (
            not isinstance(yield_data, Exception)
            and yield_data
            and "data" in yield_data
            and yield_data["data"]
        ):
            macro_data["TREASURY_YIELD_10Y"] = float(yield_data["data"][0]["value"])
        if (
            not isinstance(inflation_data, Exception)
            and inflation_data
            and "data" in inflation_data
            and inflation_data["data"]
        ):
            macro_data["INFLATION"] = float(inflation_data["data"][0]["value"])
        if (
            not isinstance(consumer_sentiment_data, Exception)
            and consumer_sentiment_data
            and "data" in consumer_sentiment_data
            and consumer_sentiment_data["data"]
        ):
            macro_data["CONSUMER_SENTIMENT"] = float(
                consumer_sentiment_data["data"][0]["value"]
            )
        if (
            not isinstance(retail_sales_data, Exception)
            and retail_sales_data
            and "data" in retail_sales_data
            and retail_sales_data["data"]
        ):
            macro_data["RETAIL_SALES"] = float(retail_sales_data["data"][0]["value"])

        if self.redis and macro_data:
            await self.redis.set(cache_key, json.dumps(macro_data), ex=cache_ttl)

        return macro_data

    async def get_comprehensive_macro_data(self) -> Dict[str, Any]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.macro_key(self.source_name, "comprehensive")
        cache_ttl = self.config_manager.get_cache_ttl("macro")

        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}")

        if not self.api_key:
            return {}

        tasks = {
            "cpi": self._fetch_json(
                self.base_url,
                params={
                    "function": "CPI",
                    "interval": "monthly",
                    "apikey": self.api_key,
                },
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="cpi",
            ),
            "fed_rate": self._fetch_json(
                self.base_url,
                params={
                    "function": "FEDERAL_FUNDS_RATE",
                    "interval": "monthly",
                    "apikey": self.api_key,
                },
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="fed_rate",
            ),
            "gdp": self._fetch_json(
                self.base_url,
                params={
                    "function": "REAL_GDP",
                    "interval": "quarterly",
                    "apikey": self.api_key,
                },
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="gdp",
            ),
            "unemployment": self._fetch_json(
                self.base_url,
                params={"function": "UNEMPLOYMENT", "apikey": self.api_key},
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="unemployment",
            ),
            "treasury_yield": self._fetch_json(
                self.base_url,
                params={
                    "function": "TREASURY_YIELD",
                    "interval": "monthly",
                    "maturity": "10year",
                    "apikey": self.api_key,
                },
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="treasury_yield",
            ),
            "inflation": self._fetch_json(
                self.base_url,
                params={"function": "INFLATION", "apikey": self.api_key},
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="inflation",
            ),
            "consumer_sentiment": self._fetch_json(
                self.base_url,
                params={"function": "CONSUMER_SENTIMENT", "apikey": self.api_key},
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="consumer_sentiment",
            ),
            "retail_sales": self._fetch_json(
                self.base_url,
                params={"function": "RETAIL_SALES", "apikey": self.api_key},
                limiter=alpha_vantage_rate_limiter,
                endpoint_name="retail_sales",
            ),
        }

        results = await asyncio.gather(*tasks.values(), return_exceptions=True)

        macro_data = {}
        keys_list = list(tasks.keys())

        for i, result in enumerate(results):
            key = keys_list[i]
            if (
                not isinstance(result, Exception)
                and result
                and "data" in result
                and result["data"]
            ):
                try:
                    macro_data[key.upper()] = float(result["data"][0]["value"])
                except (KeyError, ValueError, IndexError) as e:
                    logger.warning(f"Could not parse {key}: {e}")

        if self.redis and macro_data:
            await self.redis.set(cache_key, json.dumps(macro_data), ex=cache_ttl)

        return macro_data


=====

[data\sources\alternativeme_fetcher.py] :

import json
from typing import List, Dict, Any

from data.sources.base_fetcher import BaseFetcher
from common.utils import RateLimiter
from common.cache import CacheKeyBuilder, CacheCategory


alternative_me_rate_limiter = RateLimiter(max_requests=20, time_window=60)


class AlternativeMeFetcher(BaseFetcher):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.source_name = "alternative.me"

    async def fetch_fear_greed(self, limit: int = 1) -> List[Dict[str, Any]]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.FEAR_GREED, self.source_name, [str(limit)]
        )
        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"https://api.alternative.me/fng/?limit={limit}"
        data = await self._fetch_json(
            url, limiter=alternative_me_rate_limiter, endpoint_name="fng"
        )

        results = data.get("data", [])
        if results and self.redis:
            await self.redis.set(cache_key, json.dumps(results), ex=3600)
        return results
    

=====

[data\sources\base_fetcher.py] :

from typing import Optional
import aiohttp
import redis.asyncio as redis
from config.logger import logger
from utils.circuit_breaker import CircuitBreaker
from common.exceptions import (
    APIRateLimitError,
    NetworkError,
    ObjectClosedError,
    InvalidSymbolError,
)
from common.utils import RateLimiter, async_retry
from config.settings import ConfigManager


class BaseFetcher:
    def __init__(
        self,
        redis_client: Optional[redis.Redis] = None,
        session: Optional[aiohttp.ClientSession] = None,
        config_manager: Optional[ConfigManager] = None,
    ):
        self.redis = redis_client
        self.session = session
        self.config_manager = config_manager or ConfigManager()
        self._is_closed = False
        self.circuit_breaker = CircuitBreaker()
        self.class_name = self.__class__.__name__

    def _check_if_closed(self):
        if self._is_closed:
            raise ObjectClosedError(
                f"{self.class_name} has been closed and cannot be used"
            )

    async def _get_session(self) -> aiohttp.ClientSession:
        self._check_if_closed()
        if self.session is None or self.session.closed:
            # This should ideally not be hit if ResourceManager is used correctly
            raise RuntimeError(
                f"Session for {self.class_name} is not available. It should be provided by ResourceManager."
            )
        return self.session

    async def close(self):
        # This method is now a no-op as the session is managed externally
        # by ResourceManager. This prevents accidental closing of the shared session.
        if self._is_closed:
            return
        self._is_closed = True
        logger.debug(
            f"Fetcher {self.class_name} instance closed (shared session not affected)."
        )

    @async_retry(
        attempts=3,
        delay=5,
        exceptions=(NetworkError, APIRateLimitError),
        ignore_exceptions=(InvalidSymbolError,),
    )
    async def _fetch_json(
        self,
        url,
        params=None,
        headers=None,
        limiter: Optional[RateLimiter] = None,
        endpoint_name: str = "default",
    ):
        self._check_if_closed()
        if limiter:
            await limiter.wait_if_needed(endpoint_name)

        async def _do_fetch():
            session = await self._get_session()
            async with session.get(url, params=params, headers=headers) as response:
                if response.status == 429:
                    raise APIRateLimitError(f"Rate limit exceeded for {url}")
                if response.status >= 400:
                    try:
                        error_text = await response.text()
                    except Exception:
                        error_text = "Could not read response body."

                    if "Invalid symbol" in error_text:
                        raise InvalidSymbolError(f"Invalid symbol for {url}")

                    logger.error(
                        f"Error fetching from {url}: Status {response.status}, Body: {error_text}"
                    )
                    response.raise_for_status()
                return await response.json()

        return await self.circuit_breaker.call(_do_fetch)


=====

[data\sources\binance_fetcher.py] :

import asyncio
import json
from typing import Any, Dict, List, Optional

import aiohttp
import pandas as pd
import redis.asyncio as redis

from config.logger import logger
from utils.circuit_breaker import CircuitBreaker
from common.core import OrderBook
from common.exceptions import (
    APIRateLimitError,
    DataError,
    InvalidSymbolError,
    NetworkError,
)
from common.utils import RateLimiter, async_retry
from data.sources.base_fetcher import BaseFetcher
from common.cache import CacheKeyBuilder, CacheCategory


binance_rate_limiter = RateLimiter(max_requests=1200, time_window=60)


class BinanceFetcher(BaseFetcher):
    FUTURES_BLACKLIST = {"SHIBUSDT", "PEPEUSDT", "FLOKIUSDT", "ASTRUSDT", "ONDOUSDT"}

    def __init__(
        self,
        redis_client: Optional[redis.Redis] = None,
        session: Optional[aiohttp.ClientSession] = None,
        **kwargs,
    ):
        super().__init__(redis_client=redis_client, session=session, **kwargs)
        self.base_url = "https://api.binance.com/api/v3"
        self.fapi_url = "https://fapi.binance.com/fapi/v1"
        self.futures_data_url = "https://fapi.binance.com/futures/data"
        self.source_name = "binance"

    @async_retry(
        attempts=3,
        delay=5,
        exceptions=(NetworkError, APIRateLimitError),
        ignore_exceptions=(InvalidSymbolError,),
    )
    async def _fetch(self, url: str, endpoint: str, params: Optional[Dict] = None):
        self._check_if_closed()
        full_url = f"{url}/{endpoint}"
        await binance_rate_limiter.wait_if_needed(endpoint)

        async def _do_fetch():
            session = await self._get_session()
            async with session.get(full_url, params=params) as response:
                if response.status == 429:
                    raise APIRateLimitError(f"Rate limit exceeded for {endpoint}")
                if response.status == 418:
                    logger.warning(
                        f"IP banned by Binance for {endpoint}. Sleeping for 120s."
                    )
                    await asyncio.sleep(120)
                    raise APIRateLimitError(f"IP banned, retrying for {endpoint}")
                if response.status >= 400:
                    try:
                        error_data = await response.json()
                        if error_data.get(
                            "code"
                        ) == -1121 or "Invalid symbol" in error_data.get("msg", ""):
                            raise InvalidSymbolError(
                                f"Invalid symbol for Binance: {params.get('symbol') if params else 'N/A'}"
                            )
                    except (aiohttp.ContentTypeError, json.JSONDecodeError):
                        pass  # Raise the original error below

                    response.raise_for_status()

                return await response.json()

        try:
            return await self.circuit_breaker.call(_do_fetch)
        except aiohttp.ClientResponseError as e:
            if e.status == 400:
                raise InvalidSymbolError(
                    f"Invalid symbol for Binance: {params.get('symbol') if params else 'N/A'}"
                )
            raise NetworkError(f"HTTP Error {e.status} for {full_url}") from e
        except Exception as e:
            if isinstance(e, (NetworkError, APIRateLimitError, InvalidSymbolError)):
                raise
            logger.error(f"Error in _fetch for {full_url}: {e}", exc_info=True)
            raise DataError(f"Failed to fetch data from {full_url}") from e

    async def get_historical_ohlc(
        self, symbol: str, timeframe: str, limit: int
    ) -> Optional[pd.DataFrame]:
        self._check_if_closed()
        try:
            binance_symbol = symbol.replace("/", "").upper()
            params = {"symbol": binance_symbol, "interval": timeframe, "limit": limit}
            data = await self._fetch(self.base_url, "klines", params)

            if not data:
                return None

            df = pd.DataFrame(
                data,
                columns=[
                    "timestamp",
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                    "close_time",
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                    "ignore",
                ],
            )
            df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
            df.set_index("timestamp", inplace=True)

            numeric_cols = ["open", "high", "low", "close", "volume"]
            df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors="coerce")

            df = df[numeric_cols].dropna()

            return df
        except InvalidSymbolError:
            logger.warning(f"Invalid symbol for Binance OHLC: {symbol}")
            return None
        except Exception as e:
            logger.error(
                f"Error fetching historical OHLC from Binance for {symbol}: {e}"
            )
            raise DataError(f"Failed to fetch OHLC for {symbol}") from e

    async def get_open_interest(self, symbol: str) -> Optional[float]:
        self._check_if_closed()
        binance_symbol = symbol.replace("/", "").upper()
        if binance_symbol in self.FUTURES_BLACKLIST:
            return None

        cache_key = CacheKeyBuilder.derivatives_key(
            self.source_name, "open_interest", symbol
        )
        cache_ttl = self.config_manager.get_cache_ttl("derivatives")

        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return float(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}", exc_info=True)

        try:
            params = {"symbol": binance_symbol}
            data = await self._fetch(self.fapi_url, "openInterest", params)
            if data and "openInterest" in data:
                open_interest = float(data["openInterest"])
                try:
                    if self.redis:
                        await self.redis.set(cache_key, open_interest, ex=cache_ttl)
                except Exception as e:
                    logger.warning(
                        f"Redis SET failed for {cache_key}: {e}", exc_info=True
                    )
                return open_interest
            return None
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(f"Error fetching open interest from Binance for {symbol}: {e}")
            return None

    async def get_taker_long_short_ratio(self, symbol: str) -> Optional[float]:
        self._check_if_closed()
        binance_symbol = symbol.replace("/", "").upper()
        if binance_symbol in self.FUTURES_BLACKLIST:
            return None

        cache_key = CacheKeyBuilder.derivatives_key(
            self.source_name, "long_short_ratio", symbol
        )
        cache_ttl = self.config_manager.get_cache_ttl("derivatives")

        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return float(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}", exc_info=True)

        try:
            params = {"symbol": binance_symbol, "period": "5m", "limit": 1}
            data = await self._fetch(
                self.futures_data_url, "takerlongshortRatio", params
            )
            if data and isinstance(data, list) and data and data[0].get("buySellRatio"):
                ratio = float(data[0]["buySellRatio"])
                try:
                    if self.redis:
                        await self.redis.set(cache_key, ratio, ex=cache_ttl)
                except Exception as e:
                    logger.warning(
                        f"Redis SET failed for {cache_key}: {e}", exc_info=True
                    )
                return ratio
            return None
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(
                f"Error fetching taker long/short ratio from Binance for {symbol}: {e}"
            )
            return None

    async def get_top_trader_long_short_ratio_accounts(
        self, symbol: str
    ) -> Optional[float]:
        self._check_if_closed()
        binance_symbol = symbol.replace("/", "").upper()
        if binance_symbol in self.FUTURES_BLACKLIST:
            return None
        try:
            params = {"symbol": binance_symbol, "period": "5m", "limit": 1}
            data = await self._fetch(
                self.futures_data_url, "globalLongShortAccountRatio", params
            )
            if (
                data
                and isinstance(data, list)
                and data
                and data[0].get("longShortRatio")
            ):
                return float(data[0]["longShortRatio"])
            return None
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(
                f"Error fetching top trader acc ratio from Binance for {symbol}: {e}"
            )
            return None

    async def get_top_trader_long_short_ratio_positions(
        self, symbol: str
    ) -> Optional[float]:
        self._check_if_closed()
        binance_symbol = symbol.replace("/", "").upper()
        if binance_symbol in self.FUTURES_BLACKLIST:
            return None
        try:
            params = {"symbol": binance_symbol, "period": "5m", "limit": 1}
            data = await self._fetch(
                self.futures_data_url, "topLongShortPositionRatio", params
            )
            if (
                data
                and isinstance(data, list)
                and data
                and data[0].get("longShortRatio")
            ):
                return float(data[0]["longShortRatio"])
            return None
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(
                f"Error fetching top trader pos ratio from Binance for {symbol}: {e}"
            )
            return None

    async def get_liquidation_orders(self, symbol: str) -> List[Dict]:
        self._check_if_closed()
        logger.debug(
            f"Skipping get_liquidation_orders for {symbol} as the endpoint is likely deprecated."
        )
        return []

    async def get_order_book_depth(self, symbol: str) -> Optional[OrderBook]:
        self._check_if_closed()
        try:
            binance_symbol = symbol.replace("/", "").upper()
            params = {"symbol": binance_symbol, "limit": 100}
            data = await self._fetch(self.base_url, "depth", params)
            if not data:
                return None

            bids = [(float(p), float(q)) for p, q in data.get("bids", [])]
            asks = [(float(p), float(q)) for p, q in data.get("asks", [])]

            return OrderBook(
                bids=bids,
                asks=asks,
                bid_ask_spread=asks[0][0] - bids[0][0] if asks and bids else 0,
                total_bid_volume=sum(q for _, q in bids),
                total_ask_volume=sum(q for _, q in asks),
            )
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(
                f"Error fetching order book depth from Binance for {symbol}: {e}"
            )
            return None

    async def get_mark_price(self, symbol: str) -> Optional[float]:
        self._check_if_closed()
        binance_symbol = symbol.replace("/", "").upper()
        if binance_symbol in self.FUTURES_BLACKLIST:
            return None
        try:
            params = {"symbol": binance_symbol}
            data = await self._fetch(self.fapi_url, "premiumIndex", params)
            if data and "markPrice" in data:
                return float(data["markPrice"])
            return None
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(f"Error fetching mark price from Binance for {symbol}: {e}")
            return None

    async def get_24h_ticker(self, symbol: str) -> Optional[Dict[str, Any]]:
        self._check_if_closed()
        try:
            binance_symbol = symbol.replace("/", "").upper()
            params = {"symbol": binance_symbol}
            data = await self._fetch(self.base_url, "ticker/24hr", params)
            if data:
                return {
                    "price_change": float(data.get("priceChange", 0)),
                    "price_change_percent": float(data.get("priceChangePercent", 0)),
                    "weighted_avg_price": float(data.get("weightedAvgPrice", 0)),
                    "prev_close_price": float(data.get("prevClosePrice", 0)),
                    "last_price": float(data.get("lastPrice", 0)),
                    "bid_price": float(data.get("bidPrice", 0)),
                    "ask_price": float(data.get("askPrice", 0)),
                    "open_price": float(data.get("openPrice", 0)),
                    "high_price": float(data.get("highPrice", 0)),
                    "low_price": float(data.get("lowPrice", 0)),
                    "volume": float(data.get("volume", 0)),
                    "quote_volume": float(data.get("quoteVolume", 0)),
                    "open_time": data.get("openTime"),
                    "close_time": data.get("closeTime"),
                    "first_id": data.get("firstId"),
                    "last_id": data.get("lastId"),
                    "count": data.get("count"),
                }
            return None
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(f"Error fetching 24h ticker from Binance for {symbol}: {e}")
            return None

    async def get_long_short_ratio_history(
        self, symbol: str, period: str = "5m", limit: int = 30
    ) -> List[Dict]:
        self._check_if_closed()
        binance_symbol = symbol.replace("/", "").upper()
        if binance_symbol in self.FUTURES_BLACKLIST:
            return []

        try:
            params = {"symbol": binance_symbol, "period": period, "limit": limit}
            data = await self._fetch(
                self.futures_data_url, "takerlongshortRatio", params
            )
            return data if isinstance(data, list) else []
        except InvalidSymbolError:
            return []
        except Exception as e:
            logger.error(
                f"Error fetching long/short ratio history from Binance for {symbol}: {e}"
            )
            return []

    async def get_open_interest_history(
        self, symbol: str, period: str = "5m", limit: int = 30
    ) -> List[Dict]:
        self._check_if_closed()
        binance_symbol = symbol.replace("/", "").upper()
        if binance_symbol in self.FUTURES_BLACKLIST:
            return []

        try:
            params = {"symbol": binance_symbol, "period": period, "limit": limit}
            data = await self._fetch(self.fapi_url, "openInterestHist", params)
            return data if isinstance(data, list) else []
        except InvalidSymbolError:
            return []
        except Exception as e:
            logger.error(
                f"Error fetching open interest history from Binance for {symbol}: {e}"
            )
            return []

    async def get_funding_rate(self, symbol: str) -> Optional[float]:
        binance_symbol = symbol.replace("/", "").upper()
        if binance_symbol in self.FUTURES_BLACKLIST:
            return None

        cache_key = CacheKeyBuilder.derivatives_key(
            self.source_name, "funding_rate", symbol
        )
        cache_ttl = self.config_manager.get_cache_ttl("derivatives")

        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return float(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}", exc_info=True)

        try:
            params = {"symbol": binance_symbol, "limit": 1}
            data = await self._fetch(self.fapi_url, "fundingRate", params)
            if data and isinstance(data, list) and data[0].get("fundingRate"):
                funding_rate = float(data[0]["fundingRate"])
                try:
                    if self.redis:
                        await self.redis.set(cache_key, funding_rate, ex=cache_ttl)
                except Exception as e:
                    logger.warning(
                        f"Redis SET failed for {cache_key}: {e}", exc_info=True
                    )
                return funding_rate
            return None
        except InvalidSymbolError:
            return None
        except Exception:
            # Fallback to premiumIndex if fundingRate fails
            try:
                params = {"symbol": binance_symbol}
                data = await self._fetch(self.fapi_url, "premiumIndex", params)
                if data and "lastFundingRate" in data:
                    funding_rate = float(data["lastFundingRate"])
                    if self.redis:
                        await self.redis.set(cache_key, funding_rate, ex=cache_ttl)
                    return funding_rate
                return None
            except Exception as e:
                logger.error(
                    f"Error fetching funding rate (fallback) from Binance for {symbol}: {e}"
                )
                return None

    async def get_kline_data(
        self,
        symbol: str,
        interval: str,
        start_time: Optional[int] = None,
        end_time: Optional[int] = None,
        limit: int = 500,
    ) -> List:
        self._check_if_closed()
        try:
            binance_symbol = symbol.replace("/", "").upper()
            params = {"symbol": binance_symbol, "interval": interval, "limit": limit}
            if start_time:
                params["startTime"] = start_time
            if end_time:
                params["endTime"] = end_time
            data = await self._fetch(self.base_url, "klines", params)
            return data if data else []
        except InvalidSymbolError:
            return []
        except Exception as e:
            logger.error(f"Error fetching kline data from Binance for {symbol}: {e}")
            return []

    async def get_ticker_price(self, symbol: str) -> Optional[float]:
        self._check_if_closed()
        try:
            binance_symbol = symbol.replace("/", "").upper()
            params = {"symbol": binance_symbol}
            data = await self._fetch(self.base_url, "ticker/price", params)
            if data and "price" in data:
                return float(data["price"])
            return None
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(f"Error fetching ticker price from Binance for {symbol}: {e}")
            return None

    async def get_exchange_info(self, symbol: Optional[str] = None) -> Optional[Dict]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.generic_key(
            self.source_name, "exchange_info", symbol
        )
        cache_ttl = self.config_manager.get_cache_ttl("generic")
        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}", exc_info=True)

        try:
            params = {}
            if symbol:
                params["symbol"] = symbol.replace("/", "").upper()
            data = await self._fetch(self.base_url, "exchangeInfo", params)
            if data and self.redis:
                await self.redis.set(cache_key, json.dumps(data), ex=cache_ttl)
            return data
        except InvalidSymbolError:
            return None
        except Exception as e:
            logger.error(f"Error fetching exchange info from Binance: {e}")
            return None

    async def get_agg_trades(self, symbol: str, limit: int = 500) -> List[Dict]:
        self._check_if_closed()
        try:
            binance_symbol = symbol.replace("/", "").upper()
            params = {"symbol": binance_symbol, "limit": limit}
            data = await self._fetch(self.base_url, "aggTrades", params)
            return data if data else []
        except InvalidSymbolError:
            return []
        except Exception as e:
            logger.error(
                f"Error fetching aggregate trades from Binance for {symbol}: {e}"
            )
            return []

    async def get_all_futures_symbols(self) -> List[Dict]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.generic_key(self.source_name, "futures_symbols")
        cache_ttl = self.config_manager.get_cache_ttl("generic")
        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}", exc_info=True)

        try:
            data = await self._fetch(self.fapi_url, "exchangeInfo")
            if data and "symbols" in data:
                symbols = data["symbols"]
                if self.redis:
                    await self.redis.set(cache_key, json.dumps(symbols), ex=cache_ttl)
                return symbols
            return []
        except Exception as e:
            logger.error(f"Error fetching futures symbols: {e}")
            return []


=====

[data\sources\coindesk_fetcher.py] :

import json
from typing import Any, Dict, List, Optional

import aiohttp
import pandas as pd
import redis
from config.logger import logger
from utils.circuit_breaker import CircuitBreaker
from common.exceptions import APIRateLimitError, NetworkError, InvalidSymbolError
from common.utils import RateLimiter, async_retry
from data.sources.base_fetcher import BaseFetcher
from common.cache import CacheKeyBuilder, CacheCategory


coindesk_rate_limiter = RateLimiter(max_requests=20, time_window=60)


class CoinDeskFetcher(BaseFetcher):
    def __init__(
        self,
        api_key: str,
        redis_client: Optional[redis.Redis] = None,
        session: Optional[aiohttp.ClientSession] = None,
        **kwargs,
    ):
        super().__init__(redis_client=redis_client, session=session, **kwargs)
        self.api_key = api_key
        self.base_url = "https://data-api.coindesk.com"
        self.source_name = "coindesk"

    @async_retry(
        attempts=3,
        delay=5,
        exceptions=(NetworkError, APIRateLimitError),
        ignore_exceptions=(InvalidSymbolError,),
    )
    async def _fetch(self, endpoint: str, params: Optional[Dict] = None):
        self._check_if_closed()
        headers = {"x-api-key": self.api_key}
        url = f"{self.base_url}/{endpoint}"
        await coindesk_rate_limiter.wait_if_needed(endpoint)

        async def _do_fetch():
            session = await self._get_session()
            async with session.get(url, params=params, headers=headers) as response:
                if response.status == 429:
                    raise APIRateLimitError("CoinDesk rate limit exceeded")
                if response.status >= 400:
                    text = await response.text()
                    if "not found" in text.lower():
                        raise InvalidSymbolError(
                            f"Invalid symbol for CoinDesk: {params.get('symbol') if params else endpoint}"
                        )
                    raise NetworkError(
                        f"HTTP Error {response.status} for {url}: {text}"
                    )
                return await response.json()

        return await self.circuit_breaker.call(_do_fetch)

    async def get_historical_ohlc(
        self, symbol: str, timeframe: str, limit: int
    ) -> Optional[pd.DataFrame]:
        self._check_if_closed()
        try:
            timeframe_lower = timeframe.lower()
            asset_id = symbol.upper().replace("/USDT", "-USD")

            if (
                "d" in timeframe_lower
                or "w" in timeframe_lower
                or "m" in timeframe_lower
            ):
                endpoint = f"spot/v1/historical/ohlcv/{asset_id}/d"
                params = {"limit": limit}
            else:
                endpoint = f"spot/v1/historical/ohlcv/{asset_id}/h"
                params = {"time_frame": timeframe, "limit": limit}

            data = await self._fetch(endpoint, params)

            if not data or "data" not in data or not data["data"]:
                return None

            ohlc_data = data["data"]
            if not ohlc_data:
                return None

            df = pd.DataFrame(ohlc_data)
            df.rename(
                columns={
                    "ts": "timestamp",
                    "o": "open",
                    "h": "high",
                    "l": "low",
                    "c": "close",
                    "v": "volume",
                },
                inplace=True,
            )
            df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
            df.set_index("timestamp", inplace=True)
            df = df.astype(float).sort_index()

            return df.tail(limit)
        except InvalidSymbolError:
            logger.warning(f"Invalid symbol for CoinDesk OHLC: {symbol}")
            return None
        except Exception as e:
            logger.error(
                f"Error fetching historical OHLC from CoinDesk for {symbol}: {e}"
            )
            return None

    async def get_news(self, symbols: List[str]) -> List[Dict]:
        self._check_if_closed()
        try:
            all_news = []
            news_url = "https://api.coindesk.com/v1/news"
            params = {"slug": ",".join(s.lower().replace("-usdt", "") for s in symbols)}
            headers = {"X-CoinDesk-API-Key": self.api_key}

            session = await self._get_session()
            async with session.get(
                news_url, params=params, headers=headers
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    if data and "data" in data:
                        all_news.extend(data["data"])

            unique_news = list({item["id"]: item for item in all_news}.values())
            return unique_news
        except Exception as e:
            logger.error(f"Error fetching news from CoinDesk: {e}")
            return []

    async def get_bitcoin_price_index(self) -> Optional[Dict[str, Any]]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.indices_key(self.source_name, "bpi")
        cache_ttl = self.config_manager.get_cache_ttl("indices")

        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        try:
            session = await self._get_session()
            async with session.get(
                "https://api.coindesk.com/v1/bpi/currentprice.json"
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    if self.redis:
                        await self.redis.set(cache_key, json.dumps(data), ex=cache_ttl)
                    return data
            return None
        except Exception as e:
            logger.error(f"Error fetching Bitcoin Price Index from CoinDesk: {e}")
            return None

    async def get_latest_news(self, limit: int = 20) -> List[Dict]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.build(
            CacheCategory.NEWS, self.source_name, ["latest", str(limit)]
        )
        cache_ttl = self.config_manager.get_cache_ttl("news")

        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}")

        try:
            news_url = "https://api.coindesk.com/v1/news/latest"
            params = {"limit": limit}
            headers = {"X-CoinDesk-API-Key": self.api_key}

            session = await self._get_session()
            async with session.get(
                news_url, params=params, headers=headers
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    news_items = data.get("data", [])
                    if self.redis and news_items:
                        await self.redis.set(
                            cache_key, json.dumps(news_items), ex=cache_ttl
                        )
                    return news_items
            return []
        except Exception as e:
            logger.error(f"Error fetching latest news: {e}")
            return []


=====

[data\sources\coingecko_fetcher.py] :

import asyncio
import json
from typing import Dict, List, Optional

import pandas as pd

from config.logger import logger
from common.core import FundamentalAnalysis
from common.exceptions import APIRateLimitError, NetworkError, InvalidSymbolError
from common.utils import RateLimiter, async_retry
from common.cache import CacheKeyBuilder, CacheCategory
from data.sources.base_fetcher import BaseFetcher


coingecko_rate_limiter = RateLimiter(max_requests=8, time_window=60)


class CoinGeckoFetcher(BaseFetcher):
    def __init__(self, api_key: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.api_key = api_key
        self.base_url = "https://api.coingecko.com/api/v3"
        self.pro_base_url = "https://pro-api.coingecko.com/api/v3"
        self._coin_list: Optional[List[Dict]] = None
        self._coin_map: Optional[Dict[str, str]] = None
        self.source_name = "coingecko"

    async def _ensure_coin_list(self):
        if self._coin_list is None:
            coins = await self.get_coins_list()
            if coins:
                self._coin_list = coins
                self._coin_map = {c["symbol"].upper(): c["id"] for c in coins}
            else:
                raise NetworkError("Failed to fetch coin list from CoinGecko.")

    async def _get_coin_id(self, symbol: str) -> Optional[str]:
        try:
            await self._ensure_coin_list()
            base_symbol = symbol.upper().split("/")[0]
            if self._coin_map:
                return self._coin_map.get(base_symbol, base_symbol.lower())
            return base_symbol.lower()
        except NetworkError:
            logger.error("Cannot get coin_id because coin list is unavailable.")
            return None

    @async_retry(
        attempts=3,
        delay=5,
        exceptions=(NetworkError, APIRateLimitError),
        ignore_exceptions=(InvalidSymbolError,),
    )
    async def _fetch_json(
        self,
        url,
        params=None,
        headers=None,
        limiter: Optional[RateLimiter] = None,
        endpoint_name: str = "default",
    ):
        self._check_if_closed()
        if limiter:
            await limiter.wait_if_needed(endpoint_name)

        request_params = params.copy() if params else {}
        request_headers = headers.copy() if headers else {"accept": "application/json"}

        effective_url = url
        if self.api_key:
            effective_url = effective_url.replace(self.base_url, self.pro_base_url)
            if "pro-api" in effective_url:
                request_headers["x-cg-pro-api-key"] = self.api_key
            else:  # Demo API key for non-pro endpoints
                request_params["x_cg_demo_api_key"] = self.api_key
        else:
            if "pro-api" in url:  # Should not happen if no key, but as a safeguard
                effective_url = url.replace(self.pro_base_url, self.base_url)

        async def _do_fetch():
            session = await self._get_session()
            async with session.get(
                effective_url, params=request_params, headers=request_headers
            ) as response:
                if response.status == 429:
                    raise APIRateLimitError(f"Rate limit exceeded for {effective_url}")
                if response.status != 200:
                    text = await response.text()
                    if "invalid vs_currency" in text or "coin not found" in text:
                        raise InvalidSymbolError(
                            f"Invalid symbol or currency for CoinGecko: {url}"
                        )
                    logger.error(
                        f"Error fetching from {effective_url}: {response.status} {text}"
                    )
                    raise NetworkError(
                        f"HTTP Error {response.status} for {effective_url}"
                    )
                return await response.json()

        return await self.circuit_breaker.call(_do_fetch)

    async def get_historical_ohlc(
        self, symbol: str, days: int = 365
    ) -> Optional[pd.DataFrame]:
        coin_id = await self._get_coin_id(symbol)
        if not coin_id:
            logger.warning(f"Could not find coin ID for symbol {symbol}")
            return None
        url = f"{self.base_url}/coins/{coin_id}/ohlc"
        params = {"vs_currency": "usd", "days": str(days)}
        data = await self._fetch_json(
            url,
            params,
            limiter=coingecko_rate_limiter,
            endpoint_name=f"coins_ohlc_{coin_id}",
        )

        if not data:
            return None

        df = pd.DataFrame(data, columns=["timestamp", "open", "high", "low", "close"])
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
        df.set_index("timestamp", inplace=True)
        df["volume"] = 0.0  # CoinGecko OHLC does not provide volume
        return df

    async def get_fundamental_data(
        self, coin_id_or_symbol: str
    ) -> Optional[FundamentalAnalysis]:
        coin_id = await self._get_coin_id(coin_id_or_symbol)
        if not coin_id:
            return None

        cache_key = CacheKeyBuilder.build(
            CacheCategory.FUNDAMENTAL, self.source_name, [coin_id]
        )
        cache_ttl = self.config_manager.get_cache_ttl("fundamental")

        if self.redis:
            try:
                cached = await self.redis.get(cache_key)
                if cached:
                    data = json.loads(cached)
                    return FundamentalAnalysis(**data)
            except Exception as e:
                logger.warning(f"Redis GET failed for {cache_key}: {e}")

        try:
            url = f"{self.base_url}/coins/{coin_id}"
            params = {
                "localization": "false",
                "tickers": "false",
                "market_data": "true",
                "community_data": "true",
                "developer_data": "true",
            }
            data = await self._fetch_json(
                url,
                params,
                limiter=coingecko_rate_limiter,
                endpoint_name=f"coins_{coin_id}_full",
            )

            if not data:
                return None

            market_data = data.get("market_data", {})
            community_data = data.get("community_data", {})
            developer_data = data.get("developer_data", {})

            fundamental_data = FundamentalAnalysis(
                market_cap=market_data.get("market_cap", {}).get("usd", 0.0),
                total_volume=market_data.get("total_volume", {}).get("usd", 0.0),
                developer_score=developer_data.get("pull_requests_merged", 0) * 0.4
                + developer_data.get("stars", 0) * 0.6,
                community_score=community_data.get("twitter_followers", 0),
            )

            if self.redis:
                await self.redis.set(
                    cache_key, json.dumps(fundamental_data.__dict__), ex=cache_ttl
                )

            return fundamental_data
        except Exception as e:
            logger.warning(f"Could not fetch fundamental data for {coin_id}: {e}")
            return None

    async def get_coins_list(self) -> List[Dict]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.COIN_LIST, self.source_name, ["all"]
        )
        cache_ttl = self.config_manager.get_cache_ttl("generic")

        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"{self.base_url}/coins/list"
        params = {"include_platform": "false"}
        data = await self._fetch_json(
            url, params, limiter=coingecko_rate_limiter, endpoint_name="coins_list"
        )

        if data and self.redis:
            await self.redis.set(cache_key, json.dumps(data), ex=cache_ttl)
        return data if data else []

    async def get_trending_searches(self) -> List[str]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.TRENDING, self.source_name, ["searches"]
        )
        cache_ttl = self.config_manager.get_cache_ttl("trending")

        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"{self.base_url}/search/trending"
        data = await self._fetch_json(
            url, limiter=coingecko_rate_limiter, endpoint_name="trending"
        )

        if data and "coins" in data:
            trending_symbols = [coin["item"]["symbol"] for coin in data["coins"]]
            if self.redis:
                await self.redis.set(
                    cache_key, json.dumps(trending_symbols), ex=cache_ttl
                )
            return trending_symbols
        return []

    async def get_circulating_supply(self, coin_id_or_symbol: str) -> Optional[float]:
        self._check_if_closed()
        coin_id = await self._get_coin_id(coin_id_or_symbol)
        if not coin_id:
            return None
        try:
            url = f"{self.base_url}/coins/{coin_id}"
            params = {"market_data": "true"}
            coin_data = await self._fetch_json(
                url,
                params,
                limiter=coingecko_rate_limiter,
                endpoint_name=f"coins_info_{coin_id}",
            )

            if coin_data and "market_data" in coin_data:
                circulating = coin_data["market_data"].get("circulating_supply")
                if circulating:
                    return float(circulating)

            return None
        except Exception as e:
            logger.error(f"Error fetching circulating supply for {coin_id}: {e}")
            return None


=====

[data\sources\cryptopanic_fetcher.py] :

import json
from typing import Dict, List, Optional

from common.utils import RateLimiter
from common.cache import CacheKeyBuilder, CacheCategory
from data.sources.base_fetcher import BaseFetcher


cryptopanic_rate_limiter = RateLimiter(max_requests=20, time_window=60)


class CryptoPanicFetcher(BaseFetcher):
    def __init__(self, api_key: str, **kwargs):
        super().__init__(**kwargs)
        self.api_key = api_key
        self.base_url = "https://cryptopanic.com/api/v1"
        self.source_name = "cryptopanic"

    async def fetch_posts(
        self, currencies: Optional[List[str]] = None, kind: Optional[str] = None
    ) -> List[Dict]:
        url = f"{self.base_url}/posts/"
        params = {"auth_token": self.api_key, "public": "true"}
        if currencies:
            params["currencies"] = ",".join(currencies)
        if kind:
            params["kind"] = kind

        data = await self._fetch_json(
            url, params, limiter=cryptopanic_rate_limiter, endpoint_name="posts"
        )
        return data.get("results", [])

    async def fetch_media(self) -> List[Dict]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.GENERIC, self.source_name, ["media"]
        )
        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"{self.base_url}/media/"
        params = {"auth_token": self.api_key}

        data = await self._fetch_json(
            url, params, limiter=cryptopanic_rate_limiter, endpoint_name="media"
        )
        media_list = data.get("results", [])
        if self.redis and media_list:
            await self.redis.set(cache_key, json.dumps(media_list), ex=86400)
        return media_list


=====

[data\sources\defillama_fetcher.py] :

import json
from typing import Dict, List, Optional

from config.logger import logger
from common.utils import RateLimiter
from common.cache import CacheKeyBuilder, CacheCategory
from data.sources.base_fetcher import BaseFetcher


defillama_rate_limiter = RateLimiter(max_requests=30, time_window=60)


class DeFiLlamaFetcher(BaseFetcher):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.base_url = "https://api.llama.fi"
        self.source_name = "defillama"

    async def get_total_tvl_for_chains(self) -> List:
        url = f"{self.base_url}/tvl/chains"
        data = await self._fetch_json(url, limiter=defillama_rate_limiter)
        return data if isinstance(data, list) else []

    async def get_protocol_chart(self, protocol: str) -> Optional[Dict]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.GENERIC, self.source_name, ["protocol_chart", protocol]
        )
        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"{self.base_url}/protocol/{protocol}"
        data = await self._fetch_json(
            url, limiter=defillama_rate_limiter, endpoint_name=f"protocol_{protocol}"
        )

        if data and self.redis:
            await self.redis.set(cache_key, json.dumps(data), ex=3600)
        return data

    async def get_defi_protocols(self) -> List[Dict]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.GENERIC, self.source_name, ["protocols"]
        )
        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"{self.base_url}/protocols"
        data = await self._fetch_json(
            url, limiter=defillama_rate_limiter, endpoint_name="protocols"
        )

        if data and self.redis:
            await self.redis.set(cache_key, json.dumps(data), ex=3600)
        return data if data else []

    async def get_chain_tvl(self, chain: str) -> Optional[Dict]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.METRICS, self.source_name, ["chain_tvl", chain]
        )
        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"{self.base_url}/v2/historicalChainTvl/{chain}"
        data = await self._fetch_json(
            url, limiter=defillama_rate_limiter, endpoint_name=f"chain_tvl_{chain}"
        )

        if data and self.redis:
            await self.redis.set(cache_key, json.dumps(data), ex=3600)
        return data

    async def get_stablecoins(self) -> Optional[Dict]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.GENERIC, self.source_name, ["stablecoins"]
        )
        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"{self.base_url}/stablecoins?includePrices=true"
        data = await self._fetch_json(
            url, limiter=defillama_rate_limiter, endpoint_name="stablecoins"
        )

        if data and self.redis:
            await self.redis.set(cache_key, json.dumps(data), ex=3600)
        return data

    async def get_yields(self) -> List[Dict]:
        cache_key = CacheKeyBuilder.build(
            CacheCategory.YIELD, self.source_name, ["all"]
        )
        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        url = f"{self.base_url}/pools"
        data = await self._fetch_json(
            url, limiter=defillama_rate_limiter, endpoint_name="yields"
        )

        if data and "data" in data and self.redis:
            yield_data = data["data"]
            await self.redis.set(cache_key, json.dumps(yield_data), ex=3600)
            return yield_data
        return []


=====

[data\sources\marketindices_fetcher.py] :

import asyncio
import json
from typing import Dict, Optional

import aiohttp
import redis

from config.logger import logger
from common.cache import CacheKeyBuilder, CacheCategory
from config.settings import ConfigManager
from data.sources.alphavantage_fetcher import AlphaVantageFetcher
from data.sources.coingecko_fetcher import CoinGeckoFetcher
from data.sources.defillama_fetcher import DeFiLlamaFetcher
from data.sources.yfinance_fetcher import YFinanceFetcher
from data.sources.base_fetcher import BaseFetcher


class MarketIndicesFetcher(BaseFetcher):
    def __init__(
        self,
        coingecko_fetcher: Optional[CoinGeckoFetcher] = None,
        defillama_fetcher: Optional[DeFiLlamaFetcher] = None,
        yfinance_fetcher: Optional[YFinanceFetcher] = None,
        alphavantage_fetcher: Optional[AlphaVantageFetcher] = None,
        redis_client: Optional[redis.Redis] = None,
        session: Optional[aiohttp.ClientSession] = None,
        config_manager: Optional[ConfigManager] = None,
    ):
        super().__init__(
            redis_client=redis_client, session=session, config_manager=config_manager
        )
        self.source_name = "market_indices_aggregator"

        common_args = {
            "redis_client": redis_client,
            "session": session,
            "config_manager": config_manager,
        }

        self.coingecko = coingecko_fetcher or CoinGeckoFetcher(**common_args)
        self.defillama = defillama_fetcher or DeFiLlamaFetcher(**common_args)
        self.yfinance = yfinance_fetcher or YFinanceFetcher(**common_args)
        self.alphavantage = alphavantage_fetcher

    def _check_if_closed(self):
        if self._is_closed:
            raise RuntimeError(
                "MarketIndicesFetcher has been closed and cannot be used"
            )

    async def close(self):
        if self._is_closed:
            return
        self._is_closed = True
        # Child fetchers are managed by MarketDataProvider, so no need to close them here.
        # Session is also managed externally.
        logger.debug("MarketIndicesFetcher instance closed.")

    async def get_crypto_indices(self) -> Dict[str, Optional[float]]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.build(
            CacheCategory.INDICES, self.source_name, ["crypto"]
        )
        cache_ttl = self.config_manager.get_cache_ttl("indices")

        if self.redis:
            try:
                cached_data = await self.redis.get(cache_key)
                if cached_data:
                    return json.loads(cached_data)
            except Exception as e:
                logger.warning(f"Redis get failed for {cache_key}: {e}")

        # This method in coingecko_fetcher needs to be added or adapted
        async def get_global_market_data():
            try:
                url = f"{self.coingecko.base_url}/global"
                return await self.coingecko._fetch_json(url, endpoint_name="global")
            except Exception as e:
                logger.error(f"Failed to fetch global market data from coingecko: {e}")
                return None

        tasks = {
            "global": get_global_market_data(),
            "defi_llama_tvl_total": self.defillama.get_total_tvl_for_chains(),
        }
        results = await asyncio.gather(*tasks.values(), return_exceptions=True)
        global_data, defi_llama_tvl_data = results

        indices = {}
        if (
            not isinstance(global_data, Exception)
            and global_data
            and "data" in global_data
        ):
            market_data = global_data["data"]
            dominance = market_data.get("market_cap_percentage", {})
            indices["BTC.D"] = dominance.get("btc")
            indices["ETH.D"] = dominance.get("eth")
            indices["USDT.D"] = dominance.get("usdt")
            total_mcap = market_data.get("total_market_cap", {}).get("usd")
            indices["TOTAL"] = total_mcap
            if total_mcap and indices.get("BTC.D"):
                btc_mcap = total_mcap * (indices["BTC.D"] / 100)
                indices["TOTAL2"] = total_mcap - btc_mcap
                if indices.get("ETH.D"):
                    eth_mcap = total_mcap * (indices["ETH.D"] / 100)
                    indices["TOTAL3"] = indices["TOTAL2"] - eth_mcap
            others_dominance = 100 - sum(
                d
                for d in [
                    indices.get("BTC.D"),
                    indices.get("ETH.D"),
                    indices.get("USDT.D"),
                ]
                if d is not None
            )
            indices["OTHERS.D"] = others_dominance

        if (
            not isinstance(defi_llama_tvl_data, Exception)
            and isinstance(defi_llama_tvl_data, list)
            and defi_llama_tvl_data
        ):
            total_tvl = sum(chain.get("tvl", 0) for chain in defi_llama_tvl_data)
            indices["DEFI_TVL"] = total_tvl

        if self.redis and indices:
            try:
                await self.redis.set(cache_key, json.dumps(indices), ex=cache_ttl)
            except Exception as e:
                logger.warning(f"Redis set failed for {cache_key}: {e}")
        return indices

    async def get_all_indices(self) -> Dict[str, Optional[float]]:
        self._check_if_closed()
        tasks = {
            "crypto": self.get_crypto_indices(),
            "traditional": self.yfinance.get_traditional_indices(),
        }

        if self.alphavantage:
            tasks["macro"] = self.alphavantage.get_macro_economic_data()

        results = await asyncio.gather(*tasks.values(), return_exceptions=True)

        all_indices = {}
        for key, result in zip(tasks.keys(), results):
            if not isinstance(result, Exception) and result:
                all_indices.update(result)

        return all_indices


=====

[data\sources\messari_fetcher.py] :

import asyncio
import json
from typing import Any, Dict, List, Optional

import aiohttp
import redis

from config.logger import logger
from utils.circuit_breaker import CircuitBreaker
from common.core import OnChainAnalysis
from common.exceptions import APIRateLimitError, DataError, NetworkError
from common.utils import RateLimiter, async_retry
from common.cache import CacheKeyBuilder, CacheCategory
from data.sources.base_fetcher import BaseFetcher


messari_rate_limiter = RateLimiter(max_requests=20, time_window=60)


class MessariFetcher(BaseFetcher):
    def __init__(
        self,
        api_key: str,
        redis_client: Optional[redis.Redis] = None,
        session: Optional[aiohttp.ClientSession] = None,
    ):
        super().__init__(redis_client=redis_client, session=session)
        self.api_key = api_key
        self.base_url = "https://data.messari.io/api/v1"
        self.base_url_v2 = "https://data.messari.io/api/v2"
        self.source_name = "messari"

    @async_retry(attempts=3, delay=5, exceptions=(NetworkError, APIRateLimitError))
    async def _fetch(self, endpoint: str, params: Optional[Dict] = None):
        self._check_if_closed()
        headers = {"x-messari-api-key": self.api_key}
        url = f"{self.base_url}/{endpoint}"
        await messari_rate_limiter.wait_if_needed(endpoint)

        async def _do_fetch():
            session = await self._get_session()
            async with session.get(url, params=params, headers=headers) as response:
                if response.status == 429:
                    raise APIRateLimitError("Messari rate limit exceeded")
                response.raise_for_status()
                return await response.json()

        try:
            return await self.circuit_breaker.call(_do_fetch)
        except Exception as e:
            if isinstance(e, (NetworkError, APIRateLimitError)):
                raise
            logger.error(f"Error in Messari _fetch for {url}: {e}", exc_info=True)
            raise DataError(f"Failed to fetch data from Messari API: {url}") from e

    async def get_asset_metrics(self, symbol: str) -> Optional[Dict[str, Any]]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.build(
            CacheCategory.METRICS, self.source_name, ["asset"], symbol
        )
        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}")

        try:
            asset_id = symbol.split("/")[0].lower()
            data = await self._fetch(f"assets/{asset_id}/metrics")
            if data and "data" in data:
                metrics = data["data"]
                try:
                    if self.redis:
                        await self.redis.set(cache_key, json.dumps(metrics), ex=3600)
                except Exception as e:
                    logger.warning(f"Redis SET failed for {cache_key}: {e}")
                return metrics
            return None
        except Exception as e:
            logger.error(f"Error fetching asset metrics from Messari for {symbol}: {e}")
            return None

    async def get_on_chain_data(self, symbol: str) -> Optional[OnChainAnalysis]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.build(
            CacheCategory.METRICS, self.source_name, ["onchain"], symbol
        )
        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    data = json.loads(cached)
                    return OnChainAnalysis(**data)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}")

        try:
            metrics = await self.get_asset_metrics(symbol)
            if not metrics:
                return None

            analysis_data = {
                "mvrv": metrics.get("market_data", {}).get("mvrv_usd"),
                "sopr": metrics.get("on_chain_data", {}).get("sopr"),
                "active_addresses": metrics.get("on_chain_data", {}).get(
                    "active_addresses"
                ),
                "realized_cap": metrics.get("marketcap", {}).get(
                    "realized_marketcap_usd"
                ),
            }

            on_chain_analysis = OnChainAnalysis(**analysis_data)

            try:
                if self.redis:
                    await self.redis.set(cache_key, json.dumps(analysis_data), ex=3600)
            except Exception as e:
                logger.warning(f"Redis SET failed for {cache_key}: {e}")

            return on_chain_analysis
        except Exception as e:
            logger.error(
                f"Error processing on-chain data from Messari for {symbol}: {e}"
            )
            return None

    async def get_all_assets(self) -> List[Dict]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.build(
            CacheCategory.GENERIC, self.source_name, ["all_assets"]
        )
        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}")

        try:
            data = await self._fetch("assets")
            if data and "data" in data and self.redis:
                asset_list = data["data"]
                await self.redis.set(cache_key, json.dumps(asset_list), ex=86400)
                return asset_list
            return []
        except Exception as e:
            logger.error(f"Error fetching all assets: {e}")
            return []


=====

[data\sources\news_fetcher.py] :

import asyncio
import json
from typing import Any, Dict, List, Optional
import redis

from config.logger import logger
from common.cache import CacheKeyBuilder, CacheCategory
from data.sources.alternativeme_fetcher import AlternativeMeFetcher
from data.sources.coindesk_fetcher import CoinDeskFetcher
from data.sources.cryptopanic_fetcher import CryptoPanicFetcher
from data.sources.messari_fetcher import MessariFetcher


class NewsFetcher:
    def __init__(
        self,
        cryptopanic_fetcher: CryptoPanicFetcher,
        alternativeme_fetcher: AlternativeMeFetcher,
        coindesk_fetcher: Optional[CoinDeskFetcher] = None,
        messari_fetcher: Optional[MessariFetcher] = None,
        redis_client: Optional[redis.Redis] = None,
    ):

        self.cryptopanic = cryptopanic_fetcher
        self.alternativeme = alternativeme_fetcher
        self.coindesk = coindesk_fetcher
        self.messari = messari_fetcher
        self.redis = redis_client
        self._is_closed = False
        self.source_name = "news_aggregator"

    def _check_if_closed(self):
        if self._is_closed:
            raise RuntimeError("NewsFetcher has been closed and cannot be used")

    async def close(self):
        if self._is_closed:
            return
        self._is_closed = True

        tasks = [self.cryptopanic.close(), self.alternativeme.close()]
        if self.coindesk:
            tasks.append(self.coindesk.close())
        if self.messari:
            tasks.append(self.messari.close())

        await asyncio.gather(*tasks, return_exceptions=True)

    async def fetch_fear_greed(self, limit: int = 1) -> List[Dict]:
        self._check_if_closed()
        return await self.alternativeme.fetch_fear_greed(limit)

    async def fetch_news(
        self, currencies: List[str] = ["BTC", "ETH"], kind: Optional[str] = None
    ) -> List[Dict]:
        self._check_if_closed()
        key_parts = sorted(currencies) + ([kind] if kind else [])
        key = CacheKeyBuilder.build(CacheCategory.NEWS, self.source_name, key_parts)

        if self.redis:
            try:
                cached = await self.redis.get(key)
                if cached:
                    return json.loads(cached)
            except Exception as e:
                logger.warning(f"Redis get failed for {key}: {e}")

        tasks = [self.cryptopanic.fetch_posts(currencies, kind)]

        if self.coindesk and not kind:
            tasks.append(self.coindesk.get_news(currencies))

        if self.messari and not kind:
            for currency in currencies:
                tasks.append(self.messari.get_news(f"{currency}/USDT"))

        results = await asyncio.gather(*tasks, return_exceptions=True)
        all_news = []
        for res in results:
            if isinstance(res, list):
                all_news.extend(res)
            elif isinstance(res, Exception):
                logger.warning(f"News fetch failed: {res}")

        unique_news = list(
            {item.get("id") or item.get("title"): item for item in all_news}.values()
        )

        if self.redis and unique_news:
            await self.redis.set(key, json.dumps(unique_news), ex=600)

        return unique_news

    def score_news(self, news_items: List[Dict[str, Any]]):
        score = 0
        if not news_items:
            return score
        for it in news_items:
            if "votes" in it:
                score += int(it["votes"].get("positive", 0))
                score -= int(it["votes"].get("negative", 0))

            title = (it.get("title") or "").lower()
            if any(
                k in title
                for k in [
                    "bull",
                    "rally",
                    "surge",
                    "gain",
                    "pump",
                    "partnership",
                    "adoption",
                    "breakthrough",
                ]
            ):
                score += 1
            if any(
                k in title
                for k in [
                    "crash",
                    "dump",
                    "fall",
                    "drop",
                    "hack",
                    "scam",
                    "exploit",
                    "regulatory",
                    "ban",
                ]
            ):
                score -= 1
        return score

    async def fetch_sentiment_analysis(
        self, currencies: List[str] = ["BTC", "ETH"]
    ) -> Dict[str, Any]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.build(
            CacheCategory.SENTIMENT, self.source_name, sorted(currencies)
        )

        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}")

        try:
            news_items = await self.fetch_news(currencies)
            fear_greed = await self.fetch_fear_greed(limit=1)

            sentiment_score = self.score_news(news_items)

            positive_count = sum(
                1
                for item in news_items
                if "votes" in item
                and item["votes"].get("positive", 0) > item["votes"].get("negative", 0)
            )
            negative_count = sum(
                1
                for item in news_items
                if "votes" in item
                and item["votes"].get("negative", 0) > item["votes"].get("positive", 0)
            )

            sentiment_data = {
                "overall_score": sentiment_score,
                "news_count": len(news_items),
                "positive_count": positive_count,
                "negative_count": negative_count,
                "neutral_count": len(news_items) - positive_count - negative_count,
                "fear_greed_index": int(fear_greed[0]["value"]) if fear_greed else None,
                "fear_greed_classification": (
                    fear_greed[0]["value_classification"] if fear_greed else None
                ),
            }

            if self.redis:
                await self.redis.set(cache_key, json.dumps(sentiment_data), ex=1800)

            return sentiment_data
        except Exception as e:
            logger.error(f"Error performing sentiment analysis: {e}")
            return {}

    async def fetch_trending_topics(self) -> List[str]:
        self._check_if_closed()
        cache_key = CacheKeyBuilder.build(
            CacheCategory.TRENDING, self.source_name, ["topics"]
        )

        try:
            if self.redis:
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
        except Exception as e:
            logger.warning(f"Redis GET failed for {cache_key}: {e}")

        try:
            trending_news = await self.cryptopanic.fetch_posts(kind="news")

            topics = []
            for item in trending_news[:20]:
                title = item.get("title", "").lower()
                currencies = item.get("currencies", [])
                for currency in currencies:
                    code = currency.get("code")
                    if code and code not in topics:
                        topics.append(code)

            if self.redis and topics:
                await self.redis.set(cache_key, json.dumps(topics), ex=3600)

            return topics
        except Exception as e:
            logger.error(f"Error fetching trending topics: {e}")
            return []


=====

[data\sources\yfinance_fetcher.py] :

import json
import pandas as pd
import yfinance as yf
from typing import Dict

from config.logger import logger
from common.utils import async_retry
from common.cache import CacheKeyBuilder, CacheCategory
from data.sources.base_fetcher import BaseFetcher


class YFinanceFetcher(BaseFetcher):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.source_name = "yfinance"

    @async_retry(attempts=3, delay=5)
    async def get_traditional_indices(self) -> Dict:
        cache_key = CacheKeyBuilder.indices_key(self.source_name, "traditional")
        cache_ttl = self.config_manager.get_cache_ttl("indices")

        if self.redis:
            cached = await self.redis.get(cache_key)
            if cached:
                return json.loads(cached)

        tickers = {
            "DXY": "DX-Y.NYB",
            "SPX": "^GSPC",
            "NDX": "^IXIC",
            "VIX": "^VIX",
            "GOLD": "GC=F",
            "OIL": "CL=F",
            "DJI": "^DJI",
            "RUT": "^RUT",
        }
        data = yf.download(
            tickers=list(tickers.values()),
            period="5d",
            interval="1d",
            progress=False,
            auto_adjust=True,
        )
        if data is None or data.empty:
            return {}

        price_column = "Close"
        if price_column not in data.columns:
            logger.error("Could not find 'Close' in yfinance data.")
            return {}

        indices = {
            key: float(data[price_column][ticker].iloc[-1])
            for key, ticker in tickers.items()
            if ticker in data[price_column]
            and not pd.isna(data[price_column][ticker].iloc[-1])
        }

        if self.redis and indices:
            await self.redis.set(cache_key, json.dumps(indices), ex=cache_ttl)
        return indices


=====

