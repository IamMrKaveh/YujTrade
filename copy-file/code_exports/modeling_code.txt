[modeling\models.py] :

import asyncio
import pickle
from concurrent.futures import ThreadPoolExecutor, CancelledError
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, Type
import os
import redis

import numpy as np
import pandas as pd

import tensorflow as tf
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # type: ignore
from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout  # type: ignore
from tensorflow.keras.models import Sequential  # type: ignore
from tensorflow.keras.optimizers import Adam  # type: ignore

from config.logger import logger
from utils.resource_manager import managed_tf_session
from common.exceptions import ModelError, ObjectClosedError
from features.feature_engineering import FeatureEngineer
from strategy.signal_tracker import MLConfidenceCalibrator

tf.get_logger().setLevel("ERROR")
try:
    physical_devices = tf.config.list_physical_devices("GPU")
    if physical_devices:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
    else:
        os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

except (RuntimeError, ValueError) as e:
    logger.warning(f"Could not configure TensorFlow devices: {e}")


class BaseModel:
    def __init__(self, symbol: str, timeframe: str, model_path: str):
        self.model: Any = None
        self.is_trained = False
        self.symbol = symbol
        self.timeframe = timeframe
        self.model_path = Path(model_path)
        self.model_path.mkdir(parents=True, exist_ok=True)
        self.feature_engineer = FeatureEngineer()
        self.last_training_date = None
        self._is_closed = False
        self.logger = logger
        # Add calibrator instance
        self.calibrator = MLConfidenceCalibrator()

    def _check_if_closed(self):
        if self._is_closed:
            raise ObjectClosedError(
                f"Model {self.symbol}-{self.timeframe} has been closed and cannot be used"
            )

    def _get_model_paths(self, model_name: str, extension: str) -> Tuple[Path, Path]:
        safe_symbol = self.symbol.lower().replace("/", "").replace("\\", "")
        model_suffix = f"{safe_symbol}_{self.timeframe}"
        model_file = self.model_path / f"{model_name}_{model_suffix}.{extension}"
        scaler_file = self.model_path / f"scaler_{model_name}_{model_suffix}.pkl"
        return model_file, scaler_file

    def save_scaler(self, model_name: str):
        self._check_if_closed()
        _, scaler_file = self._get_model_paths(model_name, "")
        scaler_file.parent.mkdir(parents=True, exist_ok=True)
        try:
            with open(scaler_file, "wb") as f:
                pickle.dump(self.feature_engineer.scaler, f)
            self.logger.debug(f"Scaler saved to {scaler_file}")
        except Exception as e:
            self.logger.error(f"Error saving scaler: {e}")
            raise

    def load_scaler(self, model_name: str) -> bool:
        self._check_if_closed()
        _, scaler_file = self._get_model_paths(model_name, "")
        if scaler_file.exists():
            try:
                with open(scaler_file, "rb") as f:
                    self.feature_engineer.scaler = pickle.load(f)
                self.logger.debug(f"Scaler loaded from {scaler_file}")
                return True
            except Exception as e:
                self.logger.error(f"Error loading scaler: {e}")
                return False
        return False

    def fit(self, *args, **kwargs):
        raise NotImplementedError

    def predict(self, *args, **kwargs):
        raise NotImplementedError

    def save(self):
        raise NotImplementedError

    def load(self):
        raise NotImplementedError

    def cleanup(self):
        if self._is_closed:
            return
        self._is_closed = True
        try:
            if self.model:
                del self.model
                self.model = None
                self.logger.debug(
                    f"Cleaned up model object for {self.symbol}-{self.timeframe}"
                )
        except Exception as e:
            self.logger.error(
                f"Error during model cleanup for {self.symbol}-{self.timeframe}: {e}"
            )

    def __del__(self):
        if not self._is_closed:
            self.cleanup()


class LSTMModel(BaseModel):
    def __init__(
        self,
        symbol: str,
        timeframe: str,
        model_path: str = "models/lstm",
        units=64,
        lr=0.001,
    ):
        super().__init__(symbol, timeframe, model_path)

        sequence_length_map = {"1h": 60, "4h": 48, "1d": 30, "1w": 24, "1M": 12}
        self.sequence_length = sequence_length_map.get(timeframe, 30)

        self.units = max(32, units // (2 if timeframe in ["1w", "1M"] else 1))
        self.lr = lr
        self.batch_size = 16 if timeframe in ["1w", "1M"] else 32
        self.input_shape: Optional[Tuple[int, int]] = None

    def _create_model(self):
        self._check_if_closed()
        if not self.input_shape:
            raise ModelError("Input shape must be set before creating LSTM model.")

        with managed_tf_session():
            self.model = Sequential(
                [
                    LSTM(
                        self.units,
                        input_shape=self.input_shape,
                        return_sequences=True,
                        dropout=0.2,
                        recurrent_dropout=0.2,
                    ),
                    LSTM(self.units // 2, return_sequences=False, dropout=0.2),
                    Dropout(0.3),
                    Dense(32, activation="relu"),
                    BatchNormalization(),
                    Dropout(0.2),
                    Dense(1, activation="linear"),
                ]
            )
            optimizer = Adam(learning_rate=self.lr, clipnorm=1.0)
            self.model.compile(optimizer=optimizer, loss="huber", metrics=["mae"])
            self.logger.debug(f"Created LSTM model for {self.symbol}-{self.timeframe}")

    def fit(
        self, data: pd.DataFrame, epochs=15, batch_size=None, validation_split=0.2
    ) -> bool:
        self._check_if_closed()
        try:
            if (
                data is None or data.empty or len(data) < self.sequence_length + 20
            ):  # Increased minimum data
                self.logger.warning(
                    f"Insufficient data for {self.symbol}-{self.timeframe}, skipping training. Required: {self.sequence_length + 20}, Got: {len(data) if data is not None else 0}"
                )
                return False

            features = self.feature_engineer.create_features(data)
            if features.empty or len(features) < self.sequence_length + 20:
                self.logger.warning(
                    f"No features or insufficient features created for {self.symbol}-{self.timeframe}, skipping training."
                )
                return False

            scaled_features = self.feature_engineer.scale_features(features, fit=True)
            if (
                scaled_features is None
                or len(scaled_features) < self.sequence_length + 20
            ):
                self.logger.warning(
                    f"Feature scaling failed or resulted in insufficient data for {self.symbol}-{self.timeframe}, skipping training."
                )
                return False

            self.input_shape = (self.sequence_length, scaled_features.shape[1])
            self._create_model()

            X, y = self.feature_engineer.create_sequences(
                scaled_features, scaled_features[:, 0], self.sequence_length
            )  # Target is the first column ('close')
            if X.shape[0] == 0 or y.shape[0] == 0:
                self.logger.warning(
                    f"Not enough data to create sequences for {self.symbol}-{self.timeframe}, skipping training."
                )
                return False

            # Ensure validation set is not empty
            if int(X.shape[0] * validation_split) < 1:
                self.logger.warning(
                    f"Not enough data for validation split in {self.symbol}-{self.timeframe}. Disabling validation."
                )
                validation_split = 0.0

            effective_batch_size = min(batch_size or self.batch_size, X.shape[0] // 5)
            if effective_batch_size < 1:
                effective_batch_size = 1

            callbacks = [
                (
                    EarlyStopping(
                        monitor="val_loss", patience=5, restore_best_weights=True
                    )
                    if validation_split > 0
                    else EarlyStopping(
                        monitor="loss", patience=5, restore_best_weights=True
                    )
                ),
                (
                    ReduceLROnPlateau(
                        monitor="val_loss", factor=0.7, patience=3, min_lr=1e-7
                    )
                    if validation_split > 0
                    else ReduceLROnPlateau(
                        monitor="loss", factor=0.7, patience=3, min_lr=1e-7
                    )
                ),
            ]

            with managed_tf_session():
                history = self.model.fit(
                    X,
                    y,
                    epochs=epochs,
                    batch_size=effective_batch_size,
                    validation_split=validation_split if validation_split > 0 else 0.0,
                    callbacks=callbacks,
                    verbose=0,
                    shuffle=False,
                )

            final_loss_key = "val_loss" if validation_split > 0 else "loss"
            final_val_loss = history.history.get(final_loss_key, [float("inf")])[-1]

            price_std = data["close"].std()
            loss_threshold = (
                (0.25 * (price_std / data["close"].mean()) * 1.5)
                if price_std > 0 and data["close"].mean() > 0
                else 0.75
            )

            if final_val_loss < loss_threshold:
                self.is_trained = True
                self.last_training_date = pd.Timestamp.now(tz="UTC")
                try:
                    self.save()
                    self.logger.info(
                        f"LSTM model for {self.symbol}-{self.timeframe} trained and saved successfully with {final_loss_key}: {final_val_loss:.4f}"
                    )
                except Exception as save_e:
                    self.logger.error(
                        f"Failed to save LSTM model for {self.symbol}-{self.timeframe}: {save_e}"
                    )
                    self.is_trained = False
                    return False
                return True
            else:
                self.logger.warning(
                    f"LSTM training for {self.symbol}-{self.timeframe} did not converge. Final {final_loss_key}: {final_val_loss:.4f} (Threshold: {loss_threshold:.4f})"
                )
                self.is_trained = False
                return False
        except Exception as e:
            self.logger.error(
                f"Error during LSTM training for {self.symbol}-{self.timeframe}: {e}",
                exc_info=True,
            )
            self.is_trained = False
            return False

    def predict(self, data: pd.DataFrame) -> Optional[Tuple[np.ndarray, float]]:
        self._check_if_closed()
        if not self.model or not self.is_trained:
            self.logger.debug(
                f"Prediction skipped for {self.symbol}-{self.timeframe}: model not available or not trained."
            )
            return None

        try:
            if data is None or data.empty:
                self.logger.warning(
                    f"Empty data for prediction {self.symbol}-{self.timeframe}"
                )
                return None

            features = self.feature_engineer.create_features(data)
            if features.empty or len(features) < self.sequence_length:
                self.logger.warning(
                    f"Not enough data for prediction for {self.symbol}-{self.timeframe}. Required: {self.sequence_length}, available: {len(features)}"
                )
                return None

            last_sequence_features = features.tail(self.sequence_length)
            scaled_sequence = self.feature_engineer.scale_features(
                last_sequence_features, fit=False
            )
            if scaled_sequence is None or len(scaled_sequence) == 0:
                return None

            reshaped_sequence = scaled_sequence.reshape(
                1, self.sequence_length, scaled_sequence.shape[1]
            )

            with managed_tf_session():
                # Make multiple predictions with dropout enabled for uncertainty estimation
                predictions_sample = [
                    self.model(reshaped_sequence, training=True) for _ in range(10)
                ]
                predictions_sample_np = np.array(
                    [p.numpy() for p in predictions_sample]
                ).squeeze()

            if predictions_sample_np is None or predictions_sample_np.size == 0:
                return None

            scaled_prediction = np.mean(predictions_sample_np, axis=0, keepdims=True)
            prediction_std = np.std(predictions_sample_np, axis=0)

            prediction = self.feature_engineer.inverse_scale_prediction(
                scaled_prediction
            )

            # Normalize uncertainty
            uncertainty = min(prediction_std * 0.5, 0.9)

            return prediction, float(uncertainty)
        except Exception as e:
            self.logger.error(
                f"Error during LSTM prediction for {self.symbol}-{self.timeframe}: {e}",
                exc_info=True,
            )
            return None

    def save(self):
        self._check_if_closed()
        if not self.model:
            raise ModelError("Cannot save a non-existent model.")
        try:
            model_file, _ = self._get_model_paths("lstm", "keras")
            model_file.parent.mkdir(parents=True, exist_ok=True)

            with managed_tf_session():
                self.model.save(str(model_file))

            self.save_scaler("lstm")
            self.logger.info(
                f"LSTM model for {self.symbol}-{self.timeframe} saved to {model_file}"
            )
        except Exception as e:
            self.logger.error(
                f"Error saving LSTM model for {self.symbol}-{self.timeframe}: {e}"
            )
            raise

    def load(self):
        self._check_if_closed()
        try:
            model_file, _ = self._get_model_paths("lstm", "keras")
            if not model_file.exists():
                raise ModelError(f"Model file not found at {model_file}")

            if not self.load_scaler("lstm"):
                raise ModelError(
                    f"Scaler not found for LSTM model {self.symbol}-{self.timeframe}. It will need retraining."
                )

            with managed_tf_session():
                self.model = tf.keras.models.load_model(str(model_file), compile=False)
                self.model.compile(
                    optimizer=Adam(learning_rate=self.lr, clipnorm=1.0),
                    loss="huber",
                    metrics=["mae"],
                )

            self.is_trained = True
            if hasattr(self.model, "input_shape") and self.model.input_shape:
                self.input_shape = self.model.input_shape[1:]
            else:
                self.logger.error(
                    f"Loaded LSTM model for {self.symbol}-{self.timeframe} has no input_shape."
                )
                self.is_trained = False
                raise ModelError("Loaded model has no input_shape")

            self.logger.info(
                f"LSTM model for {self.symbol}-{self.timeframe} loaded from {model_file}"
            )
        except Exception as e:
            self.logger.error(
                f"Error loading LSTM model for {self.symbol}-{self.timeframe}: {e}"
            )
            self.is_trained = False
            raise

    def cleanup(self):
        if self._is_closed:
            return
        super().cleanup()
        try:
            tf.keras.backend.clear_session()
            self.logger.debug(
                f"Keras session cleared for {self.symbol}-{self.timeframe}"
            )
        except Exception as e:
            self.logger.error(
                f"Error clearing Keras session for {self.symbol}-{self.timeframe}: {e}"
            )


class XGBoostModel(BaseModel):
    def __init__(
        self, symbol: str, timeframe: str, model_path: str = "models/xgboost", **params
    ):
        super().__init__(symbol, timeframe, model_path)
        default_params = {
            "objective": "reg:squarederror",
            "eval_metric": "rmse",
            "n_estimators": 150,
            "learning_rate": 0.05,
            "tree_method": "hist",
            "max_depth": 5,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "min_child_weight": 3,
        }
        if params:
            default_params.update(params)
        self.params = default_params
        self.model = xgb.XGBRegressor(**self.params)

    def fit(self, data: pd.DataFrame, validation_split=0.2) -> bool:
        self._check_if_closed()
        try:
            if data is None or data.empty or len(data) < 50:
                self.logger.warning(
                    f"Insufficient data for XGBoost {self.symbol}-{self.timeframe}"
                )
                return False

            features = self.feature_engineer.create_features(data)
            if features.empty or len(features) < 50:
                self.logger.warning(
                    f"No features or insufficient features created for XGBoost {self.symbol}-{self.timeframe}"
                )
                return False

            target = features["close"].shift(-1).dropna()
            features = features.loc[target.index]

            scaled_features = self.feature_engineer.scale_features(features, fit=True)
            if scaled_features is None:
                self.logger.warning(
                    f"Feature scaling failed for XGBoost {self.symbol}-{self.timeframe}"
                )
                return False

            scaled_features_df = pd.DataFrame(
                scaled_features, index=features.index, columns=features.columns
            )

            X = scaled_features_df
            y = target

            if len(X) != len(y) or len(X) == 0:
                self.logger.warning(
                    f"Feature-target mismatch for XGBoost {self.symbol}-{self.timeframe}"
                )
                return False

            split_index = max(1, int(len(X) * (1 - validation_split)))
            X_train, X_val = X[:split_index], X[split_index:]
            y_train, y_val = y[:split_index], y[split_index:]

            if len(X_train) == 0 or len(X_val) == 0:
                self.logger.warning(
                    f"Insufficient split data for XGBoost {self.symbol}-{self.timeframe}"
                )
                return False

            self.model.fit(
                X_train.values,
                y_train.values,
                eval_set=[(X_val.values, y_val.values)],
                verbose=False,
                early_stopping_rounds=10,
            )

            self.is_trained = True
            self.last_training_date = pd.Timestamp.now(tz="UTC")

            try:
                self.save()
                self.logger.info(
                    f"XGBoost model for {self.symbol}-{self.timeframe} trained and saved successfully"
                )
            except Exception as save_e:
                self.logger.error(
                    f"Failed to save XGBoost model for {self.symbol}-{self.timeframe}: {save_e}"
                )
                self.is_trained = False
                return False

            return True
        except Exception as e:
            self.logger.error(
                f"Error during XGBoost training for {self.symbol}-{self.timeframe}: {e}",
                exc_info=True,
            )
            self.is_trained = False
            return False

    def predict(self, data: pd.DataFrame) -> Optional[Tuple[np.ndarray, float]]:
        self._check_if_closed()
        if not self.is_trained or self.model is None:
            selflogger.debug(
                f"Prediction skipped for XGBoost {self.symbol}-{self.timeframe}: model not trained"
            )
            return None

        try:
            if data is None or data.empty:
                self.logger.warning(
                    f"Empty data for XGBoost prediction {self.symbol}-{self.timeframe}"
                )
                return None

            features = self.feature_engineer.create_features(data)
            if features.empty:
                self.logger.warning(
                    f"No features for XGBoost prediction {self.symbol}-{self.timeframe}"
                )
                return None

            last_features = features.tail(1)
            if last_features.empty:
                return None

            scaled_features = self.feature_engineer.scale_features(
                last_features, fit=False
            )
            if scaled_features is None:
                return None

            prediction = self.model.predict(scaled_features)

            # Simulate uncertainty using feature importance
            uncertainty = 0.05
            try:
                importances = self.model.feature_importances_
                # Higher variance in importances might mean more uncertainty
                uncertainty = (
                    np.std(importances) / np.mean(importances)
                    if np.mean(importances) > 0
                    else 0.5
                )
                uncertainty = min(max(uncertainty * 0.1, 0.05), 0.5)
            except Exception:
                uncertainty = 0.15  # Fallback uncertainty

            return prediction, float(uncertainty)
        except Exception as e:
            self.logger.error(
                f"Error during XGBoost prediction for {self.symbol}-{self.timeframe}: {e}",
                exc_info=True,
            )
            return None

    def save(self):
        self._check_if_closed()
        try:
            model_file, _ = self._get_model_paths("xgboost", "json")
            model_file.parent.mkdir(parents=True, exist_ok=True)

            self.model.save_model(str(model_file))
            self.save_scaler("xgboost")  # Save the scaler
            self.logger.info(
                f"XGBoost model for {self.symbol}-{self.timeframe} saved to {model_file}"
            )
        except Exception as e:
            self.logger.error(
                f"Error saving XGBoost model for {self.symbol}-{self.timeframe}: {e}"
            )
            raise

    def load(self):
        self._check_if_closed()
        try:
            model_file, _ = self._get_model_paths("xgboost", "json")
            if not model_file.exists():
                raise ModelError(f"Model file not found at {model_file}")

            if not self.load_scaler("xgboost"):  # Load the scaler
                raise ModelError(
                    f"Scaler not found for XGBoost model {self.symbol}-{self.timeframe}. It will need retraining."
                )

            self.model.load_model(str(model_file))
            self.is_trained = True
            self.logger.info(
                f"XGBoost model for {self.symbol}-{self.timeframe} loaded from {model_file}"
            )
        except Exception as e:
            self.logger.error(
                f"Error loading XGBoost model for {self.symbol}-{self.timeframe}: {e}"
            )
            self.is_trained = False
            raise


=====

[modeling\model_manager.py] :

import asyncio
import os
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, CancelledError
from typing import Any, Dict, Optional, Type, Protocol, Awaitable

import numpy as np
import pandas as pd
import redis

from config.logger import logger
from common.exceptions import ObjectClosedError
from modeling.models import BaseModel, LSTMModel, XGBoostModel


class ModelDataProvider(Protocol):
    async def get_data_for_model(
        self, symbol: str, timeframe: str, for_prediction: bool = False
    ) -> Optional[pd.DataFrame]: ...


class ModelManager:
    def __init__(
        self,
        data_provider: ModelDataProvider,
        model_path: str = "models",
        redis_client: Optional[redis.Redis] = None,
    ):
        self.model_path = Path(model_path)
        self.model_path.mkdir(parents=True, exist_ok=True)
        self.redis_client = redis_client
        self._cache: Dict[str, "BaseModel"] = {}
        self._lock = asyncio.Lock()
        self._is_closed = False
        self.logger = logger
        self._training_executor = None
        self._prediction_executor = None
        self.data_provider = data_provider

    def _check_if_closed(self):
        if self._is_closed:
            raise ObjectClosedError("ModelManager has been closed and cannot be used")

    async def _get_executor(self, executor_type: str) -> ThreadPoolExecutor:
        self._check_if_closed()

        if executor_type == "training":
            if self._training_executor is None or self._training_executor._shutdown:
                max_workers = max(1, (os.cpu_count() or 4) // 2)
                self._training_executor = ThreadPoolExecutor(
                    max_workers=max_workers, thread_name_prefix="ModelTrainingExecutor"
                )
            return self._training_executor
        elif executor_type == "prediction":
            if self._prediction_executor is None or self._prediction_executor._shutdown:
                max_workers = max(1, (os.cpu_count() or 2))
                self._prediction_executor = ThreadPoolExecutor(
                    max_workers=max_workers,
                    thread_name_prefix="ModelPredictionExecutor",
                )
            return self._prediction_executor
        else:
            raise ValueError("Invalid executor type specified.")

    async def _run_in_executor(
        self, func, *args, executor_type: str = "prediction", **kwargs
    ) -> Any:
        self._check_if_closed()
        loop = asyncio.get_running_loop()
        executor = await self._get_executor(executor_type)
        return await loop.run_in_executor(executor, lambda: func(*args, **kwargs))

    def _get_model_class(self, model_type: str) -> Type["BaseModel"]:
        if model_type == "lstm":
            return LSTMModel
        if model_type == "xgboost":
            return XGBoostModel
        raise ValueError(f"Unknown model type: {model_type}")

    async def get_model(
        self, model_type: str, symbol: str, timeframe: str, auto_load: bool = True
    ) -> Optional["BaseModel"]:
        self._check_if_closed()
        key = f"{model_type}-{symbol}-{timeframe}"

        async with self._lock:
            if key in self._cache:
                cached_model = self._cache[key]
                if not cached_model._is_closed:
                    return cached_model
                del self._cache[key]

        model_dir = self.model_path / model_type
        model_class = self._get_model_class(model_type)
        model_path_str = str(model_dir)

        try:
            model = await self._run_in_executor(
                lambda: model_class(
                    symbol=symbol, timeframe=timeframe, model_path=model_path_str
                ),
                executor_type="prediction",
            )

            if auto_load:
                model_name_map = {"lstm": "keras", "xgboost": "json"}
                model_ext = model_name_map.get(model_type)
                if model_ext:
                    model_file, scaler_file = model._get_model_paths(
                        model_type, model_ext
                    )

                    if model_file.exists() and scaler_file.exists():
                        try:
                            await self._run_in_executor(
                                model.load, executor_type="prediction"
                            )
                            self.logger.info(
                                f"Loaded existing {model_type.upper()} model for {key}"
                            )
                        except Exception as e:
                            self.logger.warning(
                                f"Failed to load {model_type.upper()} model for {key}: {e}"
                            )
                            model.is_trained = False
                    else:
                        self.logger.info(
                            f"No existing {model_type.upper()} model found for {key}, needs training"
                        )
                        model.is_trained = False
                else:
                    model.is_trained = False

            async with self._lock:
                if key not in self._cache:
                    self._cache[key] = model
                else:
                    # Another task created the model in the meantime, use it and clean the new one.
                    model.cleanup()
                    return self._cache[key]
            return model
        except Exception as e:
            self.logger.error(
                f"Could not get or create {model_type.upper()} model for {key}: {e}",
                exc_info=True,
            )
            return None

    async def train_model(
        self, model_type: str, symbol: str, timeframe: str, data: pd.DataFrame, **kwargs
    ) -> bool:
        self._check_if_closed()

        if data is None or data.empty:
            self.logger.warning(
                f"Cannot train {model_type} for {symbol}-{timeframe}: empty data"
            )
            return False

        model = await self.get_model(model_type, symbol, timeframe, auto_load=False)
        if not model:
            self.logger.error(
                f"Failed to create model {model_type} for {symbol}-{timeframe}"
            )
            return False

        try:
            success = await self._run_in_executor(
                model.fit, data, executor_type="training", **kwargs
            )
            if success:
                self.logger.info(
                    f"Successfully trained {model_type.upper()} model for {symbol}-{timeframe}"
                )
                async with self._lock:
                    key = f"{model_type}-{symbol}-{timeframe}"
                    # Ensure the newly trained model is in the cache
                    if key in self._cache:
                        self._cache[key].cleanup()
                    self._cache[key] = model
                return True
            else:
                self.logger.warning(
                    f"Training failed for {model_type.upper()} model {symbol}-{timeframe}"
                )
                return False
        except Exception as e:
            self.logger.error(
                f"Error training {model_type.upper()} model for {symbol}-{timeframe}: {e}",
                exc_info=True,
            )
            return False

    async def initialize_all_models(self):
        self._check_if_closed()
        self.logger.info("Initializing all models (LSTM, XGBoost)...")
        tasks = []
        model_files = []

        for model_type_dir in self.model_path.iterdir():
            if not model_type_dir.is_dir():
                continue

            model_type = model_type_dir.name
            if model_type == "lstm":
                ext = "keras"
            elif model_type == "xgboost":
                ext = "json"
            else:
                continue

            for file in model_type_dir.glob(f"*.{ext}"):
                model_files.append((file, model_type))

        for file, model_type in model_files:
            try:
                base_name = file.stem
                if model_type == "lstm":
                    parts_str = base_name.replace("lstm_", "")
                elif model_type == "xgboost":
                    parts_str = base_name.replace("xgboost_", "")
                else:
                    continue

                symbol, timeframe = parts_str.rsplit("_", 1)
                symbol_formatted = symbol.upper()
                if "/" not in symbol_formatted and len(symbol_formatted) > 4:
                    # Attempt to reformat symbol like 'BTCUSDT' to 'BTC/USDT'
                    if "USDT" in symbol_formatted:
                        symbol_formatted = symbol_formatted.replace("USDT", "/USDT", 1)
                    elif "BUSD" in symbol_formatted:
                        symbol_formatted = symbol_formatted.replace("BUSD", "/BUSD", 1)

                tasks.append(self.get_model(model_type, symbol_formatted, timeframe))
            except (IndexError, ValueError) as e:
                self.logger.warning(
                    f"Could not parse model file name: {file.name}. Error: {e}"
                )

        results = await asyncio.gather(*tasks, return_exceptions=True)
        for res in results:
            if isinstance(res, Exception):
                self.logger.error(
                    f"Error during model initialization: {res}", exc_info=res
                )
        self.logger.info(
            f"Model initialization finished. Loaded/Created {len(tasks)} models."
        )

    async def train_and_save_model(
        self, model_type: str, symbol: str, timeframe: str
    ) -> bool:
        self.logger.info(
            f"Attempting to train and save {model_type} model for {symbol}-{timeframe}"
        )
        data = await self.data_provider.get_data_for_model(symbol, timeframe)
        if data is None or data.empty:
            self.logger.warning(
                f"Could not fetch sufficient data for model training: {symbol}-{timeframe}"
            )
            return False

        return await self.train_model(model_type, symbol, timeframe, data)

    async def predict_with_confidence(
        self, model_type: str, symbol: str, timeframe: str
    ) -> Optional[Dict[str, float]]:
        self._check_if_closed()

        model = await self.get_model(model_type, symbol, timeframe)
        if not model:
            self.logger.error(
                f"Failed to get model for prediction: {model_type} on {symbol}-{timeframe}"
            )
            return None

        if not model.is_trained:
            self.logger.info(
                f"Model {model_type} for {symbol}-{timeframe} is not trained. Attempting to train now."
            )
            success = await self.train_and_save_model(model_type, symbol, timeframe)
            if not success:
                self.logger.warning(
                    f"Training failed for {model_type} on {symbol}-{timeframe}. Skipping prediction."
                )
                return None
            # Re-get the model to ensure we have the trained instance
            model = await self.get_model(model_type, symbol, timeframe)
            if not model or not model.is_trained:
                self.logger.error(
                    f"Model still not trained after attempt for {model_type} on {symbol}-{timeframe}."
                )
                return None

        data = await self.data_provider.get_data_for_model(
            symbol, timeframe, for_prediction=True
        )
        if data is None or data.empty:
            self.logger.warning(
                f"Empty data for prediction {model_type} on {symbol}-{timeframe}"
            )
            return None

        try:
            prediction_result = await self._run_in_executor(
                model.predict, data, executor_type="prediction"
            )
            if prediction_result is None or len(prediction_result) != 2:
                return None

            prediction, uncertainty = prediction_result
            if (
                prediction is None
                or len(prediction) == 0
                or np.isnan(prediction[0])
                or np.isinf(prediction[0])
            ):
                return None

            current_price = data["close"].iloc[-1]
            raw_confidence = 0.0
            if current_price > 0:
                expected_pct_change = (
                    model.feature_engineer.get_typical_volatility(timeframe) * 100
                )
                if expected_pct_change > 0:
                    price_change_pct = (
                        abs(prediction[0] - current_price) / current_price * 100
                    )
                    # A more sensible confidence based on expected volatility
                    raw_confidence = np.clip(
                        100
                        - (
                            abs(price_change_pct - expected_pct_change)
                            / expected_pct_change
                        )
                        * 100,
                        30,
                        95,
                    )
                else:
                    raw_confidence = (
                        50.0  # Default confidence if no volatility expected
                    )

            # Use the calibrator
            model_name = f"{model_type}_{symbol}_{timeframe}"
            calibrated_confidence = model.calibrator.get_calibrated_confidence(
                model_name, raw_confidence
            )

            final_confidence = max(
                0.0, min(100.0, calibrated_confidence * (1 - uncertainty**2))
            )

            return {
                "prediction": float(prediction[0]),
                "confidence": final_confidence,
                "raw_confidence": raw_confidence,
                "calibrated_confidence": calibrated_confidence,
                "uncertainty": uncertainty,
            }
        except (ObjectClosedError, CancelledError) as e:
            self.logger.warning(
                f"Prediction task cancelled or failed for {model_type} on {symbol}-{timeframe}: {e}"
            )
            return None
        except Exception as e:
            self.logger.error(
                f"Prediction failed for {model_type} on {symbol}-{timeframe}: {e}",
                exc_info=True,
            )
            return None

    async def record_signal_performance(
        self,
        model_type: str,
        symbol: str,
        timeframe: str,
        confidence: float,
        success: bool,
    ):
        self._check_if_closed()
        model = await self.get_model(model_type, symbol, timeframe)
        if model and model.calibrator:
            model_name = f"{model_type}_{symbol}_{timeframe}"
            model.calibrator.add_prediction(model_name, confidence, success)

    async def shutdown(self):
        if self._is_closed:
            return

        self._is_closed = True
        self.logger.info("Shutting down ModelManager...")

        async with self._lock:
            models_to_clean = list(self._cache.values())
            self._cache.clear()

        for model in models_to_clean:
            try:
                model.cleanup()
            except Exception as e:
                self.logger.error(
                    f"Error cleaning up model {model.symbol}-{model.timeframe}: {e}"
                )

        for executor_attr in ["_training_executor", "_prediction_executor"]:
            if hasattr(self, executor_attr):
                executor = getattr(self, executor_attr)
                if executor:
                    try:
                        # Use cancel_futures=True for a faster shutdown on exit
                        executor.shutdown(wait=False, cancel_futures=True)
                    except Exception as e:
                        self.logger.error(f"Error shutting down executor: {e}")

        self.logger.info("ModelManager shut down.")

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()


=====

[modeling\optimization.py] :

import asyncio
import pandas as pd
from typing import Optional

from backtesting.engine import BacktestingEngine
from config.logger import logger
from services.trading_service import TradingService
from data.data_provider import MarketDataProvider
from config.settings import ConfigManager
from utils.resource_manager import ResourceManager
from modeling.models import LSTMModel, XGBoostModel


class HyperparameterOptimizer:
    def __init__(
        self,
        symbol: str,
        timeframe: str,
        data: pd.DataFrame,
        model_type="lstm",
        n_trials=50,
    ):
        self.symbol = symbol
        self.timeframe = timeframe
        self.data = data
        self.model_type = model_type.lower()
        self.n_trials = n_trials
        self.loop = asyncio.get_running_loop()

        # Check if optuna is available
        try:
            import optuna

            self.optuna = optuna
        except ImportError:
            self.optuna = None
            logger.error(
                "Optuna is not installed. Please install it to use HyperparameterOptimizer: pip install optuna"
            )

    def _define_lstm_search_space(self, trial):
        return {
            "units": trial.suggest_int("units", 32, 256, step=32),
            "lr": trial.suggest_float("lr", 1e-5, 1e-2, log=True),
            "sequence_length": trial.suggest_int("sequence_length", 20, 120, step=10),
        }

    def _define_xgboost_search_space(self, trial):
        return {
            "n_estimators": trial.suggest_int("n_estimators", 50, 500, step=50),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        }

    async def _objective(self, trial):
        if not self.optuna:
            raise ImportError("Optuna is not installed.")

        resource_manager: Optional[ResourceManager] = None
        trading_service: Optional[TradingService] = None

        try:
            # Setup a temporary trading service for backtesting
            config_manager = ConfigManager()
            resource_manager = ResourceManager()
            await resource_manager.get_session()
            await resource_manager.get_redis_client()

            market_data_provider = MarketDataProvider(
                resource_manager=resource_manager, config_manager=config_manager
            )
            await market_data_provider.initialize()

            trading_service = TradingService(
                market_data_provider=market_data_provider,
                config_manager=config_manager,
                resource_manager=resource_manager,
            )

            if self.model_type == "lstm":
                params = self._define_lstm_search_space(trial)
                model = LSTMModel(
                    symbol=self.symbol, timeframe=self.timeframe, **params
                )
            elif self.model_type == "xgboost":
                params = self._define_xgboost_search_space(trial)
                model = XGBoostModel(
                    symbol=self.symbol, timeframe=self.timeframe, **params
                )
            else:
                raise ValueError("Unsupported model type")

            # Train the model with the suggested parameters
            is_trained = await self.loop.run_in_executor(None, model.fit, self.data)
            if not is_trained:
                logger.warning(f"Trial {trial.number}: Model training failed. Pruning.")
                raise self.optuna.exceptions.TrialPruned()

            # Inject the trained model into the signal generator
            trading_service.signal_generator.model_manager._cache[
                f"{self.model_type}-{self.symbol}-{self.timeframe}"
            ] = model

            # Run backtest
            backtester = BacktestingEngine(trading_service)
            start_date = self.data.index.min().strftime("%Y-%m-%d")
            end_date = self.data.index.max().strftime("%Y-%m-%d")

            results = backtester.run_backtest(
                symbol=self.symbol,
                timeframe=self.timeframe,
                start=start_date,
                end=end_date,
            )

            # Use Sharpe Ratio as the objective to maximize
            sharpe_ratio = results.get("sharpe_ratio", -1.0)

            if sharpe_ratio is None or pd.isna(sharpe_ratio):
                sharpe_ratio = -1.0

            trial.report(sharpe_ratio, step=0)

            if trial.should_prune():
                raise self.optuna.exceptions.TrialPruned()

            return sharpe_ratio
        except Exception as e:
            logger.error(f"Trial {trial.number} failed: {e}", exc_info=True)
            return -2.0  # Return a very low value for failed trials
        finally:
            # Cleanup resources
            if trading_service:
                await trading_service.cleanup()
            if resource_manager:
                await resource_manager.cleanup()

    async def run(self):
        if not self.optuna:
            return {}, -1.0

        study = self.optuna.create_study(
            direction="maximize", pruner=self.optuna.pruners.MedianPruner()
        )

        # Use run_in_executor to run the sync objective function in the event loop
        def sync_objective_wrapper(trial):
            future = asyncio.run_coroutine_threadsafe(self._objective(trial), self.loop)
            return future.result()

        # Run optimize in a separate thread to avoid blocking the event loop
        await self.loop.run_in_executor(
            None,
            lambda: study.optimize(
                sync_objective_wrapper, n_trials=self.n_trials, n_jobs=1
            ),
        )

        pruned_trials = study.get_trials(
            deepcopy=False, states=[self.optuna.trial.TrialState.PRUNED]
        )
        complete_trials = study.get_trials(
            deepcopy=False, states=[self.optuna.trial.TrialState.COMPLETE]
        )

        logger.info("Study statistics: ")
        logger.info(f"  Number of finished trials: {len(study.trials)}")
        logger.info(f"  Number of pruned trials: {len(pruned_trials)}")
        logger.info(f"  Number of complete trials: {len(complete_trials)}")

        if not complete_trials:
            logger.error("No trials were completed successfully.")
            return {}, -1.0

        logger.info("Best trial:")
        trial = study.best_trial

        logger.info(f"  Value (Sharpe Ratio): {trial.value}")
        logger.info("  Params: ")
        for key, value in trial.params.items():
            logger.info(f"    {key}: {value}")

        return trial.params, trial.value


=====

