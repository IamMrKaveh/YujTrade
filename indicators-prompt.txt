این فایل شامل تمام اندیکاتورهای موجود در پوشه indicators هست.

=====================

candlestick_patterns.py

from logger_config import logger
import pandas as pd
from typing import Dict, Optional, Union

from .cache_utils import cached_calculation


@cached_calculation('hammer_doji_patterns')
def _detect_hammer_doji_patterns(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """تشخیص الگوهای Hammer و Doji with caching"""
    try:
        if df is None or len(df) < 3:
            return None
        
        open_price = df['open']
        high = df['high']
        low = df['low']
        close = df['close']
        
        # محاسبه اجزای کندل
        body = abs(close - open_price)
        upper_shadow = high - pd.concat([open_price, close], axis=1).max(axis=1)
        lower_shadow = pd.concat([open_price, close], axis=1).min(axis=1) - low
        total_range = high - low
        
        patterns = pd.DataFrame(index=df.index)
        
        # Hammer Pattern
        hammer_condition = (
            (lower_shadow >= 2 * body) &
            (upper_shadow <= 0.1 * total_range) &
            (body <= 0.3 * total_range)
        )
        patterns['hammer'] = hammer_condition
        
        # Doji Pattern
        doji_condition = (body <= 0.1 * total_range)
        patterns['doji'] = doji_condition
        
        # Shooting Star Pattern
        shooting_star_condition = (
            (upper_shadow >= 2 * body) &
            (lower_shadow <= 0.1 * total_range) &
            (body <= 0.3 * total_range)
        )
        patterns['shooting_star'] = shooting_star_condition
        
        return patterns
    except Exception as e:
        logger.warning(f"Error detecting Hammer/Doji patterns: {e}")
        return None


@cached_calculation('engulfing_patterns')
def _detect_engulfing_patterns(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """تشخیص الگوهای Engulfing"""
    try:
        if df is None or len(df) < 2:
            return None
        
        open_price = df['open']
        close = df['close']
        
        patterns = pd.DataFrame(index=df.index)
        patterns['bullish_engulfing'] = False
        patterns['bearish_engulfing'] = False
        
        for i in range(1, len(df)):
            prev_open = open_price.iloc[i-1]
            prev_close = close.iloc[i-1]
            curr_open = open_price.iloc[i]
            curr_close = close.iloc[i]
            
            # Bullish Engulfing
            if (prev_close < prev_open and  # Previous red candle
                curr_close > curr_open and  # Current green candle
                curr_open < prev_close and  # Current opens below previous close
                curr_close > prev_open):    # Current closes above previous open
                patterns.iloc[i, patterns.columns.get_loc('bullish_engulfing')] = True
            
            # Bearish Engulfing
            if (prev_close > prev_open and  # Previous green candle
                curr_close < curr_open and  # Current red candle
                curr_open > prev_close and  # Current opens above previous close
                curr_close < prev_open):    # Current closes below previous open
                patterns.iloc[i, patterns.columns.get_loc('bearish_engulfing')] = True
        
        return patterns
    except Exception as e:
        logger.warning(f"Error detecting Engulfing patterns: {e}")
        return None


@cached_calculation('star_patterns')
def _detect_star_patterns(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """تشخیص الگوهای Morning/Evening Star"""
    try:
        if df is None or len(df) < 3:
            return None
        
        open_price = df['open']
        high = df['high']
        low = df['low']
        close = df['close']
        
        patterns = pd.DataFrame(index=df.index)
        patterns['morning_star'] = False
        patterns['evening_star'] = False
        
        for i in range(2, len(df)):
            # Morning Star Pattern
            first_red = close.iloc[i-2] < open_price.iloc[i-2]
            small_body = abs(close.iloc[i-1] - open_price.iloc[i-1]) < abs(close.iloc[i-2] - open_price.iloc[i-2]) * 0.3
            gap_down = high.iloc[i-1] < low.iloc[i-2]
            third_green = close.iloc[i] > open_price.iloc[i]
            closes_into_first = close.iloc[i] > (open_price.iloc[i-2] + close.iloc[i-2]) / 2
            
            if first_red and small_body and gap_down and third_green and closes_into_first:
                patterns.iloc[i, patterns.columns.get_loc('morning_star')] = True
            
            # Evening Star Pattern
            first_green = close.iloc[i-2] > open_price.iloc[i-2]
            gap_up = low.iloc[i-1] > high.iloc[i-2]
            third_red = close.iloc[i] < open_price.iloc[i]
            closes_into_first = close.iloc[i] < (open_price.iloc[i-2] + close.iloc[i-2]) / 2
            
            if first_green and small_body and gap_up and third_red and closes_into_first:
                patterns.iloc[i, patterns.columns.get_loc('evening_star')] = True
        
        return patterns
    except Exception as e:
        logger.warning(f"Error detecting Star patterns: {e}")
        return None


@cached_calculation('dark_cloud_cover')
def _detect_dark_cloud_cover(df: pd.DataFrame) -> Optional[pd.Series]:
    """تشخیص الگوی Dark Cloud Cover"""
    try:
        if df is None or len(df) < 2:
            return None
        
        open_price = df['open']
        high = df['high']
        close = df['close']
        
        patterns = pd.Series(False, index=df.index)
        
        for i in range(1, len(df)):
            # کندل اول: صعودی قوی
            first_bullish = close.iloc[i-1] > open_price.iloc[i-1]
            first_body = close.iloc[i-1] - open_price.iloc[i-1]
            
            # کندل دوم: نزولی
            second_bearish = close.iloc[i] < open_price.iloc[i]
            second_body = open_price.iloc[i] - close.iloc[i]
            
            # شرایط Dark Cloud Cover
            opens_above_prev_high = open_price.iloc[i] > high.iloc[i-1]
            closes_into_first_body = (close.iloc[i] < (open_price.iloc[i-1] + close.iloc[i-1]) / 2)
            significant_penetration = second_body > first_body * 0.5
            
            if (first_bullish and second_bearish and opens_above_prev_high and 
                closes_into_first_body and significant_penetration):
                patterns.iloc[i] = True
        
        return patterns
    except Exception as e:
        logger.warning(f"Error detecting Dark Cloud Cover: {e}")
        return None


@cached_calculation('piercing_line')
def _detect_piercing_line(df: pd.DataFrame) -> Optional[pd.Series]:
    """تشخیص الگوی Piercing Line"""
    try:
        if df is None or len(df) < 2:
            return None
        
        open_price = df['open']
        low = df['low']
        close = df['close']
        
        patterns = pd.Series(False, index=df.index)
        
        for i in range(1, len(df)):
            # کندل اول: نزولی قوی
            first_bearish = close.iloc[i-1] < open_price.iloc[i-1]
            first_body = open_price.iloc[i-1] - close.iloc[i-1]
            
            # کندل دوم: صعودی
            second_bullish = close.iloc[i] > open_price.iloc[i]
            second_body = close.iloc[i] - open_price.iloc[i]
            
            # شرایط Piercing Line
            opens_below_prev_low = open_price.iloc[i] < low.iloc[i-1]
            closes_into_first_body = (close.iloc[i] > (open_price.iloc[i-1] + close.iloc[i-1]) / 2)
            significant_penetration = second_body > first_body * 0.5
            
            if (first_bearish and second_bullish and opens_below_prev_low and 
                closes_into_first_body and significant_penetration):
                patterns.iloc[i] = True
        
        return patterns
    except Exception as e:
        logger.warning(f"Error detecting Piercing Line: {e}")
        return None


@cached_calculation('harami_patterns')
def _detect_harami_patterns(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """تشخیص الگوهای Harami"""
    try:
        if df is None or len(df) < 2:
            return None
        
        open_price = df['open']
        close = df['close']
        
        patterns = pd.DataFrame(index=df.index)
        patterns['bullish_harami'] = False
        patterns['bearish_harami'] = False
        
        for i in range(1, len(df)):
            # محاسبه اندازه بدنه کندل‌ها
            first_body_size = abs(close.iloc[i-1] - open_price.iloc[i-1])
            second_body_size = abs(close.iloc[i] - open_price.iloc[i])
            
            # کندل دوم باید در داخل کندل اول باشد
            first_max = max(open_price.iloc[i-1], close.iloc[i-1])
            first_min = min(open_price.iloc[i-1], close.iloc[i-1])
            second_max = max(open_price.iloc[i], close.iloc[i])
            second_min = min(open_price.iloc[i], close.iloc[i])
            
            is_inside = (second_max < first_max and second_min > first_min)
            is_smaller = second_body_size < first_body_size * 0.7
            
            # Bullish Harami
            first_bearish = close.iloc[i-1] < open_price.iloc[i-1]
            second_bullish = close.iloc[i] > open_price.iloc[i]
            
            if first_bearish and second_bullish and is_inside and is_smaller:
                patterns.iloc[i, patterns.columns.get_loc('bullish_harami')] = True
            
            # Bearish Harami
            first_bullish = close.iloc[i-1] > open_price.iloc[i-1]
            second_bearish = close.iloc[i] < open_price.iloc[i]
            
            if first_bullish and second_bearish and is_inside and is_smaller:
                patterns.iloc[i, patterns.columns.get_loc('bearish_harami')] = True
        
        return patterns
    except Exception as e:
        logger.warning(f"Error detecting Harami patterns: {e}")
        return None


def detect_all_candlestick_patterns(df: pd.DataFrame) -> Dict[str, Union[pd.DataFrame, pd.Series, None]]:
    """
    تشخیص تمام الگوهای کندل استیک موجود
    
    Args:
        df: DataFrame حاوی ستون‌های open, high, low, close
        
    Returns:
        Dict حاوی تمام الگوهای تشخیص داده شده
    """
    if df is None or df.empty:
        logger.warning("Input DataFrame is empty or None")
        return {}
    
    required_columns = ['open', 'high', 'low', 'close']
    if not all(col in df.columns for col in required_columns):
        logger.error(f"DataFrame must contain columns: {required_columns}")
        return {}
    
    logger.info("Starting candlestick pattern detection...")
    
    patterns = {}
    
    # تشخیص الگوهای تک کندلی
    hammer_doji_patterns = _detect_hammer_doji_patterns(df)
    if hammer_doji_patterns is not None:
        patterns.update({
            'hammer': hammer_doji_patterns['hammer'],
            'doji': hammer_doji_patterns['doji'],
            'shooting_star': hammer_doji_patterns['shooting_star']
        })
    
    # تشخیص الگوهای دو کندلی
    engulfing_patterns = _detect_engulfing_patterns(df)
    if engulfing_patterns is not None:
        patterns.update({
            'bullish_engulfing': engulfing_patterns['bullish_engulfing'],
            'bearish_engulfing': engulfing_patterns['bearish_engulfing']
        })
    
    harami_patterns = _detect_harami_patterns(df)
    if harami_patterns is not None:
        patterns.update({
            'bullish_harami': harami_patterns['bullish_harami'],
            'bearish_harami': harami_patterns['bearish_harami']
        })
    
    dark_cloud_cover = _detect_dark_cloud_cover(df)
    if dark_cloud_cover is not None:
        patterns['dark_cloud_cover'] = dark_cloud_cover
    
    piercing_line = _detect_piercing_line(df)
    if piercing_line is not None:
        patterns['piercing_line'] = piercing_line
    
    # تشخیص الگوهای سه کندلی
    star_patterns = _detect_star_patterns(df)
    if star_patterns is not None:
        patterns.update({
            'morning_star': star_patterns['morning_star'],
            'evening_star': star_patterns['evening_star']
        })
    
    detected_count = sum(1 for pattern in patterns.values() if pattern is not None and pattern.any())
    logger.info(f"Pattern detection completed. Found {detected_count} pattern types with signals.")
    
    return patterns


def get_pattern_summary(patterns: Dict[str, Union[pd.DataFrame, pd.Series, None]]) -> Dict[str, int]:
    """
    خلاصه‌ای از تعداد هر الگوی تشخیص داده شده
    
    Args:
        patterns: خروجی تابع detect_all_candlestick_patterns
        
    Returns:
        Dict حاوی نام الگو و تعداد رخدادهای آن
    """
    summary = {}
    
    for pattern_name, pattern_data in patterns.items():
        if pattern_data is not None:
            if isinstance(pattern_data, (pd.Series, pd.DataFrame)):
                count = int(pattern_data.sum()) if hasattr(pattern_data, 'sum') else 0
                summary[pattern_name] = count
            else:
                summary[pattern_name] = 0
        else:
            summary[pattern_name] = 0
    
    return summary


def get_recent_patterns(patterns: Dict[str, Union[pd.DataFrame, pd.Series, None]], 
                       last_n: int = 10) -> Dict[str, Union[pd.DataFrame, pd.Series, None]]:
    """
    الگوهای اخیر (n کندل آخر)
    
    Args:
        patterns: خروجی تابع detect_all_candlestick_patterns
        last_n: تعداد کندل‌های آخر برای بررسی
        
    Returns:
        Dict حاوی الگوهای اخیر
    """
    recent_patterns = {}
    
    for pattern_name, pattern_data in patterns.items():
        if pattern_data is not None and len(pattern_data) > 0:
            recent_data = pattern_data.tail(last_n)
            if recent_data.any():
                recent_patterns[pattern_name] = recent_data
    
    return recent_patterns


def filter_significant_patterns(patterns: Dict[str, Union[pd.DataFrame, pd.Series, None]], 
                               min_occurrences: int = 1) -> Dict[str, Union[pd.DataFrame, pd.Series, None]]:
    """
    فیلتر کردن الگوهای قابل توجه
    
    Args:
        patterns: خروجی تابع detect_all_candlestick_patterns
        min_occurrences: حداقل تعداد رخداد برای در نظر گیری الگو
        
    Returns:
        Dict حاوی الگوهای فیلتر شده
    """
    significant_patterns = {}
    
    for pattern_name, pattern_data in patterns.items():
        if pattern_data is not None:
            count = int(pattern_data.sum()) if hasattr(pattern_data, 'sum') else 0
            if count >= min_occurrences:
                significant_patterns[pattern_name] = pattern_data
    
    return significant_patterns

=====================

correlation.py

import pandas as pd
import numpy as np
from logger_config import logger
from .cache_utils import cached_calculation, NUMBA_AVAILABLE

if NUMBA_AVAILABLE:
    from numba import jit
else:
    def jit():
        def decorator(func):
            return func
        return decorator


@jit(nopython=True, cache=True)
def _fast_rolling_correlation(x, y, window):
    """محاسبه سریع همبستگی غلتان با numba"""
    n = len(x)
    result = np.full(n, np.nan)
    
    for i in range(window - 1, n):
        start_idx = i - window + 1
        x_window = x[start_idx:i + 1]
        y_window = y[start_idx:i + 1]
        
        x_mean = np.mean(x_window)
        y_mean = np.mean(y_window)
        
        numerator = np.sum((x_window - x_mean) * (y_window - y_mean))
        x_std = np.sqrt(np.sum((x_window - x_mean) ** 2))
        y_std = np.sqrt(np.sum((y_window - y_mean) ** 2))
        
        if x_std > 0 and y_std > 0:
            result[i] = numerator / (x_std * y_std)
    
    return result


def _create_btc_cache_key(df_sig, btc_sig, period):
    """ایجاد کلید کش برای همبستگی با بیت کوین"""
    return f"btc_corr_{df_sig}_{btc_sig}_{period}"


@cached_calculation("correlation_with_btc")
def _calculate_correlation_with_btc(df, btc_df, period=20, method='pearson', min_periods=None):
    """محاسبه همبستگی با بیت کوین با کشینگ بهینه"""
    try:
        if df is None or btc_df is None or len(df) < period or len(btc_df) < period:
            return None
        
        if min_periods is None:
            min_periods = period // 2
            
        # هم‌تراز کردن داده‌ها بر اساس زمان
        merged = pd.merge(df[['close']], btc_df[['close']], 
                        left_index=True, right_index=True, 
                        suffixes=('', '_btc'), how='inner')
        
        if len(merged) < period:
            return None
            
        # انتخاب روش محاسبه بر اساس در دسترس بودن numba
        if NUMBA_AVAILABLE and method == 'pearson':
            correlation_values = _fast_rolling_correlation(
                merged['close'].values, 
                merged['close_btc'].values, 
                period
            )
            correlation = pd.Series(correlation_values, index=merged.index)
        else:
            # محاسبه همبستگی غلتان با pandas
            correlation = merged['close'].rolling(
                window=period, 
                min_periods=min_periods
            ).corr(merged['close_btc'])
        
        return correlation
        
    except Exception as e:
        logger.error(f"Error calculating BTC correlation: {e}")
        return None


@cached_calculation("rolling_correlation")
def calculate_rolling_correlation(df1, df2, column1='close', column2='close', period=20, min_periods=None):
    """محاسبه همبستگی غلتان بین دو دیتافریم"""
    try:
        if df1 is None or df2 is None or len(df1) < period or len(df2) < period:
            return None
        
        if min_periods is None:
            min_periods = period // 2
            
        # هم‌تراز کردن داده‌ها
        merged = pd.merge(df1[[column1]], df2[[column2]], 
                        left_index=True, right_index=True, 
                        suffixes=('_1', '_2'), how='inner')
        
        if len(merged) < period:
            return None
        
        col1_name = f"{column1}_1" if column1 in df2.columns else column1
        col2_name = f"{column2}_2" if column2 in df1.columns else column2
        
        if NUMBA_AVAILABLE:
            correlation_values = _fast_rolling_correlation(
                merged[col1_name].values, 
                merged[col2_name].values, 
                period
            )
            correlation = pd.Series(correlation_values, index=merged.index)
        else:
            correlation = merged[col1_name].rolling(
                window=period, 
                min_periods=min_periods
            ).corr(merged[col2_name])
        
        return correlation
        
    except Exception as e:
        logger.error(f"Error calculating rolling correlation: {e}")
        return None


@cached_calculation("correlation_matrix")
def calculate_correlation_matrix(dataframes_dict, period=20, method='pearson'):
    """محاسبه ماتریس همبستگی برای چندین دارایی"""
    try:
        if not dataframes_dict or len(dataframes_dict) < 2:
            return None
        
        # استخراج داده‌های قیمت پایانی
        price_data = {}
        for name, df in dataframes_dict.items():
            if df is not None and 'close' in df.columns and len(df) >= period:
                price_data[name] = df['close']
        
        if len(price_data) < 2:
            return None
        
        # ایجاد دیتافریم ترکیبی
        combined_df = pd.DataFrame(price_data)
        combined_df = combined_df.dropna()
        
        if len(combined_df) < period:
            return None
        
        # محاسبه ماتریس همبستگی غلتان
        correlation_matrices = []
        
        for i in range(period - 1, len(combined_df)):
            window_data = combined_df.iloc[i - period + 1:i + 1]
            corr_matrix = window_data.corr(method=method)
            correlation_matrices.append({
                'timestamp': combined_df.index[i],
                'correlation_matrix': corr_matrix
            })
        
        return correlation_matrices
        
    except Exception as e:
        logger.error(f"Error calculating correlation matrix: {e}")
        return None


@cached_calculation("correlation_strength")
def calculate_correlation_strength(correlation_series, threshold=0.7):
    """محاسبه قدرت همبستگی و آمار مربوطه"""
    try:
        if correlation_series is None or len(correlation_series) == 0:
            return None
        
        # حذف مقادیر NaN
        clean_corr = correlation_series.dropna()
        
        if len(clean_corr) == 0:
            return None
        
        stats = {
            'mean_correlation': clean_corr.mean(),
            'std_correlation': clean_corr.std(),
            'max_correlation': clean_corr.max(),
            'min_correlation': clean_corr.min(),
            'current_correlation': clean_corr.iloc[-1] if len(clean_corr) > 0 else None,
            'strong_positive_ratio': (clean_corr > threshold).sum() / len(clean_corr),
            'strong_negative_ratio': (clean_corr < -threshold).sum() / len(clean_corr),
            'weak_correlation_ratio': (abs(clean_corr) < 0.3).sum() / len(clean_corr),
            'correlation_trend': 'increasing' if len(clean_corr) > 10 and clean_corr.tail(5).mean() > clean_corr.head(5).mean() else 'decreasing'
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"Error calculating correlation strength: {e}")
        return None


def get_correlation_signals(correlation_series, entry_threshold=0.8, exit_threshold=0.3):
    """تولید سیگنال‌های معاملاتی بر اساس همبستگی"""
    try:
        if correlation_series is None or len(correlation_series) == 0:
            return None
        
        signals = pd.Series(0, index=correlation_series.index)
        clean_corr = correlation_series.fillna(0)
        
        # سیگنال خرید: همبستگی قوی مثبت
        signals[clean_corr > entry_threshold] = 1
        
        # سیگنال فروش: همبستگی قوی منفی
        signals[clean_corr < -entry_threshold] = -1
        
        # خروج: همبستگی ضعیف
        signals[abs(clean_corr) < exit_threshold] = 0
        
        return signals
        
    except Exception as e:
        logger.error(f"Error generating correlation signals: {e}")
        return None


def comprehensive_correlation_analysis(
    primary_df,
    comparison_data,
    btc_df=None,
    period=20,
    method='pearson',
    correlation_threshold=0.7,
    signal_entry_threshold=0.8,
    signal_exit_threshold=0.3,
    min_periods=None,
    include_matrix=True,
    include_signals=True,
    include_strength_analysis=True
):
    """
    تابع جامع تحلیل همبستگی که از تمام توابع موجود استفاده می‌کند
    
    Parameters:
    -----------
    primary_df : pd.DataFrame
        دیتافریم اصلی (دارایی اول)
    comparison_data : dict or pd.DataFrame
        دارایی‌های مقایسه - می‌تواند دیتافریم یا دیکشنری از دیتافریم‌ها باشد
    btc_df : pd.DataFrame, optional
        دیتافریم بیت کوین برای تحلیل همبستگی با بازار
    period : int, default=20
        پریود محاسبه همبستگی غلتان
    method : str, default='pearson'
        روش محاسبه همبستگی
    correlation_threshold : float, default=0.7
        آستانه تعیین همبستگی قوی
    signal_entry_threshold : float, default=0.8
        آستانه ورود برای سیگنال‌های معاملاتی
    signal_exit_threshold : float, default=0.3
        آستانه خروج برای سیگنال‌های معاملاتی
    min_periods : int, optional
        حداقل دوره‌های مورد نیاز برای محاسبه
    include_matrix : bool, default=True
        شامل کردن ماتریس همبستگی چندگانه
    include_signals : bool, default=True
        شامل کردن سیگنال‌های معاملاتی
    include_strength_analysis : bool, default=True
        شامل کردن تحلیل قدرت همبستگی
    
    Returns:
    --------
    dict : نتایج کامل تحلیل همبستگی
    """
    
    try:
        logger.info(f"Starting comprehensive correlation analysis with period={period}")
        
        results = {
            'timestamp': pd.Timestamp.now(),
            'period': period,
            'method': method,
            'correlations': {},
            'btc_correlation': None,
            'correlation_matrix': None,
            'strength_analysis': {},
            'signals': {},
            'summary': {}
        }
        
        # بررسی صحت داده‌های ورودی
        if primary_df is None or len(primary_df) < period:
            logger.warning("Primary dataframe is insufficient for analysis")
            return results
        
        if min_periods is None:
            min_periods = max(period // 2, 5)
        
        # 1. محاسبه همبستگی با بیت کوین (در صورت وجود)
        if btc_df is not None:
            logger.info("Calculating BTC correlation")
            btc_correlation = _calculate_correlation_with_btc(
                primary_df, btc_df, period, method, min_periods
            )
            results['btc_correlation'] = btc_correlation
            
            if btc_correlation is not None and include_strength_analysis:
                results['strength_analysis']['btc'] = calculate_correlation_strength(
                    btc_correlation, correlation_threshold
                )
            
            if btc_correlation is not None and include_signals:
                results['signals']['btc'] = get_correlation_signals(
                    btc_correlation, signal_entry_threshold, signal_exit_threshold
                )
        
        # 2. محاسبه همبستگی با سایر دارایی‌ها
        if isinstance(comparison_data, pd.DataFrame):
            # تبدیل دیتافریم منفرد به دیکشنری
            comparison_data = {'comparison': comparison_data}
        
        if isinstance(comparison_data, dict):
            logger.info(f"Calculating correlations with {len(comparison_data)} assets")
            
            for asset_name, asset_df in comparison_data.items():
                if asset_df is None or len(asset_df) < period:
                    logger.warning(f"Insufficient data for asset: {asset_name}")
                    continue
                
                # محاسبه همبستگی غلتان
                correlation = calculate_rolling_correlation(
                    primary_df, asset_df, 'close', 'close', period, min_periods
                )
                
                if correlation is not None:
                    results['correlations'][asset_name] = correlation
                    
                    # تحلیل قدرت همبستگی
                    if include_strength_analysis:
                        strength = calculate_correlation_strength(
                            correlation, correlation_threshold
                        )
                        results['strength_analysis'][asset_name] = strength
                    
                    # تولید سیگنال‌های معاملاتی
                    if include_signals:
                        signals = get_correlation_signals(
                            correlation, signal_entry_threshold, signal_exit_threshold
                        )
                        results['signals'][asset_name] = signals
            
            # 3. محاسبه ماتریس همبستگی چندگانه
            if include_matrix and len(comparison_data) > 1:
                logger.info("Calculating correlation matrix")
                
                # اضافه کردن دیتافریم اصلی به مجموعه
                all_data = {'primary': primary_df}
                all_data.update(comparison_data)
                
                if btc_df is not None:
                    all_data['btc'] = btc_df
                
                correlation_matrix = calculate_correlation_matrix(
                    all_data, period, method
                )
                results['correlation_matrix'] = correlation_matrix
        
        # 4. ایجاد خلاصه تحلیل
        results['summary'] = _create_analysis_summary(results, correlation_threshold)
        
        logger.info("Comprehensive correlation analysis completed successfully")
        return results
        
    except Exception as e:
        logger.error(f"Error in comprehensive correlation analysis: {e}")
        return {
            'error': str(e),
            'timestamp': pd.Timestamp.now(),
            'period': period,
            'method': method
        }


def _create_analysis_summary(results, threshold=0.7):
    """ایجاد خلاصه تحلیل همبستگی"""
    try:
        summary = {
            'total_assets_analyzed': len(results.get('correlations', {})),
            'strong_correlations': 0,
            'weak_correlations': 0,
            'negative_correlations': 0,
            'current_correlations': {},
            'trend_analysis': {},
            'trading_signals_count': {}
        }
        
        # تحلیل همبستگی‌های فردی
        for asset_name, strength_data in results.get('strength_analysis', {}).items():
            if strength_data is None:
                continue
            
            current_corr = strength_data.get('current_correlation')
            if current_corr is not None:
                summary['current_correlations'][asset_name] = current_corr
                
                if abs(current_corr) > threshold:
                    summary['strong_correlations'] += 1
                elif abs(current_corr) < 0.3:
                    summary['weak_correlations'] += 1
                
                if current_corr < 0:
                    summary['negative_correlations'] += 1
                
                # ترند همبستگی
                trend = strength_data.get('correlation_trend')
                if trend:
                    summary['trend_analysis'][asset_name] = trend
        
        # تحلیل سیگنال‌های معاملاتی
        for asset_name, signals in results.get('signals', {}).items():
            if signals is not None:
                signal_counts = {
                    'buy_signals': (signals == 1).sum(),
                    'sell_signals': (signals == -1).sum(),
                    'neutral_signals': (signals == 0).sum()
                }
                summary['trading_signals_count'][asset_name] = signal_counts
        
        # اضافه کردن تحلیل بیت کوین
        if results.get('btc_correlation') is not None:
            btc_strength = results.get('strength_analysis', {}).get('btc')
            if btc_strength:
                summary['btc_correlation_strength'] = btc_strength.get('current_correlation')
                summary['btc_trend'] = btc_strength.get('correlation_trend')
        
        return summary
        
    except Exception as e:
        logger.error(f"Error creating analysis summary: {e}")
        return {'error': str(e)}


def quick_correlation_check(df1, df2, period=20, asset_name="Asset"):
    """بررسی سریع همبستگی بین دو دارایی"""
    try:
        correlation = calculate_rolling_correlation(df1, df2, period=period)
        
        if correlation is None:
            return None
        
        strength = calculate_correlation_strength(correlation)
        signals = get_correlation_signals(correlation)
        
        return {
            'asset_name': asset_name,
            'correlation_series': correlation,
            'strength_analysis': strength,
            'signals': signals,
            'current_correlation': correlation.iloc[-1] if len(correlation) > 0 else None
        }
        
    except Exception as e:
        logger.error(f"Error in quick correlation check: {e}")
        return None


def batch_correlation_analysis(primary_df, assets_dict, btc_df=None, period=20):
    """تحلیل همبستگی دسته‌ای برای چندین دارایی"""
    try:
        results = {}
        
        for asset_name, asset_df in assets_dict.items():
            logger.info(f"Analyzing correlation for {asset_name}")
            
            result = quick_correlation_check(
                primary_df, asset_df, period, asset_name
            )
            
            if result is not None:
                results[asset_name] = result
        
        # اضافه کردن تحلیل بیت کوین
        if btc_df is not None:
            btc_result = quick_correlation_check(
                primary_df, btc_df, period, "Bitcoin"
            )
            if btc_result is not None:
                results['Bitcoin'] = btc_result
        
        return results
        
    except Exception as e:
        logger.error(f"Error in batch correlation analysis: {e}")
        return {}

=====================

fibonacci.py

import numpy as np
import pandas as pd
from typing import Dict, Optional, Union, Tuple, Any
from .cache_utils import cached_calculation, NUMBA_AVAILABLE, jit
from logger_config import logger


@jit() if NUMBA_AVAILABLE else lambda x: x
def _calculate_fib_numba(high_values, low_values, lookback):
    """محاسبه سطوح فیبوناچی با Numba برای بهینه سازی سرعت"""
    n = len(high_values)
    fib_levels = np.zeros((n, 7))  # 7 سطح فیبوناچی
    
    fib_ratios = np.array([0.0, 0.236, 0.382, 0.5, 0.618, 0.786, 1.0])
    
    for i in range(lookback, n):
        start_idx = max(0, i - lookback)
        period_high = np.max(high_values[start_idx:i+1])
        period_low = np.min(low_values[start_idx:i+1])
        
        diff = period_high - period_low
        
        for j in range(7):
            fib_levels[i, j] = period_high - (diff * fib_ratios[j])
    
    return fib_levels


def _calculate_fib_standard(high_values, low_values, lookback):
    """محاسبه سطوح فیبوناچی بدون Numba"""
    n = len(high_values)
    fib_levels = np.zeros((n, 7))
    
    fib_ratios = [0.0, 0.236, 0.382, 0.5, 0.618, 0.786, 1.0]
    
    for i in range(lookback, n):
        start_idx = max(0, i - lookback)
        period_high = np.max(high_values[start_idx:i+1])
        period_low = np.min(low_values[start_idx:i+1])
        
        diff = period_high - period_low
        
        for j, ratio in enumerate(fib_ratios):
            fib_levels[i, j] = period_high - (diff * ratio)
    
    return fib_levels


@cached_calculation('fibonacci_levels')
def calculate_fibonacci_levels(df: pd.DataFrame, lookback: int = 50, price_columns: Optional[Dict[str, str]] = None) -> Optional[pd.DataFrame]:
    """
    محاسبه سطوح فیبوناچی با استفاده از کشینگ پیشرفته
    
    Parameters:
    -----------
    df : pd.DataFrame
        دیتافریم حاوی داده های قیمت
    lookback : int
        تعداد کندل های قبلی برای محاسبه بالاترین و پایین ترین قیمت
    price_columns : dict
        نام ستون های قیمت {'high': 'high', 'low': 'low'}
    
    Returns:
    --------
    pd.DataFrame
        دیتافریم حاوی سطوح فیبوناچی
    """
    try:
        if df is None or len(df) == 0:
            logger.warning("DataFrame is empty or None")
            return None
        
        if price_columns is None:
            price_columns = {'high': 'high', 'low': 'low'}
        
        required_columns = [price_columns['high'], price_columns['low']]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"Missing required columns: {missing_columns}")
            return None
        
        high_col = price_columns['high']
        low_col = price_columns['low']
        
        high_values = df[high_col].values
        low_values = df[low_col].values
        
        if NUMBA_AVAILABLE:
            fib_levels = _calculate_fib_numba(high_values, low_values, lookback)
        else:
            fib_levels = _calculate_fib_standard(high_values, low_values, lookback)
        
        fib_columns = ['fib_0', 'fib_23.6', 'fib_38.2', 'fib_50', 'fib_61.8', 'fib_78.6', 'fib_100']
        result_df = pd.DataFrame(fib_levels, columns=fib_columns, index=df.index)
        
        result_df['fib_range'] = result_df['fib_0'] - result_df['fib_100']
        result_df['fib_mid'] = (result_df['fib_0'] + result_df['fib_100']) / 2
        
        logger.debug(f"Fibonacci levels calculated successfully for {len(df)} rows with lookback {lookback}")
        return result_df
        
    except Exception as e:
        logger.error(f"Error calculating Fibonacci levels: {e}")
        return None


@cached_calculation('fibonacci_retracements')
def calculate_fibonacci_retracements(df: pd.DataFrame, swing_high_idx: int, swing_low_idx: int, 
                                   price_columns: Optional[Dict[str, str]] = None) -> Optional[Dict[str, float]]:
    """
    محاسبه سطوح بازگشت فیبوناچی بین دو نقطه مشخص
    
    Parameters:
    -----------
    df : pd.DataFrame
        دیتافریم حاوی داده های قیمت
    swing_high_idx : int
        ایندکس نقطه بالا
    swing_low_idx : int
        ایندکس نقطه پایین
    price_columns : dict
        نام ستون های قیمت
    
    Returns:
    --------
    dict
        دیکشنری حاوی سطوح بازگشت فیبوناچی
    """
    try:
        if price_columns is None:
            price_columns = {'high': 'high', 'low': 'low'}
        
        high_price = df.iloc[swing_high_idx][price_columns['high']]
        low_price = df.iloc[swing_low_idx][price_columns['low']]
        
        diff = high_price - low_price
        
        retracement_levels = {
            'swing_high': high_price,
            'swing_low': low_price,
            'fib_23.6': low_price + (diff * 0.236),
            'fib_38.2': low_price + (diff * 0.382),
            'fib_50.0': low_price + (diff * 0.5),
            'fib_61.8': low_price + (diff * 0.618),
            'fib_78.6': low_price + (diff * 0.786),
            'range': diff
        }
        
        return retracement_levels
        
    except Exception as e:
        logger.error(f"Error calculating Fibonacci retracements: {e}")
        return None


@cached_calculation('fibonacci_extensions')
def calculate_fibonacci_extensions(df: pd.DataFrame, swing_high_idx: int, swing_low_idx: int, 
                                 current_idx: int, price_columns: Optional[Dict[str, str]] = None) -> Optional[Dict[str, float]]:
    """
    محاسبه سطوح تمدید فیبوناچی
    
    Parameters:
    -----------
    df : pd.DataFrame
        دیتافریم حاوی داده های قیمت
    swing_high_idx : int
        ایندکس نقطه بالا
    swing_low_idx : int
        ایندکس نقطه پایین
    current_idx : int
        ایندکس نقطه فعلی
    price_columns : dict
        نام ستون های قیمت
    
    Returns:
    --------
    dict
        دیکشنری حاوی سطوح تمدید فیبوناچی
    """
    try:
        if price_columns is None:
            price_columns = {'close': 'close'}
        
        high_price = df.iloc[swing_high_idx]['high'] if 'high' in df.columns else df.iloc[swing_high_idx][price_columns['close']]
        low_price = df.iloc[swing_low_idx]['low'] if 'low' in df.columns else df.iloc[swing_low_idx][price_columns['close']]
        current_price = df.iloc[current_idx][price_columns['close']]
        
        diff = high_price - low_price
        
        extension_levels = {
            'current_price': current_price,
            'ext_0': current_price,
            'ext_23.6': current_price + (diff * 0.236),
            'ext_38.2': current_price + (diff * 0.382),
            'ext_61.8': current_price + (diff * 0.618),
            'ext_100': current_price + diff,
            'ext_127.2': current_price + (diff * 1.272),
            'ext_161.8': current_price + (diff * 1.618),
            'ext_261.8': current_price + (diff * 2.618)
        }
        
        return extension_levels
        
    except Exception as e:
        logger.error(f"Error calculating Fibonacci extensions: {e}")
        return None


def _find_swing_points(df: pd.DataFrame, lookback: int = 5, price_columns: Optional[Dict[str, str]] = None) -> Tuple[list, list]:
    """
    یافتن نقاط بالا و پایین (Swing Points) در داده های قیمت
    
    Parameters:
    -----------
    df : pd.DataFrame
        دیتافریم حاوی داده های قیمت
    lookback : int
        تعداد کندل های قبل و بعد برای تایید swing point
    price_columns : dict
        نام ستون های قیمت
    
    Returns:
    --------
    tuple
        (swing_highs, swing_lows) - لیست ایندکس های نقاط بالا و پایین
    """
    if price_columns is None:
        price_columns = {'high': 'high', 'low': 'low'}
    
    swing_highs = []
    swing_lows = []
    
    high_values = df[price_columns['high']].values
    low_values = df[price_columns['low']].values
    
    for i in range(lookback, len(df) - lookback):
        # بررسی swing high
        is_swing_high = True
        for j in range(i - lookback, i + lookback + 1):
            if j != i and high_values[j] >= high_values[i]:
                is_swing_high = False
                break
        
        if is_swing_high:
            swing_highs.append(i)
        
        # بررسی swing low
        is_swing_low = True
        for j in range(i - lookback, i + lookback + 1):
            if j != i and low_values[j] <= low_values[i]:
                is_swing_low = False
                break
        
        if is_swing_low:
            swing_lows.append(i)
    
    return swing_highs, swing_lows


def _validate_dataframe(df: pd.DataFrame, required_columns: list) -> bool:
    """
    اعتبارسنجی DataFrame و ستون های مورد نیاز
    
    Parameters:
    -----------
    df : pd.DataFrame
        دیتافریم برای بررسی
    required_columns : list
        لیست ستون های مورد نیاز
    
    Returns:
    --------
    bool
        True اگر DataFrame معتبر باشد
    """
    if df is None or len(df) == 0:
        logger.error("DataFrame is empty or None")
        return False
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logger.error(f"Missing required columns: {missing_columns}")
        return False
    
    return True


def fibonacci_complete_analysis(df: pd.DataFrame, 
                              lookback: int = 50,
                              swing_lookback: int = 5,
                              price_columns: Optional[Dict[str, str]] = None,
                              auto_detect_swings: bool = True,
                              swing_high_idx: Optional[int] = None,
                              swing_low_idx: Optional[int] = None,
                              current_idx: Optional[int] = None) -> Dict[str, Any]:
    """
    تابع اصلی برای تحلیل کامل فیبوناچی
    
    این تابع تمام انواع محاسبات فیبوناچی را انجام می‌دهد:
    - سطوح فیبوناچی متحرک
    - سطوح بازگشت فیبوناچی
    - سطوح تمدید فیبوناچی
    - تشخیص خودکار نقاط swing
    
    Parameters:
    -----------
    df : pd.DataFrame
        دیتافریم حاوی داده های قیمت (باید شامل ستون های high, low, close باشد)
    lookback : int, default=50
        تعداد کندل های قبلی برای محاسبه سطوح فیبوناچی متحرک
    swing_lookback : int, default=5
        تعداد کندل های قبل و بعد برای تشخیص swing points
    price_columns : dict, optional
        نام ستون های قیمت {'high': 'high', 'low': 'low', 'close': 'close'}
    auto_detect_swings : bool, default=True
        آیا swing points به صورت خودکار تشخیص داده شوند
    swing_high_idx : int, optional
        ایندکس دستی نقطه بالا (اگر auto_detect_swings=False)
    swing_low_idx : int, optional
        ایندکس دستی نقطه پایین (اگر auto_detect_swings=False)
    current_idx : int, optional
        ایندکس نقطه فعلی برای محاسبه extensions (پیش‌فرض: آخرین ردیف)
    
    Returns:
    --------
    dict
        دیکشنری شامل تمام نتایج تحلیل فیبوناچی:
        {
            'fibonacci_levels': pd.DataFrame,      # سطوح فیبوناچی متحرک
            'swing_points': dict,                  # نقاط swing تشخیص داده شده
            'retracements': list,                  # سطوح بازگشت برای هر جفت swing
            'extensions': list,                    # سطوح تمدید برای هر جفت swing
            'summary': dict,                       # خلاصه تحلیل
            'status': str                          # وضعیت انجام عملیات
        }
    
    Examples:
    ---------
    >>> # استفاده ساده با تشخیص خودکار swing points
    >>> result = fibonacci_complete_analysis(df)
    >>> 
    >>> # استفاده با پارامترهای سفارشی
    >>> result = fibonacci_complete_analysis(
    ...     df, 
    ...     lookback=30, 
    ...     swing_lookback=7,
    ...     price_columns={'high': 'High', 'low': 'Low', 'close': 'Close'}
    ... )
    >>> 
    >>> # استفاده با swing points دستی
    >>> result = fibonacci_complete_analysis(
    ...     df,
    ...     auto_detect_swings=False,
    ...     swing_high_idx=100,
    ...     swing_low_idx=50
    ... )
    """
    
    try:
        logger.info("Starting complete Fibonacci analysis")
        
        # تنظیم پیش‌فرض ستون های قیمت
        if price_columns is None:
            price_columns = {'high': 'high', 'low': 'low', 'close': 'close'}
        
        # اعتبارسنجی DataFrame
        required_columns = list(price_columns.values())
        if not _validate_dataframe(df, required_columns):
            return {'status': 'error', 'message': 'DataFrame validation failed'}
        
        # ایجاد دیکشنری نتایج
        results = {
            'fibonacci_levels': None,
            'swing_points': {'highs': [], 'lows': []},
            'retracements': [],
            'extensions': [],
            'summary': {},
            'status': 'success'
        }
        
        # 1. محاسبه سطوح فیبوناچی متحرک
        logger.debug("Calculating moving Fibonacci levels")
        results['fibonacci_levels'] = calculate_fibonacci_levels(df, lookback, price_columns)
        
        # 2. تشخیص یا استفاده از swing points
        if auto_detect_swings:
            logger.debug("Auto-detecting swing points")
            swing_highs, swing_lows = _find_swing_points(df, swing_lookback, price_columns)
            results['swing_points']['highs'] = swing_highs
            results['swing_points']['lows'] = swing_lows
        else:
            if swing_high_idx is not None and swing_low_idx is not None:
                results['swing_points']['highs'] = [swing_high_idx]
                results['swing_points']['lows'] = [swing_low_idx]
                swing_highs = [swing_high_idx]
                swing_lows = [swing_low_idx]
            else:
                logger.warning("Manual swing points not provided, skipping retracement/extension calculations")
                swing_highs, swing_lows = [], []
        
        # 3. محاسبه سطوح بازگشت برای هر جفت swing point
        if swing_highs and swing_lows:
            logger.debug("Calculating Fibonacci retracements")
            for high_idx in swing_highs:
                for low_idx in swing_lows:
                    if abs(high_idx - low_idx) > swing_lookback:  # فقط swing points معنادار
                        retracement = calculate_fibonacci_retracements(df, high_idx, low_idx, price_columns)
                        if retracement:
                            retracement['high_idx'] = high_idx
                            retracement['low_idx'] = low_idx
                            results['retracements'].append(retracement)
        
        # 4. محاسبه سطوح تمدید
        if swing_highs and swing_lows:
            logger.debug("Calculating Fibonacci extensions")
            current_idx = current_idx or len(df) - 1
            
            for high_idx in swing_highs:
                for low_idx in swing_lows:
                    if abs(high_idx - low_idx) > swing_lookback and current_idx > max(high_idx, low_idx):
                        extension = calculate_fibonacci_extensions(df, high_idx, low_idx, current_idx, price_columns)
                        if extension:
                            extension['high_idx'] = high_idx
                            extension['low_idx'] = low_idx
                            extension['current_idx'] = current_idx
                            results['extensions'].append(extension)
        
        # 5. ایجاد خلاصه تحلیل
        results['summary'] = {
            'total_rows': len(df),
            'lookback_period': lookback,
            'swing_lookback': swing_lookback,
            'swing_highs_count': len(results['swing_points']['highs']),
            'swing_lows_count': len(results['swing_points']['lows']),
            'retracements_count': len(results['retracements']),
            'extensions_count': len(results['extensions']),
            'fibonacci_levels_available': results['fibonacci_levels'] is not None,
            'analysis_method': 'auto_detect' if auto_detect_swings else 'manual_swings'
        }
        
        logger.info(f"Fibonacci analysis completed successfully. Found {len(results['swing_points']['highs'])} swing highs and {len(results['swing_points']['lows'])} swing lows")
        
        return results
        
    except Exception as e:
        logger.error(f"Error in complete Fibonacci analysis: {e}")
        return {
            'status': 'error',
            'message': str(e),
            'fibonacci_levels': None,
            'swing_points': {'highs': [], 'lows': []},
            'retracements': [],
            'extensions': [],
            'summary': {}
        }


# تابع سازگاری برای حفظ کد قدیمی
def _calculate_fibonacci_levels(df: pd.DataFrame, lookback: int = 50) -> Optional[pd.DataFrame]:
    """تابع سازگاری - استفاده از calculate_fibonacci_levels توصیه می شود"""
    return calculate_fibonacci_levels(df, lookback)

=====================

market_structure.py

from logger_config import logger
import numpy as np
import pandas as pd
from .support_resistance import _find_swing_points, _get_recent_high, _get_recent_low
from .cache_utils import _cached_indicator_calculation


def _detect_breaks(structure_breaks, current_price, swing_highs, swing_lows):
    """Detect bullish and bearish breaks"""
    # Check if current price breaks recent swing high (bullish break)
    recent_high = _get_recent_high(swing_highs)
    if recent_high and current_price > recent_high:
        structure_breaks.loc[structure_breaks.index[-1], 'bullish_break'] = True
    
    # Check if current price breaks recent swing low (bearish break)
    recent_low = _get_recent_low(swing_lows)
    if recent_low and current_price < recent_low:
        structure_breaks.loc[structure_breaks.index[-1], 'bearish_break'] = True


def _detect_market_structure_breaks(df, swing_strength=5):
    """تشخیص Market Structure Breaks"""
    try:
        if df is None or len(df) < swing_strength * 2:
            return None
        
        high = df['high']
        low = df['low']
        
        structure_breaks = pd.DataFrame(index=df.index)
        structure_breaks['bullish_break'] = False
        structure_breaks['bearish_break'] = False
        
        # Find swing highs and lows
        swing_highs, swing_lows = _find_swing_points(high, low, swing_strength)
        
        # Detect breaks
        current_price = df['close'].iloc[-1]
        _detect_breaks(structure_breaks, current_price, swing_highs, swing_lows)
        
        return structure_breaks
    except Exception as e:
        logger.warning(f"Error detecting Market Structure Breaks: {e}")
        return None


def _calculate_market_structure_score(df, lookback=20):
    """Calculate market structure quality score"""
    try:
        if df is None or len(df) < lookback:
            return 0
        
        recent_data = df.tail(lookback)
        highs = recent_data['high'].values
        lows = recent_data['low'].values
        closes = recent_data['close'].values
        
        # Higher highs and higher lows for uptrend
        higher_highs = sum(1 for i in range(1, len(highs)) if highs[i] > highs[i-1])
        higher_lows = sum(1 for i in range(1, len(lows)) if lows[i] > lows[i-1])
        
        # Lower highs and lower lows for downtrend
        lower_highs = sum(1 for i in range(1, len(highs)) if highs[i] < highs[i-1])
        lower_lows = sum(1 for i in range(1, len(lows)) if lows[i] < lows[i-1])
        
        # Price momentum consistency
        up_moves = sum(1 for i in range(1, len(closes)) if closes[i] > closes[i-1])
        down_moves = sum(1 for i in range(1, len(closes)) if closes[i] < closes[i-1])
        
        # Volume trend consistency
        if 'volume' in recent_data.columns:
            volumes = recent_data['volume'].values
            volume_trend = sum(1 for i in range(1, len(volumes)) if volumes[i] > volumes[i-1])
            volume_consistency = volume_trend / (len(volumes) - 1) if len(volumes) > 1 else 0.5
        else:
            volume_consistency = 0.5
        
        # Calculate structure strength
        uptrend_strength = (higher_highs + higher_lows) / (2 * (lookback - 1))
        downtrend_strength = (lower_highs + lower_lows) / (2 * (lookback - 1))
        
        # Momentum consistency
        momentum_consistency = max(up_moves, down_moves) / (len(closes) - 1) if len(closes) > 1 else 0.5
        
        # Final structure score
        if uptrend_strength > downtrend_strength:
            structure_score = (uptrend_strength * 0.4 + momentum_consistency * 0.4 + volume_consistency * 0.2) * 100
        else:
            structure_score = (downtrend_strength * 0.4 + momentum_consistency * 0.4 + volume_consistency * 0.2) * 100
        
        return min(structure_score, 100)
        
    except Exception:
        return 0


def _calculate_market_microstructure_internal(df, period):
    """Internal market microstructure calculation function"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        close = df['close']
        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)
        
        # Bid-Ask Spread Proxy
        spread_proxy = (high - low) / close
        avg_spread = spread_proxy.rolling(window=period).mean()
        
        # Market Depth Indicator
        price_impact = (high - low) / volume
        market_depth = price_impact.rolling(window=period).mean()
        
        # Order Flow Imbalance
        price_change = close.pct_change()
        volume_weighted_price_change = price_change * volume
        order_flow = volume_weighted_price_change.rolling(window=period).sum()
        
        # Liquidity Score
        liquidity_score = volume / (high - low)
        liquidity_score = liquidity_score.replace([np.inf, -np.inf], 0).fillna(0)
        avg_liquidity = liquidity_score.rolling(window=period).mean()
        
        return {
            'spread_proxy': avg_spread,
            'market_depth': market_depth,
            'order_flow': order_flow,
            'liquidity_score': avg_liquidity
        }
    except Exception as e:
        logger.warning(f"Error calculating market microstructure: {e}")
        return None


def _calculate_market_microstructure(df, period=20):
    """محاسبه ساختار میکرو بازار with caching"""
    return _cached_indicator_calculation(df, 'market_microstructure', _calculate_market_microstructure_internal, period)


def _detect_market_regime(df, lookback=50):
    """تشخیص رژیم بازار"""
    try:
        if df is None or len(df) < lookback:
            return None
            
        close = df['close']
        
        # محاسبه نوسانات
        returns = close.pct_change()
        volatility = returns.rolling(lookback).std() * np.sqrt(252)  # سالانه
        
        # محاسبه ترند
        sma_short = close.rolling(10).mean()
        sma_long = close.rolling(50).mean()
        trend = sma_short - sma_long
        
        # تعیین رژیم بازار
        regime = pd.Series(index=df.index, dtype=str)
        
        for i in range(lookback, len(df)):
            vol = volatility.iloc[i]
            tr = trend.iloc[i]
            
            if vol > volatility.rolling(lookback).quantile(0.75).iloc[i]:
                if tr > 0:
                    regime.iloc[i] = 'Bull_Volatile'
                else:
                    regime.iloc[i] = 'Bear_Volatile'
            else:
                if tr > 0:
                    regime.iloc[i] = 'Bull_Stable'
                else:
                    regime.iloc[i] = 'Bear_Stable'
        
        return regime
    except Exception:
        return None


def analyze_market_structure(df, swing_strength=5, lookback=20, microstructure_period=20, regime_lookback=50):
    """
    تحلیل جامع ساختار بازار شامل تمام اجزای مهم
    
    Args:
        df (pd.DataFrame): داده‌های قیمت شامل ستون‌های high, low, close و volume (اختیاری)
        swing_strength (int): قدرت شناسایی نقاط چرخش
        lookback (int): دوره بررسی برای محاسبه امتیاز ساختاری
        microstructure_period (int): دوره محاسبه میکروساختار
        regime_lookback (int): دوره بررسی برای تشخیص رژیم بازار
    
    Returns:
        dict: نتایج تحلیل شامل تمام بخش‌های مختلف
    """
    try:
        if df is None or len(df) < max(swing_strength * 2, lookback, microstructure_period, regime_lookback):
            logger.warning("Insufficient data for market structure analysis")
            return None
        
        # تشخیص شکست‌های ساختاری
        structure_breaks = _detect_market_structure_breaks(df, swing_strength)
        
        # محاسبه امتیاز کیفیت ساختار
        structure_score = _calculate_market_structure_score(df, lookback)
        
        # محاسبه میکروساختار بازار
        microstructure = _calculate_market_microstructure(df, microstructure_period)
        
        # تشخیص رژیم بازار
        market_regime = _detect_market_regime(df, regime_lookback)
        
        # جمع‌آوری نتایج
        analysis_results = {
            'structure_breaks': structure_breaks,
            'structure_score': structure_score,
            'microstructure': microstructure,
            'market_regime': market_regime,
            'current_regime': market_regime.iloc[-1] if market_regime is not None and len(market_regime) > 0 else None,
            'last_bullish_break': structure_breaks['bullish_break'].iloc[-1] if structure_breaks is not None else False,
            'last_bearish_break': structure_breaks['bearish_break'].iloc[-1] if structure_breaks is not None else False,
            'analysis_timestamp': pd.Timestamp.now()
        }
        
        # اضافه کردن خلاصه وضعیت بازار
        market_summary = _generate_market_summary(analysis_results)
        analysis_results['market_summary'] = market_summary
        
        logger.info(f"Market structure analysis completed successfully. Score: {structure_score:.2f}")
        return analysis_results
        
    except Exception as e:
        logger.error(f"Error in market structure analysis: {e}")
        return None


def _generate_market_summary(analysis_results):
    """تولید خلاصه وضعیت بازار بر اساس نتایج تحلیل"""
    try:
        summary = {
            'structure_quality': 'Unknown',
            'trend_direction': 'Unknown',
            'volatility_level': 'Unknown',
            'break_signal': 'None',
            'overall_sentiment': 'Neutral'
        }
        
        # ارزیابی کیفیت ساختار
        score = analysis_results.get('structure_score', 0)
        if score >= 70:
            summary['structure_quality'] = 'Strong'
        elif score >= 50:
            summary['structure_quality'] = 'Moderate'
        else:
            summary['structure_quality'] = 'Weak'
        
        # تعیین جهت ترند و سطح نوسانات
        current_regime = analysis_results.get('current_regime')
        if current_regime:
            if 'Bull' in current_regime:
                summary['trend_direction'] = 'Bullish'
                summary['overall_sentiment'] = 'Positive'
            elif 'Bear' in current_regime:
                summary['trend_direction'] = 'Bearish'
                summary['overall_sentiment'] = 'Negative'
            
            if 'Volatile' in current_regime:
                summary['volatility_level'] = 'High'
            else:
                summary['volatility_level'] = 'Low'
        
        # بررسی سیگنال‌های شکست
        if analysis_results.get('last_bullish_break'):
            summary['break_signal'] = 'Bullish Break'
            summary['overall_sentiment'] = 'Positive'
        elif analysis_results.get('last_bearish_break'):
            summary['break_signal'] = 'Bearish Break'
            summary['overall_sentiment'] = 'Negative'
        
        return summary
        
    except Exception as e:
        logger.warning(f"Error generating market summary: {e}")
        return {
            'structure_quality': 'Unknown',
            'trend_direction': 'Unknown',
            'volatility_level': 'Unknown',
            'break_signal': 'None',
            'overall_sentiment': 'Neutral'
        }


def get_market_structure_signals(df, sensitivity='medium'):
    """
    دریافت سیگنال‌های ساده و کاربردی برای معاملات
    
    Args:
        df (pd.DataFrame): داده‌های قیمت
        sensitivity (str): حساسیت تحلیل ('low', 'medium', 'high')
    
    Returns:
        dict: سیگنال‌های ساده شامل buy/sell/hold
    """
    try:
        # تنظیم پارامترها بر اساس حساسیت
        params = {
            'low': {'swing_strength': 8, 'lookback': 30, 'score_threshold': 60},
            'medium': {'swing_strength': 5, 'lookback': 20, 'score_threshold': 50},
            'high': {'swing_strength': 3, 'lookback': 15, 'score_threshold': 40}
        }
        
        config = params.get(sensitivity, params['medium'])
        
        # انجام تحلیل
        analysis = analyze_market_structure(
            df, 
            swing_strength=config['swing_strength'],
            lookback=config['lookback']
        )
        
        if not analysis:
            return {'signal': 'HOLD', 'confidence': 0, 'reason': 'Insufficient data'}
        
        # تولید سیگنال
        signal = 'HOLD'
        confidence = 0
        reason = 'No clear signal'
        
        score = analysis['structure_score']
        summary = analysis['market_summary']
        
        # سیگنال خرید
        if (analysis['last_bullish_break'] and 
            summary['trend_direction'] == 'Bullish' and 
            score >= config['score_threshold']):
            signal = 'BUY'
            confidence = min(score, 90)
            reason = 'Bullish break with strong structure'
        
        # سیگنال فروش
        elif (analysis['last_bearish_break'] and 
              summary['trend_direction'] == 'Bearish' and 
              score >= config['score_threshold']):
            signal = 'SELL'
            confidence = min(score, 90)
            reason = 'Bearish break with strong structure'
        
        # سیگنال نگهداری
        elif score < config['score_threshold']:
            signal = 'HOLD'
            confidence = 100 - score
            reason = 'Weak market structure'
        
        return {
            'signal': signal,
            'confidence': confidence,
            'reason': reason,
            'structure_score': score,
            'market_regime': analysis['current_regime'],
            'timestamp': pd.Timestamp.now()
        }
        
    except Exception as e:
        logger.error(f"Error generating market structure signals: {e}")
        return {'signal': 'HOLD', 'confidence': 0, 'reason': 'Analysis error'}

=====================

momentum_indicators.py

import sys
from numba import jit
import numpy as np
import pandas as pd
from logger_config import logger
from .cache_utils import NUMBA_AVAILABLE, cached_calculation
from .moving_averages import _cached_indicator_calculation

# Fix numpy compatibility issue
if not hasattr(np, 'NaN'):
    np.NaN = np.nan

# Now import pandas_ta after fixing numpy
try:
    import pandas_ta as ta
except ImportError as e:
    print(f"Error importing pandas_ta: {e}")
    print("Please install with: pip install pandas-ta==0.3.14b")
    sys.exit(1)


# =============================================================================
# CORE CALCULATION FUNCTIONS
# =============================================================================

@jit(nopython=True)
def _fast_rsi_calculation(prices, period):
    """محاسبه سریع RSI"""
    deltas = np.diff(prices)
    gains = np.where(deltas > 0, deltas, 0.0)
    losses = np.where(deltas < 0, -deltas, 0.0)
    
    avg_gains = np.empty(len(gains))
    avg_losses = np.empty(len(losses))
    
    # محاسبه اولیه
    avg_gains[period-1] = np.mean(gains[:period])
    avg_losses[period-1] = np.mean(losses[:period])
    
    # محاسبه نمایی
    alpha = 1.0 / period
    for i in range(period, len(gains)):
        avg_gains[i] = alpha * gains[i] + (1 - alpha) * avg_gains[i-1]
        avg_losses[i] = alpha * losses[i] + (1 - alpha) * avg_losses[i-1]
    
    rs = avg_gains[period-1:] / avg_losses[period-1:]
    rsi = 100 - (100 / (1 + rs))
    
    return rsi


def _calculate_rsi(df, period):
    """محاسبه RSI"""
    try:
        if df is None or len(df) < period + 1:
            return None
        
        if NUMBA_AVAILABLE:
            prices = df['close'].values.astype(np.float64)
            rsi_values = _fast_rsi_calculation(prices, period)
            # Pad with NaN values for consistency
            result = np.full(len(df), np.nan)
            result[period:] = rsi_values
            return pd.Series(result, index=df.index)
        else:
            # Use pandas_ta RSI for accurate calculation
            rsi_result = ta.rsi(df['close'], length=period)
            if rsi_result is not None:
                return rsi_result
            else:
                # Final fallback with correct EMA calculation
                delta = df['close'].diff()
                gain = (delta.where(delta > 0, 0)).ewm(span=period, adjust=False).mean()
                loss = (-delta.where(delta < 0, 0)).ewm(span=period, adjust=False).mean()
                rs = gain / loss
                result = 100 - (100 / (1 + rs))
                return result
        
    except Exception as e:
        logger.warning(f"Error in optimized RSI calculation: {e}")
        return None


def _calculate_optimized_rsi(df, period=14):
    """محاسبه بهینه RSI"""  
    return _cached_indicator_calculation(df, 'rsi', _calculate_rsi, period)


@cached_calculation('money_flow_index')
def _calculate_money_flow_index(df, period=14):
    """محاسبه Money Flow Index"""
    try:
        if df is None or len(df) < period + 1:
            return None
        
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        money_flow = typical_price * df['volume']
        
        price_diff = typical_price.diff()
        positive_flow = pd.Series(0.0, index=df.index)
        negative_flow = pd.Series(0.0, index=df.index)
        
        positive_flow[price_diff > 0] = money_flow[price_diff > 0]
        negative_flow[price_diff < 0] = money_flow[price_diff < 0]
        
        positive_mf = positive_flow.rolling(window=period).sum()
        negative_mf = negative_flow.rolling(window=period).sum()
        
        money_ratio = positive_mf / negative_mf
        mfi = 100 - (100 / (1 + money_ratio))
        
        return mfi
    except Exception as e:
        logger.warning(f"Error calculating MFI: {e}")
        return None


@cached_calculation('commodity_channel_index')
def _calculate_commodity_channel_index(df, period=20):
    """محاسبه Commodity Channel Index"""
    try:
        if df is None or len(df) < period:
            return None
        
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        sma_tp = typical_price.rolling(window=period).mean()
        mean_deviation = typical_price.rolling(window=period).apply(
            lambda x: np.mean(np.abs(x - x.mean()))
        )
        
        cci = (typical_price - sma_tp) / (0.015 * mean_deviation)
        
        return cci
    except Exception as e:
        logger.warning(f"Error calculating CCI: {e}")
        return None


@cached_calculation('williams_r')
def _calculate_williams_r(df, period=14):
    """محاسبه Williams %R"""
    try:
        if df is None or len(df) < period:
            return None
        
        highest_high = df['high'].rolling(window=period).max()
        lowest_low = df['low'].rolling(window=period).min()
        
        williams_r = -100 * ((highest_high - df['close']) / (highest_high - lowest_low))
        
        return williams_r
    except Exception as e:
        logger.warning(f"Error calculating Williams %R: {e}")
        return None


@cached_calculation('ultimate_oscillator')
def _calculate_ultimate_oscillator(df, period1=7, period2=14, period3=28):
    """محاسبه Ultimate Oscillator"""
    try:
        if df is None or len(df) < max(period1, period2, period3):
            return None
        
        high = df['high']
        low = df['low']
        close = df['close']
        prev_close = close.shift(1)
        
        # True Low = minimum of Low or previous Close
        true_low = pd.concat([low, prev_close], axis=1).min(axis=1)
        
        # Buying Pressure = Close - True Low
        buying_pressure = close - true_low
        
        # True Range calculation
        tr1 = high - low
        tr2 = abs(high - prev_close)
        tr3 = abs(low - prev_close)
        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        # Average calculations for 3 periods
        bp1 = buying_pressure.rolling(window=period1).sum()
        tr1_sum = true_range.rolling(window=period1).sum()
        
        bp2 = buying_pressure.rolling(window=period2).sum()
        tr2_sum = true_range.rolling(window=period2).sum()
        
        bp3 = buying_pressure.rolling(window=period3).sum()
        tr3_sum = true_range.rolling(window=period3).sum()
        
        # Ultimate Oscillator formula
        uo = 100 * (4 * (bp1 / tr1_sum) + 2 * (bp2 / tr2_sum) + (bp3 / tr3_sum)) / 7
        
        return uo
    except Exception as e:
        logger.warning(f"Error calculating Ultimate Oscillator: {e}")
        return None


@cached_calculation('rate_of_change')
def _calculate_rate_of_change(df, period=14):
    """محاسبه Rate of Change (ROC)"""
    try:
        if df is None or len(df) < period:
            return None
        
        close = df['close']
        roc = ((close - close.shift(period)) / close.shift(period)) * 100
        
        return roc
    except Exception as e:
        logger.warning(f"Error calculating ROC: {e}")
        return None


@cached_calculation('awesome_oscillator')
def _calculate_awesome_oscillator(df, fast_period=5, slow_period=34):
    """محاسبه Awesome Oscillator"""
    try:
        if df is None or len(df) < slow_period:
            return None
        
        median_price = (df['high'] + df['low']) / 2
        
        fast = median_price.rolling(window=fast_period).mean()
        slow = median_price.rolling(window=slow_period).mean()

        ao = fast - slow
        ao = ao.dropna()  # Remove NaN values
        
        if ao.empty:
            logger.warning("Awesome Oscillator calculation resulted in empty series")
            return None
        
        logger.info(f"Calculated Awesome Oscillator with {len(ao)} values")
        return ao
    except Exception as e:
        logger.warning(f"Error calculating Awesome Oscillator: {e}")
        return None


@cached_calculation('trix')
def _calculate_trix(df, period=14):
    """محاسبه TRIX"""
    try:
        if df is None or len(df) < period * 3:
            return None
        
        close = df['close']
        
        # Triple smoothed EMA
        ema1 = close.ewm(span=period).mean()
        ema2 = ema1.ewm(span=period).mean()
        ema3 = ema2.ewm(span=period).mean()
        
        # TRIX = Rate of change of triple smoothed EMA
        trix = ((ema3 - ema3.shift(1)) / ema3.shift(1)) * 10000
        
        return trix
    except Exception as e:
        logger.warning(f"Error calculating TRIX: {e}")
        return None


@cached_calculation('dpo')
def _calculate_dpo(df, period=20):
    """محاسبه Detrended Price Oscillator"""
    try:
        if df is None or len(df) < period:
            return None
        
        close = df['close']
        sma = close.rolling(window=period).mean()
        
        # DPO = Close - SMA shifted by (period/2 + 1)
        shift_period = int(period/2) + 1
        dpo = close - sma.shift(shift_period)
        
        dpo = dpo.dropna()  # Remove NaN values
        
        if dpo.empty:
            logger.warning("DPO calculation resulted in empty series")
            return None
        
        logger.info(f"Calculated DPO with {len(dpo)} values")
        
        return dpo
    except Exception as e:
        logger.warning(f"Error calculating DPO: {e}")
        return None


# =============================================================================
# MAIN TECHNICAL INDICATORS CALCULATOR
# =============================================================================

def calculate_technical_indicators(df, config=None):
    """
    تابع اصلی محاسبه تمام اندیکاتورهای تکنیکال
    
    Args:
        df (pandas.DataFrame): داده‌های قیمت با ستون‌های 'open', 'high', 'low', 'close', 'volume'
        config (dict, optional): پیکربندی پارامترها برای هر اندیکاتور
    
    Returns:
        dict: دیکشنری شامل تمام اندیکاتورهای محاسبه شده
    """
    
    if df is None or df.empty:
        logger.error("DataFrame is None or empty")
        return {}
    
    # تنظیم پیکربندی پیش‌فرض
    default_config = {
        'rsi_period': 14,
        'mfi_period': 14,
        'cci_period': 20,
        'williams_r_period': 14,
        'uo_periods': [7, 14, 28],
        'roc_period': 14,
        'ao_periods': [5, 34],
        'trix_period': 14,
        'dpo_period': 20
    }
    
    if config:
        default_config.update(config)
    
    indicators = {}
    
    try:
        # RSI (Relative Strength Index)
        logger.info("Calculating RSI...")
        indicators['rsi'] = _calculate_optimized_rsi(df, default_config['rsi_period'])
        
        # MFI (Money Flow Index)
        logger.info("Calculating MFI...")
        indicators['mfi'] = _calculate_money_flow_index(df, default_config['mfi_period'])
        
        # CCI (Commodity Channel Index)
        logger.info("Calculating CCI...")
        indicators['cci'] = _calculate_commodity_channel_index(df, default_config['cci_period'])
        
        # Williams %R
        logger.info("Calculating Williams %R...")
        indicators['williams_r'] = _calculate_williams_r(df, default_config['williams_r_period'])
        
        # Ultimate Oscillator
        logger.info("Calculating Ultimate Oscillator...")
        uo_periods = default_config['uo_periods']
        indicators['ultimate_oscillator'] = _calculate_ultimate_oscillator(
            df, uo_periods[0], uo_periods[1], uo_periods[2]
        )
        
        # ROC (Rate of Change)
        logger.info("Calculating ROC...")
        indicators['roc'] = _calculate_rate_of_change(df, default_config['roc_period'])
        
        # Awesome Oscillator
        logger.info("Calculating Awesome Oscillator...")
        ao_periods = default_config['ao_periods']
        indicators['awesome_oscillator'] = _calculate_awesome_oscillator(
            df, ao_periods[0], ao_periods[1]
        )
        
        # TRIX
        logger.info("Calculating TRIX...")
        indicators['trix'] = _calculate_trix(df, default_config['trix_period'])
        
        # DPO (Detrended Price Oscillator)
        logger.info("Calculating DPO...")
        indicators['dpo'] = _calculate_dpo(df, default_config['dpo_period'])
        
        # حذف اندیکاتورهای None
        indicators = {k: v for k, v in indicators.items() if v is not None}
        
        logger.info(f"Successfully calculated {len(indicators)} technical indicators")
        
    except Exception as e:
        logger.error(f"Error in calculate_technical_indicators: {e}")
        
    return indicators


def get_indicator_summary(indicators):
    """
    خلاصه‌ای از وضعیت اندیکاتورها
    
    Args:
        indicators (dict): دیکشنری اندیکاتورهای محاسبه شده
        
    Returns:
        dict: خلاصه وضعیت اندیکاتورها
    """
    
    summary = {
        'total_indicators': len(indicators),
        'successful_calculations': 0,
        'failed_calculations': 0,
        'indicator_status': {}
    }
    
    for name, values in indicators.items():
        if values is not None and not values.empty:
            summary['successful_calculations'] += 1
            summary['indicator_status'][name] = {
                'status': 'success',
                'data_points': len(values),
                'last_value': values.iloc[-1] if len(values) > 0 else None
            }
        else:
            summary['failed_calculations'] += 1
            summary['indicator_status'][name] = {
                'status': 'failed',
                'data_points': 0,
                'last_value': None
            }
    
    return summary


def export_indicators_to_dataframe(df, indicators):
    """
    ترکیب داده‌های اصلی با اندیکاتورها در یک DataFrame
    
    Args:
        df (pandas.DataFrame): داده‌های اصلی قیمت
        indicators (dict): دیکشنری اندیکاتورهای محاسبه شده
        
    Returns:
        pandas.DataFrame: DataFrame ترکیبی شامل قیمت‌ها و اندیکاتورها
    """
    
    result_df = df.copy()
    
    for name, values in indicators.items():
        if values is not None and not values.empty:
            # تنظیم اندازه سریز با DataFrame اصلی
            if len(values) != len(result_df):
                # ایجاد سری جدید با اندازه مناسب
                aligned_series = pd.Series(index=result_df.index, dtype=float)
                aligned_series.iloc[-len(values):] = values.values
                result_df[name] = aligned_series
            else:
                result_df[name] = values
    
    return result_df


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def validate_dataframe(df):
    """
    اعتبارسنجی DataFrame ورودی
    
    Args:
        df (pandas.DataFrame): DataFrame برای بررسی
        
    Returns:
        tuple: (is_valid, error_message)
    """
    
    if df is None:
        return False, "DataFrame is None"
    
    if df.empty:
        return False, "DataFrame is empty"
    
    required_columns = ['open', 'high', 'low', 'close', 'volume']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        return False, f"Missing required columns: {missing_columns}"
    
    if len(df) < 50:  # حداقل داده مورد نیاز
        return False, f"Insufficient data: {len(df)} rows (minimum 50 required)"
    
    return True, "DataFrame is valid"


def get_available_indicators():
    """
    لیست اندیکاتورهای موجود
    
    Returns:
        list: لیست نام اندیکاتورهای موجود
    """
    
    return [
        'rsi',
        'mfi', 
        'cci',
        'williams_r',
        'ultimate_oscillator',
        'roc',
        'awesome_oscillator',
        'trix',
        'dpo'
    ]

=====================

moving_averages.py

import numpy as np
import pandas as pd
from typing import Dict, Optional, Union, List
from logger_config import logger
from .cache_utils import cached_calculation, NUMBA_AVAILABLE, jit

# ==================== Core Calculation Functions ====================

@jit(nopython=True)
def _fast_sma(prices, period):
    """محاسبه سریع میانگین متحرک ساده"""
    result = np.empty(len(prices))
    result[:period-1] = np.nan
    for i in range(period-1, len(prices)):
        result[i] = np.mean(prices[i-period+1:i+1])
    return result

@jit(nopython=True)
def _fast_ema(prices, alpha):
    """محاسبه سریع میانگین متحرک نمایی"""
    result = np.empty(len(prices))
    result[0] = prices[0]
    for i in range(1, len(prices)):
        result[i] = alpha * prices[i] + (1 - alpha) * result[i-1]
    return result

@cached_calculation('sma')
def calculate_sma(df, column='close', period=20):
    """محاسبه بهینه میانگین متحرک ساده با کشینگ"""
    try:
        if df is None or len(df) < period:
            return None
            
        prices = df[column].values.astype(np.float64)
        if NUMBA_AVAILABLE:
            result = _fast_sma(prices, period)
        else:
            result = df[column].rolling(window=period).mean().values
        
        return pd.Series(result, index=df.index)
    except Exception as e:
        logger.warning(f"Error in SMA calculation: {e}")
        return None

@cached_calculation('ema')
def calculate_ema(df, column='close', period=20):
    """محاسبه میانگین متحرک نمایی با کشینگ"""
    try:
        if df is None or len(df) < period:
            return None
            
        prices = df[column].values.astype(np.float64)
        alpha = 2.0 / (period + 1)
        
        if NUMBA_AVAILABLE:
            result = _fast_ema(prices, alpha)
        else:
            result = df[column].ewm(span=period).mean().values
            
        return pd.Series(result, index=df.index)
    except Exception as e:
        logger.warning(f"Error in EMA calculation: {e}")
        return None

@cached_calculation('kama')
def calculate_kama(df, period=10, fast_sc=2, slow_sc=30):
    """محاسبه Kaufman Adaptive Moving Average با کشینگ"""
    try:
        if df is None or len(df) < period:
            return None
        
        close = df['close']
        
        # Efficiency Ratio
        change = abs(close - close.shift(period))
        volatility = abs(close - close.shift(1)).rolling(window=period).sum()
        er = change / volatility
        er = er.fillna(0)
        
        # Smoothing Constants
        fastest_sc = 2.0 / (fast_sc + 1)
        slowest_sc = 2.0 / (slow_sc + 1)
        sc = (er * (fastest_sc - slowest_sc) + slowest_sc) ** 2
        
        # KAMA calculation
        kama = []
        kama.append(close.iloc[0])  # First value
        
        for i in range(1, len(close)):
            kama.append(kama[i-1] + sc.iloc[i] * (close.iloc[i] - kama[i-1]))
        
        return pd.Series(kama, index=df.index)
    except Exception as e:
        logger.warning(f"Error calculating KAMA: {e}")
        return None

@cached_calculation('wma')
def calculate_wma(df, column='close', period=20):
    """محاسبه میانگین متحرک وزنی با کشینگ"""
    try:
        if df is None or len(df) < period:
            return None
            
        prices = df[column]
        weights = np.arange(1, period + 1)
        result = prices.rolling(window=period).apply(
            lambda x: np.dot(x, weights) / weights.sum(), raw=True
        )
        
        return result
    except Exception as e:
        logger.warning(f"Error in WMA calculation: {e}")
        return None

@cached_calculation('vwma')
def calculate_vwma(df, price_column='close', volume_column='volume', period=20):
    """محاسبه میانگین متحرک وزنی حجم با کشینگ"""
    try:
        if df is None or len(df) < period:
            return None
        
        if volume_column not in df.columns:
            logger.warning(f"Volume column '{volume_column}' not found in dataframe")
            return None
            
        prices = df[price_column]
        volumes = df[volume_column]
        
        # Calculate price * volume and rolling sums
        price_volume = prices * volumes
        price_volume_sum = price_volume.rolling(window=period).sum()
        volume_sum = volumes.rolling(window=period).sum()
        
        # Avoid division by zero
        result = price_volume_sum / volume_sum
        return result
    except Exception as e:
        logger.warning(f"Error in VWMA calculation: {e}")
        return None

# ==================== Validation Functions ====================

def _validate_dataframe(df: pd.DataFrame) -> bool:
    """اعتبارسنجی DataFrame ورودی"""
    if df is None:
        logger.error("DataFrame is None")
        return False
        
    if df.empty:
        logger.error("DataFrame is empty")
        return False
        
    if 'close' not in df.columns:
        logger.error("'close' column not found in DataFrame")
        return False
        
    return True

def _validate_parameters(params: Dict) -> bool:
    """اعتبارسنجی پارامترهای ورودی"""
    for key, value in params.items():
        if key.endswith('_period') and (not isinstance(value, int) or value <= 0):
            logger.error(f"Invalid period parameter: {key}={value}")
            return False
            
    return True

def _get_required_columns(ma_types: List[str]) -> List[str]:
    """تعیین ستون‌های مورد نیاز برای انواع میانگین متحرک"""
    required_columns = ['close']
    
    if 'vwma' in ma_types:
        required_columns.append('volume')
        
    return required_columns

# ==================== Main Function ====================

def calculate_moving_averages(
    df: pd.DataFrame,
    ma_types: Optional[Union[str, List[str]]] = None,
    sma_period: int = 20,
    ema_period: int = 20,
    kama_period: int = 10,
    kama_fast_sc: int = 2,
    kama_slow_sc: int = 30,
    wma_period: int = 20,
    vwma_period: int = 20,
    price_column: str = 'close',
    volume_column: str = 'volume'
) -> Dict[str, Optional[pd.Series]]:
    """
    محاسبه انواع مختلف میانگین متحرک برای داده‌های قیمت
    
    Args:
        df: DataFrame شامل داده‌های قیمت
        ma_types: انواع میانگین متحرک برای محاسبه ('sma', 'ema', 'kama', 'wma', 'vwma')
        sma_period: دوره زمانی برای SMA
        ema_period: دوره زمانی برای EMA
        kama_period: دوره زمانی برای KAMA
        kama_fast_sc: ضریب سریع KAMA
        kama_slow_sc: ضریب کند KAMA
        wma_period: دوره زمانی برای WMA
        vwma_period: دوره زمانی برای VWMA
        price_column: نام ستون قیمت
        volume_column: نام ستون حجم
        
    Returns:
        Dict شامل نتایج محاسبات میانگین متحرک
    """
    # Default moving average types
    if ma_types is None:
        ma_types = ['sma', 'ema']
    elif isinstance(ma_types, str):
        ma_types = [ma_types]
    
    # Validate inputs
    if not _validate_dataframe(df):
        return {}
    
    parameters = {
        'sma_period': sma_period,
        'ema_period': ema_period,
        'kama_period': kama_period,
        'wma_period': wma_period,
        'vwma_period': vwma_period
    }
    
    if not _validate_parameters(parameters):
        return {}
    
    # Check required columns
    required_columns = _get_required_columns(ma_types)
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logger.error(f"Missing required columns: {missing_columns}")
        return {}
    
    # Calculate moving averages
    results = {}
    
    try:
        if 'sma' in ma_types:
            logger.info(f"Calculating SMA with period {sma_period}")
            results['sma'] = calculate_sma(df, column=price_column, period=sma_period)
            
        if 'ema' in ma_types:
            logger.info(f"Calculating EMA with period {ema_period}")
            results['ema'] = calculate_ema(df, column=price_column, period=ema_period)
            
        if 'kama' in ma_types:
            logger.info(f"Calculating KAMA with period {kama_period}")
            results['kama'] = calculate_kama(
                df, period=kama_period, fast_sc=kama_fast_sc, slow_sc=kama_slow_sc
            )
            
        if 'wma' in ma_types:
            logger.info(f"Calculating WMA with period {wma_period}")
            results['wma'] = calculate_wma(df, column=price_column, period=wma_period)
            
        if 'vwma' in ma_types:
            logger.info(f"Calculating VWMA with period {vwma_period}")
            results['vwma'] = calculate_vwma(
                df, price_column=price_column, volume_column=volume_column, period=vwma_period
            )
    
    except Exception as e:
        logger.error(f"Error in moving averages calculation: {e}")
        return {}
    
    # Log successful calculations
    successful_calculations = [ma_type for ma_type, result in results.items() if result is not None]
    if successful_calculations:
        logger.info(f"Successfully calculated: {', '.join(successful_calculations)}")
    
    failed_calculations = [ma_type for ma_type, result in results.items() if result is None]
    if failed_calculations:
        logger.warning(f"Failed calculations: {', '.join(failed_calculations)}")
    
    return results

# ==================== Utility Functions ====================

def get_all_moving_averages(
    df: pd.DataFrame,
    period: int = 20,
    price_column: str = 'close',
    volume_column: str = 'volume'
) -> Dict[str, Optional[pd.Series]]:
    """
    محاسبه تمام انواع میانگین متحرک با یک دوره زمانی یکسان
    
    Args:
        df: DataFrame شامل داده‌های قیمت
        period: دوره زمانی مشترک
        price_column: نام ستون قیمت
        volume_column: نام ستون حجم
        
    Returns:
        Dict شامل تمام انواع میانگین متحرک
    """
    return calculate_moving_averages(
        df=df,
        ma_types=['sma', 'ema', 'kama', 'wma', 'vwma'],
        sma_period=period,
        ema_period=period,
        kama_period=period,
        wma_period=period,
        vwma_period=period,
        price_column=price_column,
        volume_column=volume_column
    )

def get_short_long_ma_signals(
    df: pd.DataFrame,
    ma_type: str = 'sma',
    short_period: int = 10,
    long_period: int = 20,
    price_column: str = 'close'
) -> Dict[str, pd.Series]:
    """
    محاسبه سیگنال‌های خرید/فروش بر اساس تقاطع میانگین متحرک کوتاه و بلند مدت
    
    Args:
        df: DataFrame شامل داده‌های قیمت
        ma_type: نوع میانگین متحرک
        short_period: دوره کوتاه مدت
        long_period: دوره بلند مدت
        price_column: نام ستون قیمت
        
    Returns:
        Dict شامل میانگین‌های کوتاه و بلند مدت و سیگنال‌ها
    """
    if ma_type not in ['sma', 'ema', 'wma']:
        logger.error(f"Unsupported MA type for signals: {ma_type}")
        return {}
    
    # Calculate short and long moving averages
    short_ma = calculate_moving_averages(
        df, [ma_type], **{f'{ma_type}_period': short_period}, price_column=price_column
    )[ma_type]
    
    long_ma = calculate_moving_averages(
        df, [ma_type], **{f'{ma_type}_period': long_period}, price_column=price_column
    )[ma_type]
    
    if short_ma is None or long_ma is None:
        logger.error("Failed to calculate moving averages for signals")
        return {}
    
    # Generate signals
    signals = pd.Series(0, index=df.index)  # 0: hold, 1: buy, -1: sell
    signals[short_ma > long_ma] = 1  # Buy signal
    signals[short_ma < long_ma] = -1  # Sell signal
    
    return {
        f'{ma_type}_short': short_ma,
        f'{ma_type}_long': long_ma,
        'signals': signals
    }

=====================

position_sizing.py

from logger_config import logger
from .cache_utils import cached_calculation, NUMBA_AVAILABLE, jit
import pandas as pd
import numpy as np
from typing import Dict, Optional, Tuple, Union, List
import talib
import warnings

warnings.filtererrors('ignore')

# توابع کمکی با Numba (اگر موجود باشد)
if NUMBA_AVAILABLE:
    @jit(nopython=True)
    def _calculate_volatility_score(returns, window=20):
        """محاسبه امتیاز نوسانات با استفاده از Numba"""
        if len(returns) < window:
            return 3.0
        
        vol = np.std(returns[-window:]) * np.sqrt(252)
        if vol > 0.4:
            return 8.0
        elif vol > 0.3:
            return 6.0
        elif vol > 0.2:
            return 4.0
        elif vol > 0.1:
            return 2.0
        else:
            return 1.0
    
    @jit(nopython=True)
    def _calculate_trend_strength_numba(prices, window=14):
        """محاسبه قدرت ترند با استفاده از Numba"""
        if len(prices) < window:
            return 50.0
        
        recent_prices = prices[-window:]
        slope = (recent_prices[-1] - recent_prices[0]) / window
        price_range = np.max(recent_prices) - np.min(recent_prices)
        
        if price_range == 0:
            return 50.0
        
        normalized_slope = slope / price_range * 100
        return min(max(50 + normalized_slope, 0), 100)
else:
    def _calculate_volatility_score(returns, window=20):
        """محاسبه امتیاز نوسانات بدون Numba"""
        if len(returns) < window:
            return 3.0
        
        vol = np.std(returns[-window:]) * np.sqrt(252)
        if vol > 0.4:
            return 8.0
        elif vol > 0.3:
            return 6.0
        elif vol > 0.2:
            return 4.0
        elif vol > 0.1:
            return 2.0
        else:
            return 1.0
    
    def _calculate_trend_strength_numba(prices, window=14):
        """محاسبه قدرت ترند بدون Numba"""
        if len(prices) < window:
            return 50.0
        
        recent_prices = prices[-window:]
        slope = (recent_prices[-1] - recent_prices[0]) / window
        price_range = np.max(recent_prices) - np.min(recent_prices)
        
        if price_range == 0:
            return 50.0
        
        normalized_slope = slope / price_range * 100
        return min(max(50 + normalized_slope, 0), 100)

@cached_calculation("market_conditions_analysis")
def _analyze_market_conditions(df: pd.DataFrame) -> Dict[str, float]:
    """تحلیل جامع شرایط بازار"""
    try:
        if len(df) < 50:
            return {
                'volatility_score': 3.0,
                'trend_strength': 50.0,
                'momentum_score': 50.0,
                'support_resistance_score': 50.0,
                'volume_analysis': 50.0
            }
        
        prices = df['close'].values
        returns = np.diff(np.log(prices))
        
        # 1. تحلیل نوسانات
        volatility_score = _calculate_volatility_score(returns)
        
        # 2. قدرت ترند
        trend_strength = _calculate_trend_strength_numba(prices)
        
        # 3. تحلیل مومنتوم با RSI
        try:
            rsi = talib.RSI(prices, timeperiod=14)
            rsi_current = rsi[-1] if not np.isnan(rsi[-1]) else 50
            if rsi_current > 70:
                momentum_score = 80
            elif rsi_current < 30:
                momentum_score = 20
            else:
                momentum_score = rsi_current
        except:
            momentum_score = 50.0
        
        # 4. تحلیل سطوح حمایت و مقاومت
        try:
            recent_high = np.max(prices[-20:])
            recent_low = np.min(prices[-20:])
            current_price = prices[-1]
            
            if recent_high != recent_low:
                position_in_range = (current_price - recent_low) / (recent_high - recent_low)
                if position_in_range > 0.8:
                    support_resistance_score = 80
                elif position_in_range < 0.2:
                    support_resistance_score = 20
                else:
                    support_resistance_score = 50
            else:
                support_resistance_score = 50.0
        except:
            support_resistance_score = 50.0
        
        # 5. تحلیل حجم
        volume_analysis = 50.0
        if 'volume' in df.columns:
            try:
                recent_volume = df['volume'].tail(10).mean()
                avg_volume = df['volume'].tail(50).mean()
                if recent_volume > avg_volume * 1.5:
                    volume_analysis = 75
                elif recent_volume < avg_volume * 0.5:
                    volume_analysis = 25
            except:
                pass
        
        return {
            'volatility_score': float(volatility_score),
            'trend_strength': float(trend_strength),
            'momentum_score': float(momentum_score),
            'support_resistance_score': float(support_resistance_score),
            'volume_analysis': float(volume_analysis)
        }
        
    except Exception as e:
        logger.error(f"Error in market conditions analysis: {e}")
        return {
            'volatility_score': 3.0,
            'trend_strength': 50.0,
            'momentum_score': 50.0,
            'support_resistance_score': 50.0,
            'volume_analysis': 50.0
        }

@cached_calculation("kelly_criterion")
def _calculate_kelly_criterion(df: pd.DataFrame, win_rate: float, avg_win: float, avg_loss: float) -> float:
    """محاسبه معیار کلی برای اندازه پوزیشن بهینه"""
    try:
        if avg_loss == 0 or win_rate <= 0 or win_rate >= 1:
            return 0.0
        
        b = avg_win / abs(avg_loss)
        q = 1 - win_rate
        kelly_fraction = (b * win_rate - q) / b
        
        return max(0, min(kelly_fraction, 0.25))
        
    except Exception as e:
        logger.error(f"Error in Kelly Criterion calculation: {e}")
        return 0.1

@cached_calculation("correlation_analysis")
def _analyze_correlation_risk(df: pd.DataFrame, other_positions: Optional[List] = None) -> float:
    """تحلیل ریسک همبستگی با سایر پوزیشن‌ها"""
    try:
        if not other_positions:
            return 1.0
        
        correlations = []
        current_returns = df['close'].pct_change().dropna()
        
        for position_data in other_positions:
            if isinstance(position_data, pd.Series):
                other_returns = position_data.pct_change().dropna()
                if len(other_returns) >= 10 and len(current_returns) >= 10:
                    min_len = min(len(current_returns), len(other_returns))
                    corr = np.corrcoef(
                        current_returns.tail(min_len),
                        other_returns.tail(min_len)
                    )[0, 1]
                    if not np.isnan(corr):
                        correlations.append(abs(corr))
        
        if correlations:
            max_correlation = max(correlations)
            return max(0.5, 1 - max_correlation * 0.5)
        
        return 1.0
        
    except Exception as e:
        logger.error(f"Error in correlation analysis: {e}")
        return 1.0

def _calculate_base_position_size(capital: float, risk_percent: float, entry_price: float, stop_loss: float) -> float:
    """محاسبه اندازه پوزیشن پایه"""
    base_risk = capital * (risk_percent / 100)
    price_diff = abs(entry_price - stop_loss)
    
    if price_diff == 0:
        return 0
    
    return base_risk / price_diff

def _apply_market_adjustments(base_size: float, market_conditions: Dict[str, float]) -> float:
    """اعمال تعدیلات بر اساس شرایط بازار"""
    adjustment_factor = 1.0
    
    # تعدیل بر اساس نوسانات
    volatility_score = market_conditions.get('volatility_score', 3)
    if volatility_score > 6:
        adjustment_factor *= 0.6
    elif volatility_score > 4:
        adjustment_factor *= 0.8
    elif volatility_score < 2:
        adjustment_factor *= 1.2
    
    # تعدیل بر اساس قدرت ترند
    trend_strength = market_conditions.get('trend_strength', 50)
    if trend_strength > 75:
        adjustment_factor *= 1.3
    elif trend_strength < 25:
        adjustment_factor *= 0.7
    
    # تعدیل بر اساس مومنتوم
    momentum_score = market_conditions.get('momentum_score', 50)
    if momentum_score > 75 or momentum_score < 25:
        adjustment_factor *= 0.9
    
    return base_size * adjustment_factor

def _apply_safety_limits(position_size: float, capital: float, entry_price: float) -> float:
    """اعمال محدودیت‌های امنیتی"""
    max_position_value = capital * 0.15  # حداکثر 15% سرمایه
    max_position_size = max_position_value / entry_price
    
    return min(max(position_size, 0), max_position_size)

def calculate_optimal_position_size(
    df: pd.DataFrame,
    capital: float,
    risk_percent: float = 2.0,
    entry_price: float = None,
    stop_loss: float = None,
    win_rate: Optional[float] = None,
    avg_win: Optional[float] = None,
    avg_loss: Optional[float] = None,
    other_positions: Optional[List] = None,
    use_atr: bool = False,
    atr_multiplier: float = 2.0
) -> Dict[str, Union[float, Dict]]:
    """
    تابع اصلی برای محاسبه اندازه پوزیشن بهینه
    
    Args:
        df: دیتافریم حاوی داده‌های قیمت
        capital: سرمایه کل
        risk_percent: درصد ریسک (پیش‌فرض 2%)
        entry_price: قیمت ورود
        stop_loss: قیمت استاپ لاس
        win_rate: نرخ برد (اختیاری)
        avg_win: متوسط سود (اختیاری)
        avg_loss: متوسط ضرر (اختیاری)
        other_positions: لیست سایر پوزیشن‌ها برای تحلیل همبستگی
        use_atr: استفاده از ATR برای محاسبه استاپ لاس
        atr_multiplier: ضریب ATR
    
    Returns:
        دیکشنری حاوی اطلاعات کامل اندازه پوزیشن
    """
    try:
        # بررسی صحت داده‌های ورودی
        if len(df) == 0 or capital <= 0:
            return _get_default_result()
        
        # تنظیم قیمت ورود پیش‌فرض
        if entry_price is None:
            entry_price = df['close'].iloc[-1]
        
        # محاسبه ATR اگر درخواست شده باشد
        if use_atr and stop_loss is None:
            try:
                atr_values = talib.ATR(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)
                current_atr = atr_values[-1] if not np.isnan(atr_values[-1]) else entry_price * 0.02
                stop_loss = entry_price - (current_atr * atr_multiplier)
            except:
                stop_loss = entry_price * 0.98  # 2% پیش‌فرض
        
        # تنظیم استاپ لاس پیش‌فرض
        if stop_loss is None:
            stop_loss = entry_price * 0.98  # 2% زیر قیمت ورود
        
        # تحلیل شرایط بازار
        market_conditions = _analyze_market_conditions(df)
        
        # محاسبه اندازه پوزیشن پایه
        base_position_size = _calculate_base_position_size(capital, risk_percent, entry_price, stop_loss)
        
        if base_position_size == 0:
            return _get_default_result()
        
        # اعمال تعدیلات بازار
        adjusted_position_size = _apply_market_adjustments(base_position_size, market_conditions)
        
        # محاسبه ضریب کلی
        kelly_factor = 1.0
        if all([win_rate, avg_win, avg_loss]):
            kelly_fraction = _calculate_kelly_criterion(df, win_rate, avg_win, avg_loss)
            kelly_factor = kelly_fraction / (risk_percent / 100) if risk_percent > 0 else 1.0
            kelly_factor = min(kelly_factor, 2.0)
        
        # تحلیل همبستگی
        correlation_factor = _analyze_correlation_risk(df, other_positions)
        
        # اعمال تمام ضرایب
        final_adjustment_factor = kelly_factor * correlation_factor
        final_position_size = adjusted_position_size * final_adjustment_factor
        
        # اعمال محدودیت‌های امنیتی
        safe_position_size = _apply_safety_limits(final_position_size, capital, entry_price)
        
        # محاسبه نتایج نهایی
        price_diff = abs(entry_price - stop_loss)
        final_risk = safe_position_size * price_diff
        position_value = safe_position_size * entry_price
        
        return {
            'position_size': round(safe_position_size, 4),
            'position_value': round(position_value, 2),
            'risk_amount': round(final_risk, 2),
            'risk_percent': round((final_risk / capital) * 100, 3),
            'entry_price': entry_price,
            'stop_loss': stop_loss,
            'kelly_factor': round(kelly_factor, 3),
            'correlation_factor': round(correlation_factor, 3),
            'final_adjustment_factor': round(final_adjustment_factor, 3),
            'market_analysis': market_conditions,
            'recommendations': _generate_recommendations(market_conditions, safe_position_size, capital)
        }
        
    except Exception as e:
        logger.error(f"Error in calculate_optimal_position_size: {e}")
        return _get_default_result()

def _get_default_result() -> Dict[str, Union[float, Dict]]:
    """نتیجه پیش‌فرض در صورت بروز خطا"""
    return {
        'position_size': 0,
        'position_value': 0,
        'risk_amount': 0,
        'risk_percent': 0,
        'entry_price': 0,
        'stop_loss': 0,
        'kelly_factor': 1.0,
        'correlation_factor': 1.0,
        'final_adjustment_factor': 1.0,
        'market_analysis': {},
        'recommendations': []
    }

def _generate_recommendations(market_conditions: Dict[str, float], position_size: float, capital: float) -> List[str]:
    """تولید توصیه‌های معاملاتی بر اساس تحلیل"""
    recommendations = []
    
    volatility = market_conditions.get('volatility_score', 3)
    trend_strength = market_conditions.get('trend_strength', 50)
    momentum = market_conditions.get('momentum_score', 50)
    
    if volatility > 6:
        recommendations.append("نوسانات بالا - اندازه پوزیشن کاهش یافته")
    
    if trend_strength > 75:
        recommendations.append("ترند قوی شناسایی شد - فرصت مناسب برای معامله")
    elif trend_strength < 25:
        recommendations.append("ترند ضعیف - احتیاط در معامله")
    
    if momentum > 75:
        recommendations.append("اشباع خرید - احتمال تصحیح قیمت")
    elif momentum < 25:
        recommendations.append("اشباع فروش - احتمال بازگشت قیمت")
    
    position_percent = (position_size * market_conditions.get('entry_price', 1)) / capital * 100
    if position_percent > 10:
        recommendations.append("اندازه پوزیشن بالا - مراقب مدیریت ریسک باشید")
    
    return recommendations

# توابع کمکی برای سازگاری با کد قبلی
def adaptive_position_sizing(capital, risk_percent, entry_price, stop_loss, market_conditions=None):
    """تابع سازگاری برای محاسبه تطبیقی اندازه پوزیشن"""
    dummy_df = pd.DataFrame({'close': [entry_price]})
    result = calculate_optimal_position_size(
        df=dummy_df,
        capital=capital,
        risk_percent=risk_percent,
        entry_price=entry_price,
        stop_loss=stop_loss
    )
    return result['position_size']

def calculate_position_size_atr(capital, risk_percent, atr_value, atr_multiplier=2):
    """تابع سازگاری برای محاسبه اندازه پوزیشن بر اساس ATR"""
    try:
        risk_amount = capital * (risk_percent / 100)
        stop_distance = atr_value * atr_multiplier
        position_size = risk_amount / stop_distance
        return min(position_size, capital * 0.1)
    except Exception as e:
        logger.error(f"Error in calculate_position_size_atr: {e}")
        return 0

=====================

signal_processing.py

import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import StandardScaler
from .volume_indicators import _check_volume_filter
from .cache_utils import cached_calculation, NUMBA_AVAILABLE, jit
from logger_config import logger


@cached_calculation("ensemble_signal_scoring")
def _ensemble_signal_scoring(df, signals_dict, weights=None):
	"""ترکیب چندین سیگنال با وزن‌دهی"""
	try:
		if not signals_dict:
			return 0
			
		if weights is None:
			weights = dict.fromkeys(signals_dict.keys(), 1)
		
		total_score = 0
		total_weight = 0
		
		for signal_name, signal_value in signals_dict.items():
			if signal_name in weights and signal_value is not None:
				weight = weights[signal_name]
				total_score += signal_value * weight
				total_weight += weight
		
		return total_score / total_weight if total_weight > 0 else 0
	except Exception as e:
		logger.error(f"Error in ensemble signal scoring: {e}")
		return 0
	
@cached_calculation("adaptive_threshold_calculator")
def _adaptive_threshold_calculator(df, indicator_values, percentile_low=20, percentile_high=80):
	"""محاسبه آستانه‌های تطبیقی با ابزارهای پیشرفته"""
	try:
		if df is None or indicator_values is None:
			return {'low': 30, 'high': 70}
			
		# استفاده از scipy برای محاسبات آماری پیشرفته
		clean_values = indicator_values.dropna()
		if len(clean_values) == 0:
			return {'low': 30, 'high': 70}
		
		# محاسبه آستانه‌ها با روش‌های مختلف
		low_threshold = np.percentile(clean_values, percentile_low)
		high_threshold = np.percentile(clean_values, percentile_high)
		
		# استفاده از Z-score برای تشخیص نقاط غیرعادی
		z_scores = np.abs(stats.zscore(clean_values))
		outlier_threshold = 2.0
		
		# تنظیم آستانه‌ها بر اساس نقاط غیرعادی
		if np.any(z_scores > outlier_threshold):
			median_val = np.median(clean_values)
			mad = stats.median_abs_deviation(clean_values)
			low_threshold = max(low_threshold, median_val - 2 * mad)
			high_threshold = min(high_threshold, median_val + 2 * mad)
		
		return {
			'low': low_threshold,
			'high': high_threshold,
			'median': np.median(clean_values),
			'std': np.std(clean_values)
		}
	except Exception:
		return {'low': 30, 'high': 70}

def _extract_signal_type(signal_data):
	"""Extract signal type from signal data"""
	if isinstance(signal_data, dict):
		return signal_data.get('type', 'neutral')
	elif isinstance(signal_data, str):
		return signal_data.lower()
	else:
		return 'neutral'

@cached_calculation("check_trend_filter")
def _check_trend_filter(df, signal_data, min_trend_strength):
	"""Check if trend strength supports the signal"""
	if len(df) < 10:
		return True
	
	recent_closes = df['close'].tail(10).values
	if len(recent_closes) == 0 or recent_closes[0] == 0:
		return True
	
	trend_strength = (recent_closes[-1] - recent_closes[0]) / recent_closes[0]
	signal_type = _extract_signal_type(signal_data)
	
	if (signal_type == 'buy' and trend_strength < -min_trend_strength) or (signal_type == 'sell' and trend_strength > min_trend_strength):
		return False
	
	return True

@cached_calculation("filter_false_signals")
def _filter_false_signals(df, signal_data, min_volume_ratio=1.2, min_trend_strength=0.1):
	"""فیلتر سیگنال‌های کاذب"""
	try:
		if df is None or not signal_data:
			return False
		
		if not _check_volume_filter(df, min_volume_ratio):
			return False
		
		if not _check_trend_filter(df, signal_data, min_trend_strength):
			return False
		
		return True
	except Exception:
		return True

=====================

stop_loss_take_profit.py

from .cache_utils import _cached_indicator_calculation
from .support_resistance import _find_nearest_support, _find_nearest_resistance
from .volatility_indicators import _calculate_atr
from logger_config import logger

def calculate_comprehensive_risk_management(df, entry_price, position_type='long', 
                                            current_price=None, base_risk=2.0, 
                                            atr_multiplier=2.0, min_risk_reward_ratio=1.5,
                                            use_support_resistance=False, support_resistance_data=None):
    """
    تابع جامع مدیریت ریسک که تمام محاسبات حد ضرر و حد سود را انجام می‌دهد
    
    Args:
        df: DataFrame حاوی داده‌های قیمتی
        entry_price: قیمت ورود به پوزیشن
        position_type: نوع پوزیشن ('long' یا 'short')
        current_price: قیمت فعلی (برای trailing stop)
        base_risk: ریسک پایه (پیش‌فرض 2.0)
        atr_multiplier: ضریب ATR (پیش‌فرض 2.0)
        min_risk_reward_ratio: حداقل نسبت ریسک-ریوارد (پیش‌فرض 1.5)
        use_support_resistance: استفاده از سطوح حمایت و مقاومت
        support_resistance_data: داده‌های سطوح حمایت و مقاومت
    
    Returns:
        dict: شامل تمام اطلاعات مدیریت ریسک
    """
    try:
        # محاسبه حدود پویا
        dynamic_stops = _calculate_dynamic_stops(df, entry_price, position_type, base_risk)
        
        # محاسبه حد ضرر پویا با ATR
        dynamic_stop_loss = _calculate_dynamic_stop_loss(df, entry_price, position_type, atr_multiplier)
        
        # محاسبه trailing stop اگر قیمت فعلی موجود باشد
        trailing_stop = None
        if current_price is not None:
            trailing_stop = _calculate_trailing_stop(df, entry_price, current_price, 
                                                    position_type, atr_multiplier)
        
        # محاسبه حدود بر اساس سطوح حمایت و مقاومت
        sr_based_stops = None
        if use_support_resistance and support_resistance_data:
            sr_based_stops = _calculate_stops_with_support_resistance(
                df, entry_price, position_type, support_resistance_data, base_risk
            )
        
        # بهینه‌سازی نسبت ریسک-ریوارد
        optimized_target = _optimize_risk_reward_ratio(
            entry_price, 
            dynamic_stops['take_profit'], 
            dynamic_stops['stop_loss'], 
            min_risk_reward_ratio
        )
        
        # محاسبه آمار نهایی
        final_stop_loss = _get_best_stop_loss(dynamic_stops['stop_loss'], 
                                            dynamic_stop_loss, 
                                            sr_based_stops['stop_loss'] if sr_based_stops else None,
                                            position_type)
        
        final_take_profit = _get_best_take_profit(optimized_target,
                                                sr_based_stops['take_profit'] if sr_based_stops else None,
                                                position_type, entry_price)
        
        # محاسبه نسبت ریسک-ریوارد نهایی
        risk = abs(entry_price - final_stop_loss)
        reward = abs(final_take_profit - entry_price)
        final_risk_reward_ratio = reward / risk if risk > 0 else 0
        
        logger.info(f"Final Stop Loss: {final_stop_loss}, Take Profit: {final_take_profit}, ")
        return {
            'entry_price': entry_price,
            'position_type': position_type,
            'stop_loss': final_stop_loss,
            'take_profit': final_take_profit,
            'risk_reward_ratio': final_risk_reward_ratio,
            'trailing_stop': trailing_stop,
            'risk_amount': risk,
            'reward_amount': reward,
            'calculations': {
                'dynamic_stops': dynamic_stops,
                'dynamic_stop_loss': dynamic_stop_loss,
                'sr_based_stops': sr_based_stops,
                'optimized_target': optimized_target
            },
            'status': 'success'
        }
        
    except Exception as e:
        # در صورت بروز خطا، حدود پیش‌فرض را برگردان
        default_stops = _get_default_stops(entry_price, position_type)
        default_stops['status'] = 'error'
        default_stops['error_message'] = str(e)
        logger.error(f"Error in risk management calculations: {e}")
        return default_stops


def _calculate_stops_with_support_resistance(df, entry_price, position_type, 
                                            support_resistance_data, base_risk):
    """محاسبه حدود با در نظر گیری سطوح حمایت و مقاومت"""
    try:
        # محاسبه ATR برای تعیین ریسک
        risk_multiplier, _ = _calculate_risk_multipliers(None, None, base_risk)
        atr_value = _get_atr_value(df, entry_price, risk_multiplier)
        
        if position_type == 'long':
            stop_loss, take_profit = _calculate_long_stops(
                entry_price, atr_value, support_resistance_data
            )
        else:
            stop_loss, take_profit = _calculate_short_stops(
                entry_price, atr_value, support_resistance_data
            )
        
        logger.info(f"Calculated Stop Loss: {stop_loss}, Take Profit: {take_profit}")
        return {
            'stop_loss': stop_loss,
            'take_profit': take_profit
        }

    except Exception as e:
        logger.error(f"Error in stop loss / take profit calculations: {e}")
        return None


def _get_best_stop_loss(dynamic_sl, atr_sl, sr_sl, position_type):
    """انتخاب بهترین حد ضرر از بین گزینه‌های مختلف"""
    stop_losses = [sl for sl in [dynamic_sl, atr_sl, sr_sl] if sl is not None]
    
    if not stop_losses:
        return dynamic_sl
    
    if position_type == 'long':
        # برای long، بهترین stop loss بالاترین مقدار است (کمترین ریسک)
        return max(stop_losses)
    else:
        # برای short، بهترین stop loss پایین‌ترین مقدار است (کمترین ریسک)
        return min(stop_losses)


def _get_best_take_profit(optimized_tp, sr_tp, position_type, entry_price):
    """انتخاب بهترین حد سود"""
    take_profits = [tp for tp in [optimized_tp, sr_tp] if tp is not None]
    
    if not take_profits:
        return optimized_tp
    
    # انتخاب محافظه‌کارانه‌تر (نزدیک‌تر به قیمت ورود)
    if position_type == 'long':
        return min(take_profits)
    else:
        return max(take_profits)


def update_trailing_stop(df, entry_price, current_price, current_trailing_stop, 
                        position_type='long', atr_multiplier=2.0):
    """
    به‌روزرسانی trailing stop بر اساس قیمت فعلی
    
    Args:
        df: DataFrame حاوی داده‌های قیمتی
        entry_price: قیمت ورود
        current_price: قیمت فعلی
        current_trailing_stop: trailing stop فعلی
        position_type: نوع پوزیشن
        atr_multiplier: ضریب ATR
    
    Returns:
        float: trailing stop جدید
    """
    new_trailing_stop = _calculate_trailing_stop(df, entry_price, current_price, 
                                                position_type, atr_multiplier)
    
    if position_type == 'long':
        # trailing stop فقط بالا می‌رود
        return max(new_trailing_stop, current_trailing_stop)
    else:
        # trailing stop فقط پایین می‌آید
        return min(new_trailing_stop, current_trailing_stop)


def validate_risk_parameters(entry_price, stop_loss, take_profit, position_type, 
                            max_risk_percent=5.0, min_risk_reward=1.0):
    """
    اعتبارسنجی پارامترهای ریسک
    
    Args:
        entry_price: قیمت ورود
        stop_loss: حد ضرر
        take_profit: حد سود
        position_type: نوع پوزیشن
        max_risk_percent: حداکثر درصد ریسک
        min_risk_reward: حداقل نسبت ریسک-ریوارد
    
    Returns:
        dict: نتیجه اعتبارسنجی
    """
    warnings = []
    errors = []
    
    # بررسی منطقی بودن حدود
    if position_type == 'long':
        if stop_loss >= entry_price:
            errors.append("Stop loss باید کمتر از قیمت ورود باشد")
        if take_profit <= entry_price:
            errors.append("Take profit باید بیشتر از قیمت ورود باشد")
    else:
        if stop_loss <= entry_price:
            errors.append("Stop loss باید بیشتر از قیمت ورود باشد")
        if take_profit >= entry_price:
            errors.append("Take profit باید کمتر از قیمت ورود باشد")
    
    # محاسبه ریسک و ریوارد
    risk = abs(entry_price - stop_loss)
    reward = abs(take_profit - entry_price)
    risk_percent = (risk / entry_price) * 100
    risk_reward_ratio = reward / risk if risk > 0 else 0
    
    # بررسی حداکثر ریسک
    if risk_percent > max_risk_percent:
        warnings.append(f"ریسک {risk_percent:.2f}% بیش از حد مجاز {max_risk_percent}% است")
    
    # بررسی نسبت ریسک-ریوارد
    if risk_reward_ratio < min_risk_reward:
        warnings.append(f"نسبت ریسک-ریوارد {risk_reward_ratio:.2f} کمتر از حد مجاز {min_risk_reward} است")
    
    return {
        'is_valid': len(errors) == 0,
        'errors': errors,
        'warnings': warnings,
        'risk_percent': risk_percent,
        'risk_reward_ratio': risk_reward_ratio
    }


# توابع کمکی موجود از کد قبلی
def _get_default_stops(entry_price, position_type):
    """Get default stop loss and take profit values"""
    if position_type == 'long':
        return {
            'stop_loss': entry_price * 0.95,
            'take_profit': entry_price * 1.1,
            'risk_reward_ratio': 2.0
        }
    else:
        return {
            'stop_loss': entry_price * 1.05,
            'take_profit': entry_price * 0.9,
            'risk_reward_ratio': 2.0
        }


def _calculate_risk_multipliers(trend_data, volatility_data, base_risk):
    """Calculate risk and reward multipliers based on trend and volatility"""
    if trend_data is None or volatility_data is None:
        return base_risk, 2.5
    
    trend_strength = trend_data['strength']
    volatility_score = volatility_data['volatility_score']
    
    # Adjust multipliers based on trend strength
    if trend_strength > 50:
        risk_multiplier = base_risk * (1 + trend_strength / 100)
        reward_multiplier = 3.0 + trend_strength / 50
    else:
        risk_multiplier = base_risk * 0.7
        reward_multiplier = 1.5
    
    # Adjust based on volatility
    if volatility_score > 5:
        risk_multiplier *= 1.5
        reward_multiplier *= 1.3
    elif volatility_score < 2:
        risk_multiplier *= 0.8
        reward_multiplier *= 0.9
    
    return risk_multiplier, reward_multiplier


def _get_atr_value(df, entry_price, risk_multiplier):
    """Get ATR value adjusted by risk multiplier"""
    atr = _calculate_atr(df, 14)
    atr_value = atr.iloc[-1] if atr is not None else entry_price * 0.02
    return atr_value * risk_multiplier


def _calculate_long_stops(entry_price, atr_value, support_resistance):
    """Calculate stop loss and take profit for long positions"""
    stop_loss_atr = entry_price - atr_value
    
    # Adjust stop loss based on support levels
    if (support_resistance and support_resistance.get('support_levels')):
        nearest_support = _find_nearest_support(
            support_resistance['support_levels'], 
            entry_price, 
            stop_loss_atr
        )
        stop_loss = max(stop_loss_atr, nearest_support * 0.98)
    else:
        stop_loss = stop_loss_atr
    
    # Calculate take profit
    risk_amount = entry_price - stop_loss
    take_profit_atr = entry_price + (risk_amount * 2.5)
    
    # Adjust take profit based on resistance levels
    if (support_resistance and support_resistance.get('resistance_levels')):
        nearest_resistance = _find_nearest_resistance(
            support_resistance['resistance_levels'], 
            entry_price, 
            take_profit_atr
        )
        take_profit = min(take_profit_atr, nearest_resistance * 0.98)
    else:
        take_profit = take_profit_atr
    
    return stop_loss, take_profit


def _calculate_short_stops(entry_price, atr_value, support_resistance):
    """Calculate stop loss and take profit for short positions"""
    stop_loss_atr = entry_price + atr_value
    
    # Adjust stop loss based on resistance levels
    if (support_resistance and support_resistance.get('resistance_levels')):
        nearest_resistance = _find_nearest_resistance(
            support_resistance['resistance_levels'], 
            entry_price, 
            stop_loss_atr
        )
        stop_loss = min(stop_loss_atr, nearest_resistance * 1.02)
    else:
        stop_loss = stop_loss_atr
    
    # Calculate take profit
    risk_amount = stop_loss - entry_price
    take_profit_atr = entry_price - (risk_amount * 2.5)
    
    # Adjust take profit based on support levels
    if (support_resistance and support_resistance.get('support_levels')):
        nearest_support = _find_nearest_support(
            support_resistance['support_levels'], 
            entry_price, 
            take_profit_atr
        )
        take_profit = max(take_profit_atr, nearest_support * 1.02)
    else:
        take_profit = take_profit_atr
    
    return stop_loss, take_profit


def _ensure_minimum_risk_reward(entry_price, stop_loss, take_profit, position_type, min_ratio=1.5):
    """Ensure minimum risk-reward ratio"""
    risk = abs(entry_price - stop_loss)
    reward = abs(take_profit - entry_price)
    risk_reward_ratio = reward / risk if risk > 0 else 0
    
    if risk_reward_ratio < min_ratio:
        if position_type == 'long':
            take_profit = entry_price + (risk * min_ratio)
        else:
            take_profit = entry_price - (risk * min_ratio)
        risk_reward_ratio = min_ratio
    
    return take_profit, risk_reward_ratio


def _calculate_dynamic_stops(df, entry_price, position_type='long', base_risk=2.0):
    """محاسبه حد ضرر و حد سود پویا بر اساس ترند و نوسانات"""
    
    def _dynamic_stops_calc():
        try:
            if df is None or len(df) < 20:
                return _get_default_stops(entry_price, position_type)
            
            # Get volatility data using ATR
            atr = _calculate_atr(df, 14)
            if atr is None or atr.empty:
                return _get_default_stops(entry_price, position_type)
            
            volatility_data = {'volatility_score': atr.iloc[-1] / entry_price * 100}
            trend_data = {'strength': 50}  # Default trend strength
            
            # Calculate risk multipliers
            risk_multiplier, _ = _calculate_risk_multipliers(
                trend_data, volatility_data, base_risk
            )
            
            # Get ATR value
            atr_value = _get_atr_value(df, entry_price, risk_multiplier)
            
            # Calculate stops based on position type
            if position_type == 'long':
                stop_loss, take_profit = _calculate_long_stops(
                    entry_price, atr_value, None
                )
            else:
                stop_loss, take_profit = _calculate_short_stops(
                    entry_price, atr_value, None
                )
            
            # Ensure minimum risk-reward ratio
            take_profit, risk_reward_ratio = _ensure_minimum_risk_reward(
                entry_price, stop_loss, take_profit, position_type
            )
            
            return {
                'stop_loss': stop_loss,
                'take_profit': take_profit,
                'risk_reward_ratio': risk_reward_ratio
            }
            
        except Exception:
            return _get_default_stops(entry_price, position_type)
    
    return _cached_indicator_calculation(df, 'dynamic_stops', _dynamic_stops_calc)


def _calculate_trailing_stop(df, entry_price, current_price, position_type='long', atr_multiplier=2.0):
    """محاسبه حد ضرر متحرک"""
    try:
        if df is None or len(df) < 14:
            return entry_price * 0.95 if position_type == 'long' else entry_price * 1.05
        
        atr = _calculate_atr(df, 14)
        if atr is None:
            atr_value = abs(current_price - entry_price) * 0.1
        else:
            atr_value = atr.iloc[-1] * atr_multiplier
        
        if position_type == 'long':
            # برای پوزیشن خرید، حد ضرر به سمت بالا حرکت می‌کند
            trailing_stop = current_price - atr_value
            # حد ضرر نمی‌تواند پایین‌تر از قیمت ورود برود
            return max(trailing_stop, entry_price * 0.98)
        else:
            # برای پوزیشن فروش، حد ضرر به سمت پایین حرکت می‌کند
            trailing_stop = current_price + atr_value
            # حد ضرر نمی‌تواند بالاتر از قیمت ورود برود
            return min(trailing_stop, entry_price * 1.02)
            
    except Exception:
        return entry_price * 0.95 if position_type == 'long' else entry_price * 1.05


def _calculate_dynamic_stop_loss(df, entry_price, position_type='long', atr_multiplier=2):
    """محاسبه حد ضرر پویا"""
    try:
        if df is None or len(df) < 14:
            return entry_price * 0.95 if position_type == 'long' else entry_price * 1.05
            
        atr = _calculate_atr(df, 14)
        if atr is None:
            return entry_price * 0.95 if position_type == 'long' else entry_price * 1.05
            
        atr_value = atr.iloc[-1]
        
        if position_type == 'long':
            stop_loss = entry_price - (atr_value * atr_multiplier)
        else:
            stop_loss = entry_price + (atr_value * atr_multiplier)
            
        return stop_loss
    except Exception:
        return entry_price * 0.95 if position_type == 'long' else entry_price * 1.05


def _optimize_risk_reward_ratio(entry_price, target_price, stop_loss, min_ratio=2.0):
    """بهینه‌سازی نسبت ریسک-ریوارد"""
    try:
        risk = abs(entry_price - stop_loss)
        reward = abs(target_price - entry_price)
        
        current_ratio = reward / risk if risk > 0 else 0
        
        if current_ratio < min_ratio:
            # تنظیم هدف برای دستیابی به نسبت حداقل
            if entry_price > stop_loss:  # long position
                new_target = entry_price + (risk * min_ratio)
            else:  # short position
                new_target = entry_price - (risk * min_ratio)
            
            return new_target
        
        return target_price
    except Exception:
        return target_price

=====================

support_resistance.py

from logger_config import logger
from .cache_utils import cached_calculation

def _calculate_pivot_points_internal(df):
    """Internal Pivot Points calculation function"""
    try:
        if df is None or len(df) < 1:
            return None
        
        high = df['high'].iloc[-1]
        low = df['low'].iloc[-1]
        close = df['close'].iloc[-1]
        
        # Standard Pivot Points
        pivot = (high + low + close) / 3
        r1 = 2 * pivot - low
        s1 = 2 * pivot - high
        r2 = pivot + (high - low)
        s2 = pivot - (high - low)
        r3 = high + 2 * (pivot - low)
        s3 = low - 2 * (high - pivot)
        
        return {
            'pivot': pivot,
            'r1': r1, 'r2': r2, 'r3': r3,
            's1': s1, 's2': s2, 's3': s3
        }
    except Exception as e:
        logger.warning(f"Error calculating Pivot Points: {e}")
        return None

@cached_calculation('pivot_points')
def _calculate_pivot_points(df):
    """محاسبه Pivot Points with caching"""
    return _calculate_pivot_points_internal(df)
            
def _calculate_support_resistance_internal(df, window):
    """Internal calculation for Support/Resistance levels"""
    try:
        if df is None or len(df) < window:
            return None
        
        high = df['high']
        low = df['low']
        
        # Local highs and lows
        resistance_levels = []
        support_levels = []
        
        for i in range(window, len(df) - window):
            # Check for local high (resistance)
            if high.iloc[i] == high.iloc[i-window:i+window+1].max():
                resistance_levels.append(high.iloc[i])
            
            # Check for local low (support)
            if low.iloc[i] == low.iloc[i-window:i+window+1].min():
                support_levels.append(low.iloc[i])
        
        # Get most significant levels
        resistance_levels = sorted(set(resistance_levels), reverse=True)[:5]
        support_levels = sorted(set(support_levels))[:5]
        
        return {
            'resistance_levels': resistance_levels,
            'support_levels': support_levels
        }
    except Exception as e:
        logger.warning(f"Error calculating Support/Resistance: {e}")
        return None

@cached_calculation('support_resistance')
def _calculate_support_resistance(df, window=20):
    """محاسبه سطوح Support و Resistance with caching"""
    try:
        return _calculate_support_resistance_internal(df, window)
    except Exception as e:
        logger.warning(f"Error calculating Support/Resistance: {e}")
        return None

def _find_swing_points(high, low, swing_strength):
    """Helper function to find swing highs and lows"""
    swing_highs = []
    swing_lows = []
    
    for i in range(swing_strength, len(high) - swing_strength):
        # Swing High
        if high.iloc[i] == high.iloc[i-swing_strength:i+swing_strength+1].max():
            swing_highs.append((i, high.iloc[i]))
        
        # Swing Low
        if low.iloc[i] == low.iloc[i-swing_strength:i+swing_strength+1].min():
            swing_lows.append((i, low.iloc[i]))
    
    return swing_highs, swing_lows

def _get_recent_high(swing_highs):
    """Get the most recent significant high"""
    if not swing_highs:
        return None
    return max(swing_highs[-3:], key=lambda x: x[1])[1] if len(swing_highs) >= 3 else swing_highs[-1][1]

def _get_recent_low(swing_lows):
    """Get the most recent significant low"""
    if not swing_lows:
        return None
    return min(swing_lows[-3:], key=lambda x: x[1])[1] if len(swing_lows) >= 3 else swing_lows[-1][1]

def _calculate_support_resistance_levels_internal(df, window, min_touches):
    """Internal calculation for detailed Support/Resistance levels"""
    try:
        if df is None or len(df) < window * 2:
            return {
                'resistance_levels': [],
                'support_levels': [],
                'resistance_strength': [],
                'support_strength': []
            }
        
        high = df['high']
        low = df['low']
        
        # پیدا کردن نقاط pivot
        pivot_highs, pivot_lows = _find_pivot_points(high, low, window)
        
        # تجمیع سطوح مشابه
        resistance_clusters = _cluster_levels(pivot_highs)
        support_clusters = _cluster_levels(pivot_lows)
        
        # انتخاب قوی‌ترین سطوح
        strong_resistance = _extract_strong_levels(resistance_clusters, min_touches)
        strong_support = _extract_strong_levels(support_clusters, min_touches)
        
        return {
            'resistance_levels': [level[0] for level in strong_resistance],
            'support_levels': [level[0] for level in strong_support],
            'resistance_strength': [level[1] for level in strong_resistance],
            'support_strength': [level[1] for level in strong_support]
        }
    except Exception as e:
        logger.warning(f"Error calculating support/resistance levels: {e}")
        return {
            'resistance_levels': [],
            'support_levels': [],
            'resistance_strength': [],
            'support_strength': []
        }

@cached_calculation('support_resistance_levels')
def _calculate_support_resistance_levels(df, window=20, min_touches=3):
    """محاسبه سطوح حمایت و مقاومت دقیق with caching"""
    try:
        return _calculate_support_resistance_levels_internal(df, window, min_touches)
    except Exception as e:
        logger.warning(f"Error calculating support/resistance levels: {e}")
        return None

def _find_pivot_points(high, low, window):
    """Helper function to find pivot highs and lows"""
    pivot_highs = []
    pivot_lows = []
    
    for i in range(window, len(high) - window):
        # Pivot High
        if high.iloc[i] == high.iloc[i-window:i+window+1].max():
            pivot_highs.append((i, high.iloc[i]))
        
        # Pivot Low
        if low.iloc[i] == low.iloc[i-window:i+window+1].min():
            pivot_lows.append((i, low.iloc[i]))
    
    return pivot_highs, pivot_lows

def _cluster_levels(levels, tolerance=0.01):
    """Helper function to cluster similar price levels"""
    if not levels:
        return []
    
    levels = sorted(levels, key=lambda x: x[1])
    clusters = []
    current_cluster = [levels[0]]
    
    for level in levels[1:]:
        if abs(level[1] - current_cluster[-1][1]) / current_cluster[-1][1] <= tolerance:
            current_cluster.append(level)
        else:
            clusters.append(current_cluster)
            current_cluster = [level]
    clusters.append(current_cluster)
    
    return clusters

def _extract_strong_levels(clusters, min_touches):
    """Helper function to extract strong levels from clusters"""
    strong_levels = []
    
    for cluster in clusters:
        if len(cluster) >= min_touches:
            # Calculate average price level for the cluster
            avg_price = sum(level[1] for level in cluster) / len(cluster)
            # Return (price, strength) where strength is number of touches
            strong_levels.append((avg_price, len(cluster)))
    
    # Sort by strength (number of touches) in descending order
    strong_levels.sort(key=lambda x: x[1], reverse=True)
    
    return strong_levels

def _find_nearest_support(support_levels, entry_price, default_value):
    """Find nearest support level below entry price"""
    if not support_levels:
        return default_value
    
    valid_supports = [s for s in support_levels if s < entry_price]
    return max(valid_supports, default=default_value)

def _find_nearest_resistance(resistance_levels, entry_price, default_value):
    """Find nearest resistance level above entry price"""
    if not resistance_levels:
        return default_value
    
    valid_resistances = [r for r in resistance_levels if r > entry_price]
    return min(valid_resistances, default=default_value)

=====================

trend_indicators.py

from logger_config import logger
from numba import jit
import numpy as np
import pandas as pd
from .moving_averages import _fast_ema
from .volatility_indicators import _calculate_average_true_range
from .cache_utils import cached_calculation


@jit(nopython=True)
def _fast_macd_calculation(prices, fast_period, slow_period, signal_period):
    """محاسبه سریع MACD"""
    fast_alpha = 2.0 / (fast_period + 1)
    slow_alpha = 2.0 / (slow_period + 1)
    signal_alpha = 2.0 / (signal_period + 1)
    
    fast_ema = _fast_ema(prices, fast_alpha)
    slow_ema = _fast_ema(prices, slow_alpha)
    
    macd_line = fast_ema - slow_ema
    signal_line = _fast_ema(macd_line, signal_alpha)
    histogram = macd_line - signal_line
    
    return macd_line, signal_line, histogram

@cached_calculation('trend_strength')
def _calculate_trend_strength(df, period=20):
    """محاسبه قدرت ترند"""
    try:
        if df is None or len(df) < period:
            return None
        
        close = df['close']
        sma = close.rolling(window=period).mean()
        trend_strength = ((close - sma) / sma) * 100
        
        return trend_strength
    except Exception as e:
        logger.warning(f"Error calculating trend strength: {e}")
        return None

@cached_calculation('donchian_channels')
def _calculate_donchian_channels(df, period=20):
    """محاسبه Donchian Channels"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        
        upper_channel = high.rolling(window=period).max()
        lower_channel = low.rolling(window=period).min()
        middle_channel = (upper_channel + lower_channel) / 2
        
        return {
            'donchian_upper': upper_channel,
            'donchian_middle': middle_channel,
            'donchian_lower': lower_channel
        }
    except Exception as e:
        logger.warning(f"Error calculating Keltner Channels: {e}")
        return None

def _calculate_donchian_channels(df, period=20):
    """محاسبه Donchian Channels"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        
        upper_channel = high.rolling(window=period).max()
        lower_channel = low.rolling(window=period).min()
        middle_channel = (upper_channel + lower_channel) / 2
        
        return {
            'donchian_upper': upper_channel,
            'donchian_middle': middle_channel,
            'donchian_lower': lower_channel
        }
    except Exception as e:
        logger.warning(f"Error calculating Donchian Channels: {e}")
        return None

@cached_calculation('supertrend')
def _calculate_supertrend(df, period=10, multiplier=3.0):
    """محاسبه Supertrend with caching"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        close = df['close']
        
        # محاسبه ATR
        atr = _calculate_average_true_range(df, period)
        if atr is None:
            return None
        
        # محاسبه Basic Upper و Lower Bands
        hl2 = (high + low) / 2
        upper_band = hl2 + (multiplier * atr)
        lower_band = hl2 - (multiplier * atr)
        
        # Initialize arrays
        final_upper_band = np.zeros(len(df))
        final_lower_band = np.zeros(len(df))
        supertrend = np.zeros(len(df))
        direction = np.zeros(len(df))
        
        # Set initial values
        final_upper_band[0] = upper_band.iloc[0]
        final_lower_band[0] = lower_band.iloc[0]
        direction[0] = 1
        supertrend[0] = final_lower_band[0]
        
        # Calculate Supertrend
        for i in range(1, len(df)):
            # Calculate final upper band
            if upper_band.iloc[i] < final_upper_band[i-1] or close.iloc[i-1] > final_upper_band[i-1]:
                final_upper_band[i] = upper_band.iloc[i]
            else:
                final_upper_band[i] = final_upper_band[i-1]
            
            # Calculate final lower band
            if lower_band.iloc[i] > final_lower_band[i-1] or close.iloc[i-1] < final_lower_band[i-1]:
                final_lower_band[i] = lower_band.iloc[i]
            else:
                final_lower_band[i] = final_lower_band[i-1]
            
            # Calculate direction
            if direction[i-1] == -1 and close.iloc[i] < final_lower_band[i]:
                direction[i] = -1
            elif direction[i-1] == 1 and close.iloc[i] > final_upper_band[i]:
                direction[i] = 1
            elif direction[i-1] == -1 and close.iloc[i] >= final_lower_band[i]:
                direction[i] = 1
            elif direction[i-1] == 1 and close.iloc[i] <= final_upper_band[i]:
                direction[i] = -1
            else:
                direction[i] = direction[i-1]
            
            # Calculate Supertrend value
            if direction[i] == 1:
                supertrend[i] = final_lower_band[i]
            else:
                supertrend[i] = final_upper_band[i]
        
        return pd.Series(supertrend, index=df.index)
    
    except Exception as e:
        logger.warning(f"Error calculating Supertrend: {e}")
        return None
        
@cached_calculation('aroon_oscillator')
def _calculate_aroon_oscillator(df, period=14):
    try:
        if df is None or len(df) < period:
            return None
        
        if 'high' not in df.columns or 'low' not in df.columns:
            return None
        
        high = df['high'].astype(float)
        low = df['low'].astype(float)
        
        aroon_up = np.full(len(df), np.nan)
        aroon_down = np.full(len(df), np.nan)
        
        for i in range(period - 1, len(df)):
            high_period = high.iloc[i-period+1:i+1].values
            low_period = low.iloc[i-period+1:i+1].values
            
            high_max_idx = np.argmax(high_period)
            low_min_idx = np.argmin(low_period)
            
            periods_since_high = period - 1 - high_max_idx
            periods_since_low = period - 1 - low_min_idx
            
            aroon_up[i] = ((period - periods_since_high) / period) * 100
            aroon_down[i] = ((period - periods_since_low) / period) * 100
        
        aroon_up_series = pd.Series(aroon_up, index=df.index)
        aroon_down_series = pd.Series(aroon_down, index=df.index)
        aroon_oscillator = aroon_up_series - aroon_down_series
        
        return {
            'aroon_up': aroon_up_series,
            'aroon_down': aroon_down_series,
            'aroon_oscillator': aroon_oscillator
        }
    except Exception:
        return None

@cached_calculation('aroon')
def _calculate_aroon(df, period=14):
    """محاسبه Aroon Oscillator"""
    try:
        if df is None or len(df) < period:
            return None
            
        high = df['high']
        low = df['low']
        
        # پیدا کردن موقعیت بالاترین و پایین‌ترین قیمت
        aroon_up = ((period - high.rolling(period).apply(lambda x: period - 1 - x.argmax())) / period) * 100
        aroon_down = ((period - low.rolling(period).apply(lambda x: period - 1 - x.argmin())) / period) * 100
        
        aroon_oscillator = aroon_up - aroon_down
        
        return {
            'aroon_up': aroon_up,
            'aroon_down': aroon_down,
            'aroon_oscillator': aroon_oscillator
        }
        
    except Exception as e:
        logger.warning(f"Error calculating Aroon: {e}")
        return None

@cached_calculation('adx')
def _calculate_adx_internal(df, period):
    """Internal ADX calculation function"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        
        # True Range
        atr = _calculate_average_true_range(df, period)
        if atr is None:
            return None
        
        # Directional Movement
        up_move = high - high.shift(1)
        down_move = low.shift(1) - low
        
        plus_dm = pd.Series(0, index=df.index)
        minus_dm = pd.Series(0, index=df.index)
        
        plus_dm[up_move > down_move] = up_move[up_move > down_move]
        plus_dm[plus_dm < 0] = 0
        
        minus_dm[down_move > up_move] = down_move[down_move > up_move]
        minus_dm[minus_dm < 0] = 0
        
        # Smoothed DM
        plus_dm_smooth = plus_dm.rolling(window=period).mean()
        minus_dm_smooth = minus_dm.rolling(window=period).mean()
        
        # DI calculations
        plus_di = 100 * (plus_dm_smooth / atr)
        minus_di = 100 * (minus_dm_smooth / atr)
        
        # ADX calculation
        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)
        dx = dx.fillna(0)
        adx = dx.rolling(window=period).mean()
        
        return {
            'plus_di': plus_di,
            'minus_di': minus_di,
            'adx': adx
        }
    except Exception as e:
        logger.warning(f"Error calculating ADX: {e}")
        return None

def _calculate_aroon(df, period=14):
    """محاسبه Aroon Oscillator"""
    try:
        if df is None or len(df) < period:
            return None
            
        high = df['high']
        low = df['low']
        
        # پیدا کردن موقعیت بالاترین و پایین‌ترین قیمت
        aroon_up = ((period - high.rolling(period).apply(lambda x: period - 1 - x.argmax())) / period) * 100
        aroon_down = ((period - low.rolling(period).apply(lambda x: period - 1 - x.argmin())) / period) * 100
        
        aroon_oscillator = aroon_up - aroon_down
        
        return {
            'aroon_up': aroon_up,
            'aroon_down': aroon_down,
            'aroon_oscillator': aroon_oscillator
        }
        
    except Exception as e:
        logger.warning(f"Error calculating Aroon: {e}")
        return None

def _calculate_adx_internal(df, period):
    """Internal ADX calculation function"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        
        # True Range
        atr = _calculate_average_true_range(df, period)
        if atr is None:
            return None
        
        # Directional Movement
        up_move = high - high.shift(1)
        down_move = low.shift(1) - low
        
        plus_dm = pd.Series(0, index=df.index)
        minus_dm = pd.Series(0, index=df.index)
        
        plus_dm[up_move > down_move] = up_move[up_move > down_move]
        plus_dm[plus_dm < 0] = 0
        
        minus_dm[down_move > up_move] = down_move[down_move > up_move]
        minus_dm[minus_dm < 0] = 0
        
        # Smoothed DM
        plus_dm_smooth = plus_dm.rolling(window=period).mean()
        minus_dm_smooth = minus_dm.rolling(window=period).mean()
        
        # DI calculations
        plus_di = 100 * (plus_dm_smooth / atr)
        minus_di = 100 * (minus_dm_smooth / atr)
        
        # ADX calculation
        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)
        dx = dx.fillna(0)
        adx = dx.rolling(window=period).mean()
        
        return {
            'plus_di': plus_di,
            'minus_di': minus_di,
            'adx': adx
        }
    except Exception as e:
        logger.warning(f"Error calculating ADX: {e}")
        return None

def _initialize_sar_arrays(df, af, high, low):
    """Initialize arrays for Parabolic SAR calculation"""
    sar = np.zeros(len(df))
    trend = np.zeros(len(df))
    af_val = np.zeros(len(df))
    ep = np.zeros(len(df))
    
    sar[0] = low[0]
    trend[0] = 1
    af_val[0] = af
    ep[0] = high[0]
    
    return sar, trend, af_val, ep

def _handle_uptrend_sar(i, sar, trend, af_val, ep, high, low, af, max_af):
    """Handle Parabolic SAR calculation for uptrend"""
    sar[i] = sar[i-1] + af_val[i-1] * (ep[i-1] - sar[i-1])
    
    if low[i] <= sar[i]:
        # Trend reversal to downtrend
        trend[i] = -1
        sar[i] = ep[i-1]
        ep[i] = low[i]
        af_val[i] = af
    else:
        # Continue uptrend
        trend[i] = 1
        if high[i] > ep[i-1]:
            ep[i] = high[i]
            af_val[i] = min(af_val[i-1] + af, max_af)
        else:
            ep[i] = ep[i-1]
            af_val[i] = af_val[i-1]

def _handle_downtrend_sar(i, sar, trend, af_val, ep, high, low, af, max_af):
    """Handle Parabolic SAR calculation for downtrend"""
    sar[i] = sar[i-1] - af_val[i-1] * (sar[i-1] - ep[i-1])
    
    if high[i] >= sar[i]:
        # Trend reversal to uptrend
        trend[i] = 1
        sar[i] = ep[i-1]
        ep[i] = high[i]
        af_val[i] = af
    else:
        # Continue downtrend
        trend[i] = -1
        if low[i] < ep[i-1]:
            ep[i] = low[i]
            af_val[i] = min(af_val[i-1] + af, max_af)
        else:
            ep[i] = ep[i-1]
            af_val[i] = af_val[i-1]

def _calculate_final_upper_band(upper_band, final_upper_band, close, i):
    """Calculate final upper band for Supertrend"""
    if upper_band.iloc[i] < final_upper_band[i-1] or close.iloc[i-1] > final_upper_band[i-1]:
        return upper_band.iloc[i]
    else:
        return final_upper_band[i-1]

def _calculate_final_lower_band(lower_band, final_lower_band, close, i):
    """Calculate final lower band for Supertrend"""
    if lower_band.iloc[i] > final_lower_band[i-1] or close.iloc[i-1] < final_lower_band[i-1]:
        return lower_band.iloc[i]
    else:
        return final_lower_band[i-1]

def _calculate_direction(direction, close, final_upper_band, final_lower_band, i):
    """Calculate direction for Supertrend"""
    prev_direction = direction[i-1]
    current_close = close.iloc[i]
    
    if prev_direction == -1 and current_close < final_lower_band[i]:
        return -1
    elif prev_direction == 1 and current_close > final_upper_band[i]:
        return 1
    elif prev_direction == -1 and current_close >= final_lower_band[i]:
        return 1
    elif prev_direction == 1 and current_close <= final_upper_band[i]:
        return -1
    else:
        return prev_direction

def _calculate_supertrend_value(direction, final_upper_band, final_lower_band, i):
    """Calculate Supertrend value based on direction"""
    if direction[i] == 1:
        return final_lower_band[i]
    else:
        return final_upper_band[i]

def _initialize_supertrend_arrays(upper_band, lower_band):
    """Initialize arrays for Supertrend calculation"""
    final_upper_band = [upper_band.iloc[0]]
    final_lower_band = [lower_band.iloc[0]]
    supertrend = [None]  # First value is None
    direction = [1]  # Start with uptrend
    
    return final_upper_band, final_lower_band, supertrend, direction

=====================

volatility_indicators.py

from logger_config import logger
import numpy as np
import pandas as pd
from .moving_averages import _fast_sma
from .cache_utils import NUMBA_AVAILABLE, cached_calculation, jit

@jit(nopython=True)
def _fast_bollinger_bands(prices, period, std_dev):
    """محاسبه سریع باندهای بولینگر"""
    sma = _fast_sma(prices, period)
    
    # محاسبه انحراف معیار
    std = np.empty(len(prices))
    std[:period-1] = np.nan
    
    for i in range(period-1, len(prices)):
        window = prices[i-period+1:i+1]
        std[i] = np.std(window)
    
    upper_band = sma + (std * std_dev)
    lower_band = sma - (std * std_dev)
    
    return sma, upper_band, lower_band

@cached_calculation('bollinger_bands')
def _calculate_bollinger_bands(df, period, std_dev):
    try:
        if df is None or len(df) < period:
            return None
        
        prices = df['close'].values.astype(np.float64)
        if NUMBA_AVAILABLE:
            sma, upper_band, lower_band = _fast_bollinger_bands(prices, period, std_dev)
        else:
            sma = df['close'].rolling(window=period).mean().values
            std = df['close'].rolling(window=period).std().values
            upper_band = sma + (std * std_dev)
            lower_band = sma - (std * std_dev)
        
        return {
            'bb_upper': pd.Series(upper_band, index=df.index),
            'bb_middle': pd.Series(sma, index=df.index),
            'bb_lower': pd.Series(lower_band, index=df.index)
        }
    except Exception as e:
        logger.warning(f"Error in optimized Bollinger Bands calculation: {e}")
        return None
@cached_calculation('average_true_range')
def _calculate_average_true_range(df, period=14):
    """محاسبه Average True Range"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        close = df['close']
        prev_close = close.shift(1)
        
        tr1 = high - low
        tr2 = abs(high - prev_close)
        tr3 = abs(low - prev_close)
        
        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = true_range.rolling(window=period).mean()
        
        return atr
    except Exception as e:
        logger.warning(f"Error calculating ATR: {e}")
        return None
@cached_calculation('atr')
def _calculate_atr(df, period):
    """Internal ATR calculation function"""
    try:
        if df is None or len(df) < period:
            return None
            
        high = df['high']
        low = df['low']
        close = df['close']
        
        prev_close = close.shift(1)
        
        tr1 = high - low
        tr2 = abs(high - prev_close)
        tr3 = abs(low - prev_close)
        
        # Calculate True Range as the maximum of the three values
        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        # Calculate ATR as the rolling mean of True Range
        atr = true_range.rolling(window=period).mean()
        
        return atr
        
    except Exception as e:
        logger.warning(f"Error calculating ATR: {e}")
        return None
@cached_calculation('standard_deviation')
def _calculate_standard_deviation(df, period=20):
    """محاسبه Standard Deviation"""
    try:
        if df is None or len(df) < period:
            return None
        
        close = df['close']
        std_dev = close.rolling(window=period).std()
        
        return std_dev
    except Exception as e:
        logger.warning(f"Error calculating Standard Deviation: {e}")
        return None
@cached_calculation('price_std')
def _calculate_price_std(df, period=20):
    """محاسبه انحراف معیار قیمت"""
    try:
        if df is None or len(df) < period:
            return None
            
        close = df['close']
        std_dev = close.rolling(period).std()
        
        return std_dev
    except Exception as e:
        logger.warning(f"Error calculating Price Standard Deviation: {e}")
        return None

=====================

volume_indicators.py

from logger_config import logger
import numpy as np
import pandas as pd
from .cache_utils import cached_calculation

@cached_calculation('obv')
def _calculate_obv(df):
    """محاسبه On-Balance Volume"""
    try:
        if df is None or len(df) < 2:
            return None
        
        close = df['close']
        volume = df['volume']
        
        obv = []
        obv.append(0)  # مقدار اولیه
        
        for i in range(1, len(df)):
            if close.iloc[i] > close.iloc[i-1]:
                obv.append(obv[i-1] + volume.iloc[i])
            elif close.iloc[i] < close.iloc[i-1]:
                obv.append(obv[i-1] - volume.iloc[i])
            else:
                obv.append(obv[i-1])
        
        return pd.Series(obv, index=df.index)
    except Exception as e:
        logger.warning(f"Error calculating OBV: {e}")
        return None
@cached_calculation('accumulation_distribution')
def _calculate_accumulation_distribution(df):
    """محاسبه Accumulation/Distribution Line"""
    try:
        if df is None or len(df) < 1:
            return None
        
        high = df['high']
        low = df['low']
        close = df['close']
        volume = df['volume']
        
        # Money Flow Multiplier
        mfm = ((close - low) - (high - close)) / (high - low)
        # Handle division by zero
        mfm = mfm.fillna(0)
        
        # Money Flow Volume
        mfv = mfm * volume
        
        # A/D Line is cumulative sum of MFV
        ad_line = mfv.cumsum()
        
        return ad_line
    except Exception as e:
        logger.warning(f"Error calculating A/D Line: {e}")
        return None
@cached_calculation('ad_line')
def _calculate_ad_line(df):
    """محاسبه Accumulation/Distribution Line"""
    try:
        if df is None or len(df) < 1:
            return None
            
        high = df['high']
        low = df['low']
        close = df['close']
        volume = df['volume']
        
        # محاسبه Money Flow Multiplier
        clv = ((close - low) - (high - close)) / (high - low)
        clv = clv.fillna(0)  # در صورت صفر بودن دامنه
        
        # محاسبه Money Flow Volume
        mfv = clv * volume
        
        # محاسبه A/D Line تجمعی
        ad_line = mfv.cumsum()
        
        return ad_line
    except Exception:
        return None
@cached_calculation('chaikin_money_flow')
def _calculate_chaikin_money_flow(df, period=20):
    """محاسبه Chaikin Money Flow"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        close = df['close']
        volume = df['volume']
        
        # Money Flow Multiplier
        mfm = ((close - low) - (high - close)) / (high - low)
        mfm = mfm.fillna(0)
        
        # Money Flow Volume
        mfv = mfm * volume
        
        # CMF = Sum of MFV over period / Sum of Volume over period
        cmf = mfv.rolling(window=period).sum() / volume.rolling(window=period).sum()
        
        return cmf
    except Exception as e:
        logger.warning(f"Error calculating CMF: {e}")
        return None
@cached_calculation('volume_price_trend')
def _calculate_volume_price_trend(df):
    """محاسبه Volume Price Trend"""
    try:
        if df is None or len(df) < 2:
            return None
        
        close = df['close']
        volume = df['volume']
        
        # Price change percentage
        price_change_pct = (close - close.shift(1)) / close.shift(1)
        
        # VPT = Previous VPT + Volume * Price Change %
        vpt = (price_change_pct * volume).cumsum()
        
        return vpt
    except Exception as e:
        logger.warning(f"Error calculating VPT: {e}")
        return None
@cached_calculation('ease_of_movement')
def _calculate_ease_of_movement(df, period=14):
    """محاسبه Ease of Movement"""
    try:
        if df is None or len(df) < period:
            return None
        
        high = df['high']
        low = df['low']
        volume = df['volume']
        
        # Distance Moved
        distance_moved = (high + low) / 2 - (high.shift(1) + low.shift(1)) / 2
        
        # Box Height
        box_height = (volume / 100000) / (high - low)
        
        # 1-Period EMV
        emv_1period = distance_moved / box_height
        emv_1period = emv_1period.replace([np.inf, -np.inf], 0).fillna(0)
        
        # EMV = SMA of 1-Period EMV
        emv = emv_1period.rolling(window=period).mean()
        
        return emv
    except Exception as e:
        logger.warning(f"Error calculating EMV: {e}")
        return None
@cached_calculation('vwap')
def _calculate_vwap(df):
    """محاسبه Volume Weighted Average Price"""
    try:
        if df is None or len(df) < 1:
            return None
        
        high = df['high']
        low = df['low']
        close = df['close']
        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)
        
        # محاسبه Typical Price
        typical_price = (high + low + close) / 3
        
        # محاسبه VWAP
        cumulative_typical_price_volume = (typical_price * volume).cumsum()
        cumulative_volume = volume.cumsum()
        
        vwap = cumulative_typical_price_volume / cumulative_volume
        
        # محاسبه VWAP Bands (انحراف معیار)
        vwap_variance = ((typical_price - vwap) ** 2 * volume).cumsum() / cumulative_volume
        vwap_std = np.sqrt(vwap_variance)
        
        vwap_upper1 = vwap + vwap_std
        vwap_lower1 = vwap - vwap_std
        vwap_upper2 = vwap + 2 * vwap_std
        vwap_lower2 = vwap - 2 * vwap_std
        
        return {
            'vwap': vwap,
            'vwap_upper1': vwap_upper1,
            'vwap_lower1': vwap_lower1,
            'vwap_upper2': vwap_upper2,
            'vwap_lower2': vwap_lower2
        }
    except Exception as e:
        logger.warning(f"Error calculating VWAP: {e}")
        return None

def _check_volume_filter(df, min_volume_ratio):
    """Check if volume meets minimum ratio requirement"""
    if 'volume' not in df.columns or len(df) < 20:
        return True
    
    volume_sma = df['volume'].rolling(window=20).mean().iloc[-1]
    if pd.isna(volume_sma) or volume_sma <= 0:
        return True
    
    last_volume = df.iloc[-1]['volume']
    volume_ratio = last_volume / volume_sma
    return volume_ratio >= min_volume_ratio

