این فایل کشینگ من هست :
from logger_config import logger
import hashlib
from collections import OrderedDict
from threading import RLock

NUMBA_AVAILABLE = True

try:
    from numba import jit
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    logger.warning("Numba not available. Using standard calculations.")
    
    def jit():
        def decorator(func):
            return func
        return decorator


class LRUCache:
    def __init__(self, max_size=1000):
        self.max_size = max_size
        self.cache = OrderedDict()
        self.lock = RLock()
        self.hit_count = 0
        self.miss_count = 0
    
    def get(self, key):
        with self.lock:
            if key in self.cache:
                self.cache.move_to_end(key)
                self.hit_count += 1
                return self.cache[key]
            self.miss_count += 1
            return None
    
    def put(self, key, value):
        with self.lock:
            if key in self.cache:
                self.cache.move_to_end(key)
            else:
                if len(self.cache) >= self.max_size:
                    self.cache.popitem(last=False)
            self.cache[key] = value
    
    def clear(self):
        with self.lock:
            self.cache.clear()
            self.hit_count = 0
            self.miss_count = 0
    
    def stats(self):
        total = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total if total > 0 else 0
        return {
            'size': len(self.cache),
            'hits': self.hit_count,
            'misses': self.miss_count,
            'hit_rate': hit_rate
        }


_indicator_cache = LRUCache(max_size=1000)


def get_fast_hash(data, length=None):
    if data is None:
        return None
    
    try:
        if hasattr(data, 'values'):
            if length is None:
                length = len(data)
            
            if length > 100:
                step = length // 50
                sample_data = data.values[::step]
            else:
                sample_data = data.values
            
            hash_input = f"{length}_{sample_data.tobytes()}"
        else:
            hash_input = str(data)
        
        return hashlib.blake2b(hash_input.encode(), digest_size=8).hexdigest()
    except Exception as e:
        logger.error(f"Error getting fast hash: {e}")
        return None


def get_dataframe_signature(df, columns=None):
    if df is None or len(df) == 0:
        return None
    
    try:
        if columns is None:
            numeric_cols = df.select_dtypes(include='number').columns.tolist()
            columns = numeric_cols[:5] if numeric_cols else ['close']
        
        available_cols = [col for col in columns if col in df.columns]
        if not available_cols:
            return None
        
        df_subset = df[available_cols]
        length = len(df_subset)
        
        if length > 1000:
            indices = [0, length//4, length//2, 3*length//4, length-1]
            sample_data = df_subset.iloc[indices]
        else:
            sample_data = df_subset
        
        data_hash = get_fast_hash(sample_data.values.flatten())
        return f"{length}_{data_hash}" if data_hash else None
    except Exception as e:
        logger.error(f"Error getting DataFrame signature: {e}")
        return None


def create_cache_key(indicator_name, df_signature, *args, **kwargs):
    if df_signature is None:
        return None
    
    try:
        key_parts = [indicator_name, df_signature]
        
        if args:
            args_str = '_'.join(str(arg) for arg in args)
            key_parts.append(args_str)
        
        if kwargs:
            filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'df'}
            if filtered_kwargs:
                kwargs_str = '_'.join(f"{k}:{v}" for k, v in sorted(filtered_kwargs.items()))
                key_parts.append(kwargs_str)
        
        return '_'.join(key_parts)
    except Exception as e:
        logger.error(f"Error creating cache key for {indicator_name}: {e}")
        return None


def cached_calculation(indicator_name):
    def decorator(func):
        def wrapper(df, *args, **kwargs):
            try:
                if df is None or len(df) == 0:
                    return None
                
                df_signature = get_dataframe_signature(df)
                cache_key = create_cache_key(indicator_name, df_signature, *args, **kwargs)
                
                if cache_key:
                    cached_result = _indicator_cache.get(cache_key)
                    if cached_result is not None:
                        return cached_result
                
                result = func(df, *args, **kwargs)
                
                if result is not None and cache_key:
                    _indicator_cache.put(cache_key, result)
                
                return result
            except Exception as e:
                logger.warning(f"Cache error for {indicator_name}: {e}")
                return func(df, *args, **kwargs)
        
        return wrapper
    return decorator


def clear_cache():
    _indicator_cache.clear()


def get_cache_stats():
    return _indicator_cache.stats()


class CacheConfig:
    @staticmethod
    def set_max_size(size):
        global _indicator_cache
        stats = _indicator_cache.stats()
        _indicator_cache = LRUCache(max_size=size)
        logger.info(f"Cache size changed to {size}. Previous stats: {stats}")
    
    @staticmethod
    def enable_detailed_logging():
        logger.setLevel('DEBUG')
    
    @staticmethod 
    def disable_detailed_logging():
        logger.setLevel('INFO')

لطفا توابع زیر را به گونه ای ویرایش کن تا به بهترین شکل در محاسبات از کشینگ استفاده کنند.